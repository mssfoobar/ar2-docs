{"searchDocs":[{"title":"Versioning Scheme","type":0,"sectionRef":"#","url":"/docs/releases/versioning-scheme","content":"","keywords":"","version":null},{"title":"Overall System - Pragmatic Versioning​","type":1,"pageTitle":"Versioning Scheme","url":"/docs/releases/versioning-scheme#overall-system---pragmatic-versioning","content":" Moving forward, we'll be using Pragmatic Versioning for our overall release version numbers. We intend to provide more consistent release notes with a release versioning scheme that will be useful for projects utilizing AGIL Ops Hub.  Pragmatic versioning comes with 3 numbers, separated by periods, just like semantic versioning. However, the numbers correspond to BIG RELEASE . ANNOUNCEMENT . INCREMENT. Read more about Pragmatic Versioning.  ","version":null,"tagName":"h2"},{"title":"Individual Services - Semantic Versioning​","type":1,"pageTitle":"Versioning Scheme","url":"/docs/releases/versioning-scheme#individual-services---semantic-versioning","content":" As for individual services and/or packages and libraries, we'll be sticking to the typical semantic versioning.  ","version":null,"tagName":"h2"},{"title":"Why the discrepancy?​","type":1,"pageTitle":"Versioning Scheme","url":"/docs/releases/versioning-scheme#why-the-discrepancy","content":" It makes more sense to use pragmatic versioning for the overall system as it is actually an aggregation of many different services, as well as the architecture of the entire system. The versioning of the overall system also carries branding, and other meanings. If we use semantic versioning for that, it can be quite unclear which number to increase for the overall system, or simply not useful. Pragmatic versioning more closely matches the message we want to convey.  As for indivdual services or packages, it's much clearer whether a change is a MAJOR, MINOR, or PATCH and that message can be conveyed through the version numbers. ","version":null,"tagName":"h2"},{"title":"What should the deployment folder contain?","type":0,"sectionRef":"#","url":"/docs/docs/contributing/documentation/deployment","content":"What should the deployment folder contain? This folder is a catch-all section to put all development-related docs for your module. Examples of what should be here: What all the deployment configurations are, and how to set them - for example, what are all the environment variables for your service.What does each configuration option do?Recommended values under different circumstances, especially recommendations for production settings. As always, if you need to split your deployment information up into multiple files, feel free to add more files to this section.","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/contributing/documentation/development","content":"Development This folder is a catch-all section to put all development-related docs for your module. Examples of what should be here: The concepts your module introduces, such as how it operates, what entities in introduces, what are the relationship between these entities (e.g. an ER diagram would be very useful here).More installation and setup instructions if necessary (outside of the quickstart).Recipes and examples on how to use your module - concrete sample code. For the development section, it's highly likely that you'll need to split the documentation up into multiple files; please feel free to add more files to this section.","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/contributing/documentation/configuration","content":"","keywords":"","version":"Next"},{"title":"Example Configuration Table​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#example-configuration-table","content":" The following is an example of configuration rendered using a markdown table - this is good for config maps in JSON/YAML where the data type might matter:  Name\tType\tDefault\tDescriptiontitle\tstring\t'Sample Config Title'\tThe title that is displayed for your this example application. outputDir\tstring\t'./'\tThe path to output something from this example application. isSecure\tboolean\tfalse\tSet to true if you want your example application to be secure. friends\tstring[]\t[]\tThe list of friends you wish you had. enemies\tstring[]\t[ &quot;Jon Snow&quot;]\tThe list of enemies you currently have.  For environment variables, the type column can be omitted as they are always strings.  ","version":"Next","tagName":"h2"},{"title":"Example usage in JSON:​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#example-usage-in-json","content":" { &quot;title&quot;: &quot;Sample Config Title&quot;, &quot;outputDir&quot;: &quot;./&quot;, &quot;isSecure&quot;: false, &quot;friends&quot;: [], &quot;enemies&quot;: [ &quot;Jon Snow&quot; ] }   ","version":"Next","tagName":"h3"},{"title":"Example usage in YAML:​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#example-usage-in-yaml","content":" title: Sample Config Title outputDir: ./ isSecure: false friends: [] enemies: - Jon Snow   ","version":"Next","tagName":"h3"},{"title":"Example using markdown headers​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#example-using-markdown-headers","content":" For more complex configurations that require longer descriptions, you should segregate them with markdown headers. The advantage of using markdown headers is that it also creates anchor links you can use to share directly to the section like this for title:  ","version":"Next","tagName":"h2"},{"title":"title​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#title","content":" Type: string​ Default: 'Sample'​ The title that is displayed for your this example application. This longer example might include an image of how the title looks like:      ","version":"Next","tagName":"h3"},{"title":"outputDir​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#outputdir","content":" Type: string​ Default: './'​ The path to output something from this example application.  ","version":"Next","tagName":"h3"},{"title":"isSecure​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#issecure","content":" Type: boolean​ Default: false​ Set to true if you want your example application to be secure. caution In some cases you might also need to call include admonitions like this, which won't fit in a table.  ","version":"Next","tagName":"h3"},{"title":"friends​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#friends","content":" Type: string[]​ Default: []​ The list of friends you wish you had.  You might need to elaborate on your configuration, like how you wish you were friends with Princess Diana or Jeremy Clarkson. In which case you should pass in [ 'Princess Diana', 'Philomena Cunk' ] in this example.  ","version":"Next","tagName":"h3"},{"title":"enemies​","type":1,"pageTitle":"Configuration","url":"/docs/docs/contributing/documentation/configuration#enemies","content":" Type: string[]​ Default: [ 'Jon Snow' ]​ The list of friends you wish you had.  The list of enemies you currently have. ","version":"Next","tagName":"h3"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/contributing/documentation/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/contributing/documentation/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/contributing/documentation/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Source Control","type":0,"sectionRef":"#","url":"/docs/docs/contributing/development/source-control","content":"","keywords":"","version":"Next"},{"title":"Git​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#git","content":" We use Git as our VCS (version control system) because it is highly mature, widely supported, has well established best-practices, and has many tools that support it.  How this ties to Developer Operations​  Since we want our application to be cloud native, we need to have a good degree of infrastructure automation. We'll have to be able to easily scale up or down services by declaratively describing their deployment configurations, and we'll want to have preview servers automatically deployed when we check in our code to faciliate faster development cycles.  To achieve these things, our applications are containerized and orchestrated by Kubernetes, and the container images are built automatically by our CI. These images are created whenever a commit occurs on a Git branch that is tied to an environment (e.g. when a merge request is successfully merged from a feature branch to the release branch, the new code from the release branch would then be built by the CI into an image, and this image will be uploaded to a container registry, and ultimately deployed by Kubernetes).  In order for these systems to be run effectively, we adopt the principles ofGitOps, this is necessary for us to develop and deploy our system in a reliable, structured way.  What is Git Ops?​  GitOps is a practice that makes Git the single source of truth for our infrastructure definitions. This means that we will be able to make changes to the system's deployment 'simply' by looking at the state of the Git repository. By tying the configuration of the system infrastructure and deployment to source code, we also gain the ability to version the infrastructure since they can be tied to individual Git commits.  Read more about GitOps here.  ","version":"Next","tagName":"h2"},{"title":"Branching Strategy​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#branching-strategy","content":" We are essentially using a variations of Git Flow as our branching strategy. This is required because our deployment environments are tied to branches in your Git repository. These branches that get deployed can be seen as 'deployment' branches.  For each environment that you want to have automatically deployed, you need a branch. When developers need to make changes to that branch, they check out a new branch using that as the base. They then make changes to their 'personal' feature branch, and when they are done, they make a pull request to merge it back to the base branch, which then gets triggered to be deployed.  For example, someone found a typo in the web develop branch (this branch deploys to a preview server for us to view changes made to the web server) - for a change to be made, a developer must use the develop branch as the base and checkout a new branch with a new name (e.g. fix/ambulance-chart-typo). They then make the changes to fix this typographical error, and open a pull request to contribute this change back to the develop branch. This pull request must be reviewed by another developer to ensure it adheres to the projects guidelines and as a primary check to ensure it does not sabotage the system (code formatting and linting should be done automatically by pre-commit hooks).  Git Flow is a popular and well documented branching strategy, read more about ithere.  We have 4 categories of branches:  Production​ main The branch called main is reserved specifically for the most up to date code. This might map directly to a production environment. Release​ release/* Branches starting with &quot;release&quot; are reserved for code that is being prepared for release Development​ develop The branch called develop is reserved for the latest development code. This will be the base for all feature branch checkouts. Feature​ feat|fix|release.../... Developers are to work on branches with a naming scheme that starts withfeat, fix, release, refactor, chore or style, followed by a slash, then a kebab case description. This is to help us quickly identify the purpose of each branch. Example branch name - fix/ambulance-chart-typo  ","version":"Next","tagName":"h3"},{"title":"Changesets​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#changesets","content":" We use changesets to managing versioning and changelogs for our repositories.  ","version":"Next","tagName":"h3"},{"title":"Code Merging (via Pull/Merge requests)​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#code-merging-via-pullmerge-requests","content":" The expected workflow for merging code is as follows:  You've finished fixing the typographical errors in your fix/ambulance-chart-typo branch and wish to merge it back to the develop branch - you've run changeset to record your changes and committed your code following our standards.  Create a pull requestInform the team (if configured appropriately on GitHub, CODEOWNERSand other relevant members might be notified automatically via GitHub )Discuss and make changes if requiredOnce approved, and all checks passs, use Squash &amp; Mergeto merge in the commitsDelete the branch  ","version":"Next","tagName":"h3"},{"title":"Conventional Commits​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#conventional-commits","content":" Conventional commits is a specification for adding human and machine readable meaning to commit messages.  The specification is well documented, read more about conventional commits here.  The actual rule we use for allowed types (the prefix of the commit message) can be found in theweb-base's commitlint config file.  Summary​  The Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of. This convention dovetails with SemVer, by describing the features, fixes, and breaking changes made in commit messages.  The commit message should be structured as follows:  &lt;type&gt;[optional scope]: &lt;description&gt; [optional body] [optional footer(s)]   Why conventional commits?​  Following conventional commits allows many tools to parse and make sense of commits since they follow an agreed upon convention. It also standardizes them in a format that lets the reader make sense of the commit. On our web server, we use husky to apply a pre-commit check to enforce that developers commit with messages that follows conventional commits. We can then use tools to generate release notes based on these commits - but the quality of these notes will directly be related to the quality of the commit messages and pull requests, so please take this practice seriously!  ","version":"Next","tagName":"h3"},{"title":"Enforcement​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#enforcement","content":" GitHub Rulesets  GitHub rulesets for each branch is configured to restrict pushes to specific branches without a pull request with reviewers, as well as add branch naming patterns to follow, block force pushes etc. Refer to theweb-base's rule settings for reference - you can export and reuse these rules in your new repository for convenience.  Husky  Husky is a tool we use to control the quality of code and enforce commit standards. Before each commit, linting and formatting for code should be run, and commit messages are checked against commit lint's rules.    ","version":"Next","tagName":"h3"},{"title":"Signing Commits​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#signing-commits","content":" To sign commits that are valid under GitHub's rules - you must use a GPG Keypair. Signing commits allows us to verify that commits are made by the user that's committing them.  Ideally, all branches should be protected to require commits to be signed. This can be done by configuring GitHub's branch protection rules to ensure commits are only allowed when they have been verified with a GPG signature. The following section explains how you can set your computer up with a GPG keypair to sign your git commits.  ","version":"Next","tagName":"h2"},{"title":"1. Generate a GPG Keypair​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#1-generate-a-gpg-keypair","content":" To generate a new GPG key on the machine:  gpg --full-generate-key   1 for (1) RSA and RSA (default) when prompted on what kind of key you want.4096 when prompted on what keysize you want.0 when prompted how long the key should be valid so that it does not expire (unless you wish to commit to a different policy).y to confirm that the key does not expire if you picked 0.&lt;your name&gt; when asked for your name. This does not need to match the commit name.&lt;email address&gt; when asked for your email, this must match the commit email address.&lt;comment&gt; (optional) some comments to tag onto this key.O for (O)kay when you're done.A dialogue box will appear - key in your passphrase for the key - you MUST remember this passphrase and use it every time to sign a commit. You will be asked to key the passphrase in again to confirm.  tip See this reference for more information.  ","version":"Next","tagName":"h3"},{"title":"2. Set up the GPG public key​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#2-set-up-the-gpg-public-key","content":" 2.1. List your existing GPG keys​  To list existing GPG keys on the machine:  gpg --list-keys --keyid-format=long gpg --list-secret-keys --keyid-format=long   GPG key ID is located after sec rsa4096/  tip See this reference for more information.  2.2. Print the GPG public key​  Print the GPG public key in ASCII armor format:  gpg --armor --export &lt;key-ID&gt;   tip See this reference for more information.  2.3. Add the GPG public key to GitHub​  Copy the entire output from 2.1 into your GitHub account's settings &gt; keys  Select New GPG key and paste the output there.  Also, it is highly recommended that you set your account to Vigilant mode to mark any unsigned commits as unverified.    tip See this reference for more information.  ","version":"Next","tagName":"h3"},{"title":"3. Set up the GPG private key​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#3-set-up-the-gpg-private-key","content":" Edit the .gitconfig for global configuration. On Windows machines, it should be at %USERPROFILE%/.gitconfig. Or edit the .git/config for each repository's configuration.  [user] email = &lt;commit email&gt; signingkey = &lt;key-ID&gt; [commit] gpgsign = true   Alternatively, you can use the following commands to edit your git config from the terminal.   git config user.email &quot;&lt;commit email&gt;&quot; git config user.signingkey &quot;&lt;key-ID&gt;&quot; git config commit.gpgsign true   You can add the --global flag to modify the settings globally (for any/all repositories in your machine). Repository settings will supersede global settings.   git config --global user.email &quot;&lt;commit email&gt;&quot; git config --global user.signingkey &quot;&lt;key-ID&gt;&quot; git config --global commit.gpgsign true   After that, you will be prompted for a passphrase every time you commit.  tip See this reference for more information.  ","version":"Next","tagName":"h3"},{"title":"List your existing GPG keys​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#list-your-existing-gpg-keys","content":" To list existing GPG keys on the machine:  gpg --list-keys --keyid-format=long gpg --list-secret-keys --keyid-format=long   GPG key ID is located after sec rsa4096/  This will be useful, especially when you need to view the key ID (used in many of the commands).  tip See this reference for more information.  ","version":"Next","tagName":"h3"},{"title":"Migrate GPG Keypairs​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#migrate-gpg-keypairs","content":" To export the keypairs from your PC:  gpg --export &lt;key-ID&gt; &gt; public_keys.pgp gpg --export-secret-keys &lt;key-ID&gt; &gt; private_keys.pgp   To import the keypairs to your other PC:  gpg --import &lt; public_keys.pgp gpg --import &lt; private_keys.pgp   tip See this reference for more information.  ","version":"Next","tagName":"h3"},{"title":"Delete GPG Keypairs​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#delete-gpg-keypairs","content":" To delete the keypairs from your PC:  info You must delete the private key before you can delete the public key.  gpg --delete-secret-key &lt;key-ID&gt; gpg --delete-key &lt;key-ID&gt;   Or delete the .gnupg file. On Windows machines, it should be at %USERPROFILE%/.gnupg.  tip See this reference for more information.  ","version":"Next","tagName":"h3"},{"title":"Password Entry on Mac​","type":1,"pageTitle":"Source Control","url":"/docs/docs/contributing/development/source-control#password-entry-on-mac","content":" If you're on MacOS, under the default configuration, gpg might not be able to create a password prompt for you. Run the following commands in sequence to install pinetry-mac, which is a small collection of dialog programs that allow GnuPG to read passphrases and PIN numbers in a secure manner.  brew install pinentry-mac echo &quot;pinentry-program $(which pinentry-mac)&quot; &gt;&gt; ~/.gnupg/gpg-agent.conf killall gpg-agent  ","version":"Next","tagName":"h3"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/dash/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/dash/development","content":"Development","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/contributing/documentation/overview","content":"","keywords":"","version":"Next"},{"title":"What is Docusaurus, and what is this for?​","type":1,"pageTitle":"Overview","url":"/docs/docs/contributing/documentation/overview#what-is-docusaurus-and-what-is-this-for","content":" Docusaurus is a project developed by Facebook to build, maintain, and deploy documentation websites. It is React-based but does not require react knowledge. Document pages are written in markdown and rendered as HTML after being built.  We are using this project to maintain documentation to help developers understand and use AGIL Ops Hub.  ","version":"Next","tagName":"h2"},{"title":"Folder Structure​","type":1,"pageTitle":"Overview","url":"/docs/docs/contributing/documentation/overview#folder-structure","content":" The default structure you see here has a folder for each category:  Quickstart *DevelopmentConfigurationDeploymentReference  Each folder has a _category_.json and a matching default .mdx file. When the file name matches the folder name, Docusaurus collapses them into a single sidebar button. If you add one or more .md or .mdx files, it will turn into a category. This default structure allows you to expand each individual section as needed for your module.  * Ideally, only one single file in the quickstart folder should be enough, though it is understandable that as the module grows and it gets more complex, it might start to require multiple quickstarts for different parts of your module - in that scenario, just create more quickstart files and add them to this folder.  ","version":"Next","tagName":"h2"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/dash/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"What are you expected to write for this Overview section?​","type":1,"pageTitle":"Overview","url":"/docs/docs/contributing/documentation/overview#what-are-you-expected-to-write-for-this-overview-section","content":" The overview is expected to be at least (but not limited to) 1 page long and should explain clearly what the module is about and why it might provide value to the reader. How long is one page? Take it as 250 - 500 words long.  Make sure to provide definitions for all the abbreviations used here as well; the reader might have no idea what UCSor IAM stands for. All abbreviations must be written in full at least once.  This is also where you give simple examples mapping to real-world use cases on how the module might be used (not code examples). For example, if your module is UCS (Universal Communications System), talk about how it can be integrated with other modules to allow text and video communication between users; like how an alert might be triggered by some part of the system and the appropriate users can be automatically assigned to a new UCS chat room, quickly giving them a communications channel to deal with the new issue.  ","version":"Next","tagName":"h2"},{"title":"What should you NOT include in your overview?​","type":1,"pageTitle":"Overview","url":"/docs/docs/contributing/documentation/overview#what-should-you-not-include-in-your-overview","content":" The architecture and technology stack or libraries used do not need to be here. If they are relevant, particularly if it is useful for the developers take into considerations for performance or maintenance reasons, these can be discussed in a different section.  Not every module will be documented in the same way. Some might need extra sections, such as considerations for performance, maintenance, security, etc. Step-by-step instructions should be included in thedevelopment section, otherwise, these additional sections can be added at the top-level (adjacent to the overview) (Documentation in this case). ","version":"Next","tagName":"h2"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/dash/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/gis/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/contributing/documentation/quickstart","content":"","keywords":"","version":"Next"},{"title":"Pre-requisites​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/contributing/documentation/quickstart#pre-requisites","content":" Please install the following programs and tools before creating your module documentation:  Recommended tools:  Visual Studio Code  Required:  node.js (version ^20)git (version ^2.47)Access to the source code at https://github.com/mssfoobar/docs  ","version":"Next","tagName":"h2"},{"title":"Installation and running​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/contributing/documentation/quickstart#installation-and-running","content":" Clone the repository into a folder of your choosing:  git clone https://github.com/mssfoobar/docs   Go into the newly cloned directory:  cd docs   Get the required node dependencies:  docs &gt; npm install   Run the start command to view the Docusaurus site in development mode:  docs &gt; npm run start   The website should now be open on http://localhost:3000/docs/. Changes you make and save will be automatically reloaded on the page.  You can now make changes and preview them locally before merging them into the  Next, we will look at how we can add a new section in the navigation bar at the top of the page.  ","version":"Next","tagName":"h2"},{"title":"Create documentation for a new module​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/contributing/documentation/quickstart#create-documentation-for-a-new-module","content":" Choose a short unique label for your module. In this example, we're using MNM for (My New Module). Create a new folder for your module documentation files to exist in. Most of the time, you'll want to copy this documentation folder and use it as a template for your new module. Change the title of the folders accordingly.  docs cp -r ./docs/contributing/documentation ./docs/modules/mnm   Set the name, position, and icon for your module:  In the _category_.json file for your module, set the &quot;label&quot; to &quot;My New Module&quot;, and the position to the new desired position in the sidebar.  Choose an icon for your module: 4.1. Head on over to Lucide Icons and pick an icon for your module. In this example, I'll use the battery-charging icon. 4.2. At the icon's page, click Copy Component Name to get the component name in CamelCase format. Alternatively, you can manually type the kebab-case format listed (battery-charging) to theCamelCase version (BatteryCharging). 4.3. Change the icon property to your desired icon (BatteryCharging in this case).  Before modifying:​  _category_.json { &quot;label&quot;: &quot;Documentation&quot;, &quot;position&quot;: 2, &quot;customProps&quot;: { &quot;icon&quot;: &quot;BookCopy&quot; } }   After modifying:​  _category_.json { &quot;label&quot;: &quot;My New Module&quot;, &quot;position&quot;: 1337, &quot;customProps&quot;: { &quot;icon&quot;: &quot;BatteryCharging&quot; } }  ","version":"Next","tagName":"h2"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/dash/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/dash/overview","content":"Overview","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/dash/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/dash/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/dash/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/gis/development","content":"Development","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/gis/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/gis/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/gis/overview","content":"Overview The GIS module provides essential features for command and control systems to display information in a geospatial context - specifically, on a map. Features: Map rendering interface (CesiumJS implementation) Using renderes other than CesiumJS is possible but requires interface implementation Camera Bookmarks Save and delete camera positionFly to saved camera position Geospatial Entity Rendering Render entities on the map based on their positionRender entities conditionally and change their styles based on entity attributes","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/gis/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/gis/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/gis/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/gis/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/overview","content":"Overview This section highlight the necessary settings required to setup in Keycloak realm for IAMS to work. note Note that the instruction steps in the Quick Start will create a default realm named AOH with the correct settings. You don’t need to perform the following configuration for the default realm.","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Change the Initial Login Keycloak Admin Credentials","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/configuring-for-production/change-initial-login-keycloak-credentials","content":"Change the Initial Login Keycloak Admin Credentials The deployment manifests file for Keycloak create a default admin credential for initial login to setup Keycloak. The default admin account is the super admin account to Keycloak that has all the administrative right to Keycloak. Hence, it is important that the default credential be changed immediately after successful initial login. As it not possible to change the username of the default admin account, it is also recommended to create another admin account and disable the default admin account. It is also highly recommended to enable 2FA for the admin account. Refer to Keycloak administration documentation on how to enable 2FA for user: https://www.keycloak.org/docs/latest/server_admin/index.html#one-time-password-otp-policies","keywords":"","version":"Next"},{"title":"Default Roles","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/default-roles","content":"Default Roles IAMS required the following realm roles to be configured as default roles for all users: realm-management view-clientsrealm-management view-authorization Perform the following steps to configure the default roles: Login to the Web Admin Console and navigate to the realm. Click on Realm settings on the side menu. Click on User registration tab: Click on Assign role button: Check the followings from the list: realm-management view-authorizationrealm-management view-client Click on Assign button to complete the setup.","keywords":"","version":"Next"},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/docs/contributing/development/logging","content":"","keywords":"","version":"Next"},{"title":"General Logging Guidelines​","type":1,"pageTitle":"Logging","url":"/docs/docs/contributing/development/logging#general-logging-guidelines","content":" The following is a list of recommendations we have for what should be printed at which log levels. The recommendations are based on the assumption that the system will be running at INFO level when in production mode.  Log Level\tExample\tDescriptionDEBUG\trequest received, show all inputs\tProvide information that can be used to diagnose issues especially those related to program state. INFO\tserver ready to listen to requests\tRecord events that indicate that program is functioning normally. WARN\tno host name supplied, using default value\tRecord potential issues in your application. They may not be might need to be investigated. ERROR\tfailed to connect to database\tRecords unexpected errors that occur during the operation of your application. In most cases, the error should be addressed as soon as possible to prevent further problems or outages.  TRACE and FATAL might be supported by different logging frameworks in different languages. Since we support multiple languages, we will not restrict these log levels, however we don't have official recommendations on what must be logged at these levels, except that TRACE be the lowest log level (most verbose), and FATAL be the highest log level (printed at all log levels).  ","version":"Next","tagName":"h2"},{"title":"TypeScript, Svelte (Frontend)​","type":1,"pageTitle":"Logging","url":"/docs/docs/contributing/development/logging#typescript-svelte-frontend","content":" ","version":"Next","tagName":"h2"},{"title":"Format​","type":1,"pageTitle":"Logging","url":"/docs/docs/contributing/development/logging#format","content":" The logs must be formatted as a JSON - this allows us to perform searches based on the key, as well as pretty print the logs using tools:  It must minimally have one of the following log levels:  DEBUGINFOWARNERROR  Example:  { &quot;level&quot;: &quot;INFO&quot;, ... }   Any other data can then be included as more keys in the log object:  Example:  { &quot;level&quot;: &quot;ERROR&quot;, &quot;timestamp&quot;: &quot;2023-10-25T03:06:23.423+0000&quot;, &quot;message&quot;: &quot;Failed to secure the crown jewels. Aborting.&quot; }   ","version":"Next","tagName":"h3"},{"title":"Retries​","type":1,"pageTitle":"Logging","url":"/docs/docs/contributing/development/logging#retries","content":" info TLDR; errors that result in retries should be logged as WARN  Often, there might be processes that result in errors that are actually anticpated or expected (for example, token expiries, or network errors might be retried) - these errors might be retried, and succeeded. In those scenarios, the errors should be logged with WARN, only the final error should be logged as ERROR, or maybe even FATAL in fatal/panic scenarios.  ","version":"Next","tagName":"h3"},{"title":"Golang (Backend)​","type":1,"pageTitle":"Logging","url":"/docs/docs/contributing/development/logging#golang-backend","content":" ","version":"Next","tagName":"h2"},{"title":"Logging​","type":1,"pageTitle":"Logging","url":"/docs/docs/contributing/development/logging#logging-1","content":" Logging should be in JSON structured format to make it easier to be read, searched and analyzed by any application or an interested individual.  We use aohlog package from aoh-golib. aohlog is built on top of the popular logging library zap by uber with preconfiguration to suite AOH framework.  Below is the supported log levels  DEBUGINFOWARNERRORPANICFATAL  Example:  import ( aohlog &quot;github.com/mssfoobar/aoh-golib/logger&quot; &quot;go.uber.org/zap&quot; ) func main() { // by default aohlog set log level to Production which log at `INFO` or above log level // if you want to see log at `DEBUG` or above, set it explicitly aohlog.SetDevelopment() // aohlog.SetProduction() // if uncommented, this will set the log level back to `INFO` aohlog.Debug(&quot;This is a DEBUG message&quot;) aohlog.Info(&quot;This is an INFO message&quot;) aohlog.Info(&quot;This is an INFO message with additional fields&quot;, zap.String(&quot;region&quot;, &quot;us-west&quot;), zap.Int(&quot;id&quot;, 2)) aohlog.Warn(&quot;This is a WARN message&quot;) aohlog.Error(&quot;This is an ERROR message&quot;) // aohlog.Panic(&quot;This is a Panic message&quot;) // would exit if uncommented // aohlog.Fatal(&quot;This is a FATAL message&quot;) // would exit if uncommented }   Logging at development mode will print the log in semi-structured format which is easier for human to read.  2024-08-16T11:58:20.938+0800 DEBUG example/main.go:15 This is a DEBUG message 2024-08-16T11:58:20.938+0800 INFO example/main.go:16 This is an INFO message 2024-08-16T11:58:20.938+0800 INFO example/main.go:17 This is an INFO message with additional fields {&quot;region&quot;: &quot;us-west&quot;, &quot;id&quot;: 2} 2024-08-16T11:58:20.938+0800 WARN example/main.go:18 This is a WARN message 2024-08-16T11:58:20.938+0800 ERROR example/main.go:19 This is an ERROR message   Logging at production mode will print the log in json structured format which is easier for machine to parse and analyze.  {&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2024-07-19T16:46:45.726+0800&quot;,&quot;caller&quot;:&quot;example/main.go:10&quot;,&quot;msg&quot;:&quot;This is an INFO message&quot;} {&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2024-07-19T16:46:45.726+0800&quot;,&quot;caller&quot;:&quot;example/main.go:11&quot;,&quot;msg&quot;:&quot;This is an INFO message with fields&quot;,&quot;region&quot;:&quot;us-west&quot;,&quot;id&quot;:2} {&quot;level&quot;:&quot;WARN&quot;,&quot;ts&quot;:&quot;2024-07-19T16:46:45.726+0800&quot;,&quot;caller&quot;:&quot;example/main.go:12&quot;,&quot;msg&quot;:&quot;This is a WARN message&quot;} {&quot;level&quot;:&quot;ERROR&quot;,&quot;ts&quot;:&quot;2024-07-19T16:46:45.726+0800&quot;,&quot;caller&quot;:&quot;example/main.go:13&quot;,&quot;msg&quot;:&quot;This is an ERROR message&quot;}   ","version":"Next","tagName":"h3"},{"title":"Error Handling​","type":1,"pageTitle":"Logging","url":"/docs/docs/contributing/development/logging#error-handling","content":" info GO CODING GUIDE already covers how to handle error. This section on error handling is just a reiteration of it to handle error by logging.  Logging is a part of error handling. If the application encounters an error, use an appropriate log level to log it.  Don't log the error every time one occurs, as doing so will create duplicate logs in your application. Instead, return the error to the caller and let it log the error when appropriate.  FATAL should be only used at the highest level in your code which is at the main() function.FATAL logs the message and then immediately calls os.Exit(1). This can have several consequences such as  defer statements in other goroutines don't runbuffers aren't flushedtemporary files and directories aren't removed ","version":"Next","tagName":"h3"},{"title":"Limit the number of queued requests","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/configuring-for-production/limit-the-number-of-queued-requests","content":"Limit the number of queued requests A production environment should protect itself from an overload situation, so that it responds to as many valid requests as possible, and to continue regular operations once the situation returns to normal again. One way of doing this is rejecting additional requests once a certain threshold is reached. Load shedding should be implemented on all levels, including the load balancers in your environment. In addition to that, there is a feature in Keycloak to limit the number of requests that can’t be processed right away and need to be queued. By default, there is no limit set. Set the option http-max-queued-requests to limit the number of queued requests to a given threshold matching your environment. Any request that exceeds this limit would return with an immediate 503 Server not Available response. To set http-max-queued-requests, use the following environment variable: KC_HTTP_MAX_QUEUED_REQUESTS","keywords":"","version":"Next"},{"title":"Enable TLS for Secure Communication","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/configuring-for-production/enable-tls-for-secure-communication","content":"Enable TLS for Secure Communication As Keycloak continually exchanges sensitive data, which means that all communication to and from Keycloak requires a secure communication channel. To prevent several attack vectors, HTTP over TLS, or HTTPS, for that channel need to be enabled. Followings are various communication exchange for Keycloak: Incoming communication from external (Web browsers, mobile devices, etc.). These typically from Internet or Intranet and pass through firewall, load balancer, and via Ingress Controller. Incoming communication from internal (Web Backend App, Backend Services, etc.). These are backchannel requests (e.g., exchange for Tokens) and routed within Kubernetes Cluster. Outgoing communication to internal (Web Backend App, Backend Services, etc.). These are backchannel requests to do single logout and are routed within Kubernetes Cluster. Outgoing communication to external (External Idp, Active Directory, etc.). These are typically route out to Internet or Internet. At the minimum, all communication to and from external should be protected by HTTP over TLS. Some project may require that traffic routed within the Kubernetes Cluster to be also TLS protected. To enable TLS for incoming communication channel, refer to: https://www.keycloak.org/server/enabletls To enable TLS for outgoing communication: https://www.keycloak.org/server/outgoinghttp To enable TLS for cache communication: https://www.keycloak.org/server/caching","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/configuring-for-production/overview","content":"Overview This section highlight some of the configuration required to make IAMS and Keycloak ready for production. For the full list of recommendations for Keycloak, refer to:https://www.keycloak.org/server/configuration-production","keywords":"","version":"Next"},{"title":"Secure the Database Credential","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/configuring-for-production/secure-database-credentials","content":"Secure the Database Credential Keycloak deployed using the instructions in the Quick Start section is mean for development and need to be modified if to use for production. One of the key changes is to ensure that the database credential is not directly hardcoded in the deployment manifests file. The database credential should be stored in Kubernetes Secrets. Note that by default, the data in a Kubernetes Secrets is obfuscated by using merely Base64 encoding. This encoding method does not encrypt the data within it. To protect the secrets, the following steps should be taken: Enable Encryption at Rest.Enable or configure RBAC rules to restrict access to the secrets in a cluster.Restrict Secret access to specific containers, or the containers that require access to the secret to perform their operations.Consider using external Secret store providers, such as Hashicorp’s Vault. For more information on Kubernetes Secrets, refer to the following link:https://kubernetes.io/docs/concepts/configuration/secret/","keywords":"","version":"Next"},{"title":"Exposing the Keycloak Administration APIs and UI on a different hostname","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/configuring-for-production/exposing-keycloak-on-different-hostname","content":"Exposing the Keycloak Administration APIs and UI on a different hostname It is considered a best practice to expose the Keycloak Administration REST API and Console on a different hostname or context-path than the one used for the public frontend URLs that are used e.g. by login flows. This separation ensures that the Administration interfaces are not exposed to the public internet, which reduces the attack surface. To configure a different hostname or context-path for Administration REST API and console, use the following environment variable:KC_HOSTNAME_ADMIN Note that using the KC_HOSTNAME_ADMIN option does not prevent accessing the Administration REST API endpoints via the frontend URL specified by the hostname option. If you want to restrict access to the Administration REST API, you need to do it on the Network Infrastructure level (e.g., reverse proxy, load balancer, or firewall level). Administration Console implicitly accesses the API using the URL as specified by the hostname-admin option.","keywords":"","version":"Next"},{"title":"Realm Roles","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/realm-roles","content":"","keywords":"","version":"Next"},{"title":"Create realm-tenant-admin Role​","type":1,"pageTitle":"Realm Roles","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/realm-roles#create-realm-tenant-admin-role","content":" Login to the Web Admin Console and navigate to the realm. Click on Realm roles on the side menu:    Click on Create role button    Enter the following for the role:  Role name: realm-tenant-admin  Click on Save button to create    ","version":"Next","tagName":"h2"},{"title":"Create system-admin Role​","type":1,"pageTitle":"Realm Roles","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/realm-roles#create-system-admin-role","content":" Login to the Web Admin Console and navigate to the realm. Click on Realm roles on the side menu. Click on Create role button Enter the following for the role:  Role name: system-admin  Click on Save button to create the role. Click on the Action dropdown menu on the right hand side of the screen and select Add associated roles submenu.    Check realm-management realm-admin from the list:    Click on Assign button to assign selected roles to system-admin role:    Click on Assign Role button    Select Filter by realm roles from the filter    Check realm-tenant-admin from the list    Click on Assign button to complete the configuration.    ","version":"Next","tagName":"h2"},{"title":"Create tenant-admin Role​","type":1,"pageTitle":"Realm Roles","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/realm-roles#create-tenant-admin-role","content":" Login to the Web Admin Console and navigate to the realm. Click on Realm roles on the side menu. Click on Create role button Enter the following for the role:  Role name: tenant-admin  Click on Save button to create the role. Click on the Action dropdown menu on the right hand side of the screen and select Add associated roles submenu. Check realm-management realm-admin from the list. Click on Assign button to assign selected roles to tenant-admin role. ","version":"Next","tagName":"h2"},{"title":"Securing Sensitive Endpoints","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/configuring-for-production/secure-sensitive-endpoints","content":"Securing Sensitive Endpoints The deployment manifests files in Quick Start section expose all endpoints to facilitate development but should be disabled for production deployment. PostgreSQL – port 30432 is exposed via NodePort which should be disabled in production or protected by firewall. Typical production deployment will have PostgreSQL deployed in a separate tier from the application tier, i.e., not in the same Kubernetes Cluster as the IAMS. IAMS-AAS – the REST APIs are intended to be invoked by backend services and not expose outside of Kubernetes. The Ingress Controller should not be created in production. The endpoints are:{{server}}/admin/* IAMS-Keycloak – Keycloak has numbers of endpoints and not all should be expose outside of Kubernetes. Depending on requirements, the following endpoints may or may not be exposed: Token Endpoint – this endpoint allows us to retrieve access token, refresh token, or id token. It is normally invoked by backend services and should not be exposed. However, depending on Mobile Client Authentication flow implementation, this endpoint may need to be exposed. The token endpoint is: {{server}}/realms/{{realm}}/protocol/openid-connect/token User Info Endpoint – this endpoint is used to retrieve user profile data using valid access token. It can also be used to validate access token. This endpoint is used by backend services but may be invoked by Mobile Client. The endpoint is: {{server}}/realms/{{realm}}/protocol/openid-connect/userinfo Token Introspect Endpoint – this endpoint is to verify that an access token is active or to retrieve more metadata about the access token. As with above endpoints, may need to be exposed for Mobile Client. The endpoint is: {{server}}/realms/{{realm}}/protocol/openid-connect/token/introspect Administration Endpoints – those endpoints are a set of REST APIs to perform administrative tasks. Those endpoints should be protected and shouldn’t be expose but as the Keycloak Web Admin Console Client (from browser) need to directly invoke those REST APIs, those endpoints need to be expose out of Kubernetes Cluster. As a good practice, the Keycloak Web Admin Console should not be expose out to the Internet. The endpoints are: {{server}}/admin/* ","keywords":"","version":"Next"},{"title":"Deployment Architecture","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/deployment-architecture","content":"Deployment Architecture This following figure depicts the deployment architecture after executing the deployment instructions:","keywords":"","version":"Next"},{"title":"Deploy PostgreSQL","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/deploy-postgresql","content":"Deploy PostgreSQL Navigate to the “postgres” directory.Create the config map by executing the following commands: kubectl apply -f 01-configmap.yaml The config map contains the followings configuration which will be used by step 5 to create a default database: POSTGRES_DB – the name of the database to createPOSTGRES_USER – the database user that has access to the database createdPOSTGRES_PASSWORD – the password of the database user note if you change the configuration values in this config map, you will also need to change the corresponding fields in IAMS-Keycloak deployment manifests file. Create persistent volumes: kubectl apply -f 02-pv.yaml This manifests file created a 10G bytes persistent volume backed by Kubernetes cluster host file system under /data/postgresql directory. Create persistent volume claims: kubectl apply -f 03-pvc.yaml This manifests file claimed a 10G bytes storage from the persistent volume created in step 3. Create Kubernetes deployment kubectl apply -f 04-deployment.yaml This manifests file will instruct Kubernetes to deploy a Pod using postgres version 16 image. Create service kubectl apply -f 05-service.yaml This manifests file creates a NodePort service at port 30432. You can access the PostgreSQL server using database administration tools, such as pgAdmin, via port 30432 on any of the Kubernetes Nodes.","keywords":"","version":"Next"},{"title":"Development Conventions","type":0,"sectionRef":"#","url":"/docs/docs/contributing/development/conventions","content":"","keywords":"","version":"Next"},{"title":"Naming Git Repositories​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#naming-git-repositories","content":" For all source code stored in Git repositories, services will typically namespace them under organizations, or projects (e.g. GitHub and GitLab respectively). This means we do not need to add additional prefixes for our repositories.  To keep things easy to remember and associate with naming in code, we shall name all repositories based on the abbreviation of the module they're for, and the repository description must continue the full module name at the beginning.  For example, for Unified Notification Hub the repository name shall be:  unh   and a sample description would be:  Unified Notification Hub - supports email, sms, push notifications, and customized notification channels.     ","version":"Next","tagName":"h2"},{"title":"Database​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#database","content":" ","version":"Next","tagName":"h2"},{"title":"Naming schemas​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#naming-schemas","content":" Services within our system might have to share database with other external services. The deployment configuration is highly dependent on the project the system is ultimately deployed to. Because of this, all schemas shall be prefixed with aoh_ to avoid clashes in naming:  aoh_[repository name]   Example for a service named Unified Notification Hub with the repository abbreviation unh, the schema name shall be:  aoh_unh   See Also: Git Repository Naming    ","version":"Next","tagName":"h3"},{"title":"Naming tables​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#naming-tables","content":" Tables should be named after the entity they represent, in singular form (so a table representing notifications should be called notification)  warning That's notification WITHOUT the &quot;S&quot;!    ","version":"Next","tagName":"h3"},{"title":"Naming views​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#naming-views","content":" Views shall be prefixed with v_, this allows us to clearly see which tables are views. Database management UI's would also typically sort tables by name, allowing all views to be grouped together neatly.  v_[view name]   Example:  v_notification_template   note To be discussed: naming materialized views    ","version":"Next","tagName":"h3"},{"title":"Naming association tables​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#naming-association-tables","content":" Association tables are commonly used to map the relationship between entities, especially for many-to-many relationships. We'll use the simple, common covention of concatenating the two tables names and suffixing _mappingbehind.  [table 1]_[table 2]_mapping   Example:  user_notification_mapping     ","version":"Next","tagName":"h3"},{"title":"Naming the database user​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#naming-the-database-user","content":" Direct database access for each service is via a unique database account. We do this instead of having a shared account for the purpose having the ability to apply access control between services. We'll also have to namespace this user account as we might share the database with other external services (as is often the case with projects being deployed into existing infrastructure)  aoh_[repo name]_user   Example:  aoh_unh_user   See Also:  Git Repository NamingNaming Schemas    ","version":"Next","tagName":"h3"},{"title":"Mandatory database columns​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#mandatory-database-columns","content":" The following fields (database columns) are mandatory and must be NOT NULLABLE.  Name\tType\tDefault\tDescriptionid\tuuid\tgen_random_uuid()\tWe use a uuid as a primary key for all tables as a best practice. Having universally unique synthetic primary keys (as opposed to sequential ints) allows us to avoid a large class of common errors that might arise from unintended references. It is also better from a security perspective. If your module requires human-readable ID's, create another column for it and assign a unique constraint across that column + tenant_id. created_at\ttimestamp with timezone\tnow()\tThe UTC timestamp when this record was first created updated_at\ttimestamp with timezone\tnow()\tThe UTC timestamp when this record was last updated, use database triggers created_by\ttext The reference to the user who created this record - do not apply database constraints updated_by\ttext The reference to the user who last modified this record - do not apply database constraints tenant_id\ttext The reference to the tenant this row belongs to - do not apply database constraints occ_lock\tint\t0\tThis integer must match on all update queries to ensure the user is not trying to update a row with outdated data (because another user might have already updated it, changing the number). The purpose is for optimistic currency control but you can think of this as a version number.  note We only apply database constraints (e.g. foreign key constraints) when the reference is within the same service (schema). This is to avoid coupling the services at the database level.  PostgreSQL-specific types and functions Some of the types and functions (like gen_random_uuid) might be a PostgreSQL-specific function. This will have to vary based on your actual database product - use the appropriate alternatives instead.  To Be Decided: Discussion required to determine if the soft delete capability is required, and if the implementation should be via extra fields or using archive tables.    ","version":"Next","tagName":"h3"},{"title":"Golang Mandatory tables​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#golang-mandatory-tables","content":" Modules should be separated by their schema, and within their own schemas, they should have a dedicated module_infotable. This table exists for services to be able to query the current state of the schema, as well as any other information that might be useful for services. tenant_id, created_by and updated_by is not required for this table.  Module Info Table:​  Table Name module_info   This table is designed to arbitrary store key values pairs.  Table Schema:​  Name\tType\tDefault\tDescriptionid\tuuid\tgen_random_uuid()\tMandatory field, see mandatory database colunmns created_at\ttimestamp\tnow()\tMandatory field, see mandatory database colunmns updated_at\ttimestamp\tnow()\tMandatory field, see mandatory database colunmns occ_lock\tint\t0\tMandatory field, see mandatory database colunmns key\ttext The key of the pair - NOT NULL, UNIQUE value\ttext\tNULL\tThe value of the pair comment\ttext\tNULL\tA descriptive comment about the key-value pair  Recommended Initial Data:​  Key\tValue\tComment\tDescriptionINITIAL_SCHEMA_VERSION\t0.0.1\tinitial schema version\tInitial schema version for the service to check against CURRENT_SCHEMA_VERSION\t0.0.1\tcurrent schema version\tCurrent schema version for the service to check against INITIAL_APPLICATION_VERSION\t0.0.1\tinitial app version\tCurrent application version populated by the service if the row doesn't exist CURRENT_APPLICATION_VERSION\t0.0.1\tcurrent schema version\tCurrent application version, upserted by the service DEPLOYMENT_TIME\t2024-08-12T18:32:42\tthe time this service was started\tThe time when the service was run  Note that the values above will not be accurate when multiple replicas of a service is being run. In such a scenario, Kubernetes will have to be the source of truth. Otherwise, these fields are useful for the developers during development time - the particularly important keys are the SCHEMA_VERSION, which must be checked against before the application proceeds.    ","version":"Next","tagName":"h3"},{"title":"API, Routes, and Endpoints​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#api-routes-and-endpoints","content":" ","version":"Next","tagName":"h2"},{"title":"Kubernetes Liveness and Readiness probes​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#kubernetes-liveness-and-readiness-probes","content":" All services must provide a liveness and readiness endpoint for Kubernetes to call. As a rule, we'll uselivez and readyz at the root path, which is common with Google's internal practices. However, certain frameworks might already provide such endpoints for the same purpose, requiring you to utilize their naming scheme; in such a scenario, you should stick with the framework's built-in endpoints (e.g. SpringBoot's /actuator/health/liveness &amp;/actuator/health/readiness).  Liveness Endpoint: /livezReadiness Endpoint: /readyz    ","version":"Next","tagName":"h3"},{"title":"Web Pages​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#web-pages","content":" The routing of web pages shall also be namespaced - first by the project, then by the module:  /[project]/[module]/...   Examples:  View all incidents page: /aoh/incidents/View specific incident: /aoh/incidents/inc-20240607-0001Dashboard page: /aoh/dashboard?name=My+First+Dashboard    ","version":"Next","tagName":"h3"},{"title":"Data Response Format​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#data-response-format","content":" The following is a recommended response format for all your API responses. These are general guidelines and might not apply to all cases. For example, this should be the structur of your HTTP response body, or messages sent through Kafka etc.  Key\tType\tOptional\tDescriptiondata\tobject or array\tyes\tThe actual response data for the request. message\tstring\tyes\tAn accompanying message for additional information or debugging. sent_at\tstring formatted as ISO8601\tyes\tThe time this message was sent. errors\tarray of { message: string }\tyes\tAn aggregation of errors if error feedback is necessary.  Example response body { data: {...}, // arbitrary format - recommend array for lists, object for individual records message: &quot;...&quot;, // string sent_at: &quot;&quot;, //iso8601 errors: [ { message: &quot;....&quot;, // string ... } ] }     ","version":"Next","tagName":"h3"},{"title":"Pagination​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#pagination","content":" Querying for entities almost always requires pagination to pull data effectively (applications should almost never pull an entire table of data).  Since pagination is a very common requirement, and different frameworks in different languages might support different out-of-the-box implementations of pagination, we have to allow for some API to be flexible. However, the guidelines to follow for paginated endpoints should be:  Pagination arguments should be URL query paramsRequests: page, size and sort should be supported page specifies what page the caller is currently at (the offset cursor) - only positive integers starting from 1size specifies how many rows are in each page - only positive integers starting from 1sort is a list of tuples containing field and direction The field refers to the column to sort byThe direction refers to whether the sort is ascending or descending ASC for ascendingDESC for descending (default) Response: data, page: { number }, page: { size }, and page: { total_records } information should be returned data is the result of the querynumber is the current page of responsesize is the number of records per pagetotal_records is the total number of records in the tablecount is an optional field, specifying the number of elements in the current pagesort is an optional field, specifying the sort parameters used for to retrieve current page  Request Pagination Params​  Name\tType\tDefault\tDescriptionpage\tint\t1\tThe number of elements in each page. Invalid values will be ignored and default will be used size\tint\t10\tThe current page - pages start at 1. Invalid values will be ignored and default will be used sort\tstring,string\tcreated_at,DESC\tA list of sort columns and direction. Invalid values will be ignored and default will be used  Response Pagination Data​  Name\tType\tOptional\tDescriptionpage: { number }\tint\tno\tThe current page - pages start at 1 page: { size }\tint\tno\tThe number of elements in each page page: { total_records }\tint\tno\tThe number of records in each page page: { count }\tint\tyes\tThe number of elements in the current page page: { sort }\tstring,string\tyes\tThe sort column and direction  These fields need not strictly follow the same name (due to possible framework limitations), but the pagination API should follow the specs listed above. The page number, page size, and total_records are typically required by the caller in order to understand where the cursor is in relation to the rest of the table. If the API allows specifying the sort parameters, sort should be returned as well.  Example paginated API call:  Example request - page 1, default page size, and default sort example.agilopshub.com/user   Example request - page 3, 2 records per page, default sort example.agilopshub.com/user?page=3&amp;size=2   Example request - page 3, 2 records per page, sort by username, descending example.agilopshub.com/user?&amp;page=3&amp;size=2&amp;sort=username   Example request - page 3, 2 records per page, sort by email - descending, then sort by username - ascending example.agilopshub.com/user?page=3&amp;size=2&amp;sort=email,desc&amp;sort=username,asc   Example response { &quot;data&quot;: [ { &quot;username&quot;: &quot;coolguy&quot;, &quot;email&quot;: &quot;iamcool@gmail.com&quot; }, { &quot;username&quot;: &quot;example&quot;, &quot;email&quot;: &quot;example@gmail.com&quot; }, ], &quot;page&quot;: { &quot;number&quot;: 3, &quot;size&quot;: 2, &quot;total_records&quot;: 35, &quot;sort&quot;: &quot;username,asc&quot; } ... }   Example response 2 { &quot;data&quot;: [ { &quot;username&quot;: &quot;a coolguy&quot;, &quot;email&quot;: &quot;zzz@gmail.com&quot; }, ], &quot;page&quot;: { &quot;number&quot;: 4, &quot;size&quot;: 3, &quot;total_records&quot;: 10, &quot;count&quot;: 1, &quot;sort&quot;: [ &quot;username,desc&quot;, &quot;email,asc&quot; ] } ... }     ","version":"Next","tagName":"h3"},{"title":"Log levels​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#log-levels","content":" Logging is covered in the Logging &amp; Exception Handlingguidelines section.  As a quick rule of thumb, use DEBUG log level to trace inputs (and potentially outputs) to function calls that are useful for debugging the system, and leave these logs there. For INFO log level, use it to trace key events in the system to indicate it is functionally correctly (such as a successful database connection event).    ","version":"Next","tagName":"h2"},{"title":"Container Image Tagging​","type":1,"pageTitle":"Development Conventions","url":"/docs/docs/contributing/development/conventions#container-image-tagging","content":" Our standard for tagging built container images follows a version number it is working towards, followed by an optional short tag to represent its purpose, followed by a dash and the git commit hash of the source repo:  Container Tag Regex /v[\\d]+.[\\d]+.[\\d]+(dev|beta|rc)-[\\da-f]{7}/   Example Tag:  v1.12.0rc-710093d   The latest release image must also always be tagged with latest.  latest   The latest development release image must also always be tagged with latest-dev.  latest-dev   The images should also follow their repository structure - because we use monorepos and keep source code for each repository together, their tags should include the short name of the module, followed the short name of the module again, and the sub-name for the container in the module. This is required so that GitHub can properly segregate different containers linked to the same repository.  Example latest-dev tags for the IMS module's app and web containers:  Latest Development images for the IMS module ghcr.io/mssfoobar/ims/ims-app:latest-dev ghcr.io/mssfoobar/ims/ims-web:latest-dev   Full example images and their tags:  Latest ghcr.io/mssfoobar/unh/unh-app:latest   Latest Development Image ghcr.io/mssfoobar/unh/unh-app:latest-dev   Released v3.0.1 ghcr.io/mssfoobar/unh/unh-app:v3.0.1   Release candidate for v4.0.0 ghcr.io/mssfoobar/unh/unh-app:v4.0.0rc   Developer build for v12.13.14 ghcr.io/mssfoobar/unh/unh-app:v12.13.14dev-78cdaeb   Refer to the post on versioning scheme for more information on the version numbers. ","version":"Next","tagName":"h2"},{"title":"Browser Login using OpenID Connect (Authentication Flow)","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/authentication/browser-login","content":"","keywords":"","version":"Next"},{"title":"Configuration​","type":1,"pageTitle":"Browser Login using OpenID Connect (Authentication Flow)","url":"/docs/docs/modules/iams/development/authentication/browser-login#configuration","content":" For Web App Backend to handle the login flow, the following settings need to be configured in the .env file:  IAM_URL – this is the URL to the Keycloak. The URL is in the following format:https://&lt;keycloak Server URL&gt;/realms/&lt;realm name&gt;IAM_CLIENT_ID – of the Web App Backend at Keycloak.IAM_CLIENT_SECRET – the client secret for the Web App Backend to authenticate with Keycloak.  The following steps register Web App Backend with Keycloak:  Login to Keycloak Admin Console and switch to the realm. Click on Clients in the side menu    Click on Create client button    Fill up the following field in the Create client form:  Client type – OpenID ConnectClient ID – unique id to identify the Web App Backend. To be same as IAM_CLIENT_ID  Click on Next button:    Click on Client authentication checkbox to turn it on    Click on Next button. Add the redirect URL to the Valid redirect URIs field. The redirect URL is the URL that you want Keycloak to redirect user to after user has successful login to Keycloak. Keycloak accept wildcard character in the URI.    Add the Web origins that is allowed for the redirect.    Click on Save button to complete the creation.    To obtain the client secret for IAM_CLIENT_SECRET setting, click on the Credentials tab    Click on the Eye icon to reveal the secret string in the Client Secret   ","version":"Next","tagName":"h2"},{"title":"Required Actions","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/required-actions","content":"","keywords":"","version":"Next"},{"title":"Enable Select Active Tenant Required Action​","type":1,"pageTitle":"Required Actions","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/required-actions#enable-select-active-tenant-required-action","content":" The steps to enable the required action is as follows:  Login to the Keycloak Web Console. Switch to the realm if not already there.    Click on Authentication in the side menu.    Click on the Required actions tab.    Toggle the Select active tenant required action to On.   ","version":"Next","tagName":"h2"},{"title":"Browser Logout using OpenID Logout Flow","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/authentication/browser-logout","content":"Browser Logout using OpenID Logout Flow AGIL Ops Hub Web Framework also implement the OpenID Logout Flow to allow user to logout from Application. The following flow chart depicts how the Logout Flow work: ::: Note AGIL Ops Hub Web Framework currently does not support Single Logout. Will implement this feature in future release. ::: User initiate the logout request to Web App Backend. Web App Backend then redirect the user to the Keycloak server with the following information: client_id – the client id of the Web App Backend at Keycloakpost_logout_redirect_uri – the URL that Keycloak should redirect user back to after logoutid_token_hint – ID token of the user. Retrieve from the user cookies stored during login. Keycloak will then validate that id_token_hint and post_logout_redirect_uri are valid. If valid, Keycloak will terminate the user session and redirect user back to the specified post_logout_redirect_uri. Web App Backend will instruct browser to delete the cookies that store the id and access tokens.","keywords":"","version":"Next"},{"title":"Deploy IAMS-AAS","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/deploy-iams-aas","content":"Deploy IAMS-AAS Navigate to the “iams-aas” directory. Create Service kubectl apply -f 01-service.yaml This manifests file creates a ClusterIP services for accessing IAMS-AAS pod. Create the Deployment kubectl apply -f 02-deployment.yaml This manifests file will instruct Kubernetes to deploy a Pod using iams-aas image from AGIL Ops Hub container registry. Edit 03-ingress.yaml and change the value of the Host field to the actual FQDN of your Kubernetes Cluster. The example value in the 03-ingress.yaml is using nip.io to create a FQDN to point to the Kubernetes Cluster. To use nip.io, simply replace 10.10.10.100 in the example value with the Ipv4 address of your Kubernetes Cluster. Create ingress kubectl apply -f 03-ingress.yaml ","keywords":"","version":"Next"},{"title":"Token Claims","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/token-claims","content":"","keywords":"","version":"Next"},{"title":"Configure the Mapper​","type":1,"pageTitle":"Token Claims","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/token-claims#configure-the-mapper","content":" You can enable the mapper to the individual client or at the Client Scopes.  If you enable the mapper at the Client Scopes, any new client created after that will inherit the mappers and do not need require further configuration.  The default realm created in the Quick Start has the mapper configured in Client Scopes.  ","version":"Next","tagName":"h2"},{"title":"Enable at Keycloak Client​","type":1,"pageTitle":"Token Claims","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/token-claims#enable-at-keycloak-client","content":" To enable the 2 mapper for a Keycloak Client, perform the following steps:  Login to Keycloak Admin Console and switch to the realm. Click on Clients in the side menu    Click on the Client ID of the client to configure. Click on the Client scopes tab.    Click on the client scope with the name ended with dedicated.    Click on Add mapper and select By configuration.    Click on Active tenant    Enter the following fields:  Name: active_tenantToken Claim Name: active_tenantTurn on Add to lightweight access token checkbox  Click on Save to add the mapper.    Click Cancel to go back to the Mappers list. You should see active_tenant mapper in the list.    Click on Add mapper and select By configuration. Click on All tenants    Enter the following fields:  Name: all_tenantsToken Claim Name: all_tenantsTurn on Add to lightweight access token checkbox  Click on Save to add the mapper.    ","version":"Next","tagName":"h3"},{"title":"Enable at Client Scopes​","type":1,"pageTitle":"Token Claims","url":"/docs/docs/modules/iams/configuration/manual-keycloak-realm-setup/token-claims#enable-at-client-scopes","content":" As mentioned, enable the mappers in Client Scopes will allow any newly created client to automatically inherit the mappers.  Follows the following steps to enable mapper at the Client Scopes:  Login to the Web Admin Console and navigate to the realm. Click on Client scopes in the side menu:    Click on Create client scope:    Enter the followings:  Name: any prefer nameType: DefaultProtocol: OpenID ConnectTurn off the Display on consent screen checkbox  Click on Save to create the client scope.    Click on Mappers tab    Click on Configure a new mapper button:    Click on Active tenant    Enter the following fields:  Name: active_tenantToken Claim Name: active_tenantTurn on Add to lightweight access token checkbox  Click on Save to add the mapper.    Click Cancel to go back to the Mappers list. You should see active_tenant mapper in the list.    Click on Add mapper and select By configuration.    Click on All tenants    Enter the following fields:  Name: all_tenantsToken Claim Name: all_tenantsTurn on Add to lightweight access token checkbox  Click on Save to add the mapper.   ","version":"Next","tagName":"h3"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/authentication/overview","content":"Overview IAMS uses Keycloak, open-source identity and access management system, to handle the authentication. In this section, we will run through some of the common patterns and settings for authentication configuration. For more information and explanation on other Keycloak features, please refer to official Keycloak documentation at the following links: https://www.keycloak.org/documentation","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/multi-tenancy","content":"","keywords":"","version":"Next"},{"title":"Separate Realm for Each Tenant​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/iams/development/multi-tenancy#separate-realm-for-each-tenant","content":" In this setup, a separate realm is created for each tenant. Each tenant will have its own realm configuration i.e., users, themes, clients, roles, etc.  This setup is useful where each tenant is totally separated from other tenants and have no shared relationship among them.  User account in one tenant cannot be assigned to access another tenant.  Same username can appear in 2 tenants but they are totally different accounts and have no linkage between them. Each user account will have its own login credential.  note It is possible to link accounts in 2 different realms to the same Active Directory. By doing so, the account with same username will have the same login credential. But from Keycloak perspective, they are treated as different user accounts.  To use this setup to implement multitenancy, the shared application need to be able to handle multiple realms. All the OpenID endpoint URLs will be different as the Realm name is part of the URL. For example, the endpoint URL to retrieve user info is:  /realms/&lt;realm-name&gt;/protocol/openid-connect/userinfo   The shared application will need to know the Realm that the user is accessing and use the correct OpenID endpoints.  One possible approach is to use different FQDN for each tenant to access the same application. For example, tenant A will use http://tenantB.myapp.com to access the application and tenant B will use http://tenantB.myapp.com to access the same application.  The shared application can then use the requesting URL to determine the correct realm that the user is trying to access.  Currently, the AGIL Ops Hub Web Framework doesn’t support multiple realm. To work around this limitation, project can deploy distinct Web App, one for each tenant.  ","version":"Next","tagName":"h2"},{"title":"Single Realm for All Tenants​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/iams/development/multi-tenancy#single-realm-for-all-tenants","content":" In this setup, all tenants reside within same realm. All tenants will share the same realm configuration but each tenant can have their own roles, group, resource, and permission configuration.  This setup is useful for scenarios where tenants are all belong to same organization group but each tenant would want to manage their own application permissions. Each user account in this setup must be unique across all tenants and each user can be assigned to one or more tenant, allowing them to access different resources in different tenants.  For example, each company in ST Engineering (i.e., MSS, ANS, T&amp;S, etc.) can be a tenant of the application, where each have its own management of roles, groups, resource, and permission. A HR staff can be assigned to join multiple tenants to help those tenants in their HR matters. The HR staff will only have one user account in the application but will be able to access different tenants and perform different function depending on the permission granted in those tenants.  To support this setup, IAMS extended Keycloak to add Tenant concept to the realm, allowing each realm to have its own unique roles, groups, and access control settings.  note If you are don’t intent to support multitenancy within the same Realm, you will still need to create at least a Tenant to make use of IAMS authorization mechanism. ","version":"Next","tagName":"h2"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/contributing/documentation/reference/faq","content":"","keywords":"","version":"Next"},{"title":"How should I name my folders and markdown files?​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#how-should-i-name-my-folders-and-markdown-files","content":" All folder names and markdown files should be in kebab-case. Do not use numbers to order the folders in the folder-browser. Keep the name of the folder similar to the title of the text in the sidebar that it represents so other users can find the folder if they need to.  We don't use numbers to order them as it causes a lot of broken links when we attempt to do any re-ordering later.  Use the sidebar_order and sidebar_label front matter to control their position and label in the sidebar.  ","version":"Next","tagName":"h2"},{"title":"Where should I put my images?​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#where-should-i-put-my-images","content":" Store your images in the static/img/** folder. You should create folders that match where your docs are if have many images (usually as part of a tutorial or guide) that are meant to be in a specific section/module.  We don't co-locate the images in with the docs they are used as they sometimes need to be shared. We'll keep things simple by having them all in the static/img/** folder.  ","version":"Next","tagName":"h2"},{"title":"How should I name my images?​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#how-should-i-name-my-images","content":" Use kebab-case to name your images, give them a descriptive name as there will be many images in the docs, making it difficult for you to find your image if your image name and description is too general.  ","version":"Next","tagName":"h2"},{"title":"How should I show placeholder text in commands?​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#how-should-i-show-placeholder-text-in-commands","content":" We have a class called highlight in the Docusaurus custom.css file that is meant to be used to highlight placeholder text - that is text that the reader is meant to replace with their own values.  In order to be able to use this in code blocks, you can't use the usual triple backtick ``` syntax for code blocks, you should use the &lt;CodeBlock&gt; component instead - an example is given below:  Actual Code:  CodeBlock jsx &lt;CodeBlock language=&quot;bash&quot;&gt; {`$ echo `} &lt;span className=&quot;highlight&quot;&gt;TOKEN&lt;/span&gt; {` | docker login ghcr.io -u `} &lt;span className=&quot;highlight&quot;&gt;USERNAME&lt;/span&gt; {` --password-stdin`} &lt;/CodeBlock&gt; - Replace &lt;span className=&quot;highlight&quot;&gt;TOKEN&lt;/span&gt; with your token - Replace &lt;span className=&quot;highlight&quot;&gt;USERNAME&lt;/span&gt; with your username   Render Output:  $ echo TOKEN | docker login ghcr.io -u USERNAME --password-stdin  Replace TOKEN with your tokenReplace USERNAME with your username  ","version":"Next","tagName":"h2"},{"title":"Gists​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#gists","content":" If necessary and separate from the FAQ section, you can provide lists of commands, code recipes etc. here for users to copy-paste. These should be very short sections.  The gists below is a set of useful Docusaurus references to help you with writing your documentation for Docusaurus and is itself a good example of what gists should be.  ","version":"Next","tagName":"h2"},{"title":"Docusaurus Reference​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#docusaurus-reference","content":" The following is a list of useful examples for reference when editing the markdown files on Docusaurus.  note Formatting is very important for MDX: https://github.com/facebook/docusaurus/issues/3890You must follow the formatting somewhat strictly (especially excluding spaces at the start of the sentence).  ","version":"Next","tagName":"h2"},{"title":"Images​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#images","content":"   Code to render above example:  ![Svelte Logo](/img/sample.png)   ","version":"Next","tagName":"h3"},{"title":"Tabs​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#tabs","content":"     WindowsmacOSLinux shutdown -t 0 -s   Code to render above example:  &lt;!-- You must import the React components --&gt; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; &lt;Tabs&gt; &lt;TabItem value=&quot;Windows&quot; label=&quot;Windows&quot; default&gt; ```bash shutdown -t 0 -s ``` &lt;/TabItem&gt; &lt;TabItem value=&quot;macOS&quot; label=&quot;macOS&quot;&gt; ```bash sudo shutdown -h now ``` &lt;/TabItem&gt; &lt;TabItem value=&quot;Linux&quot; label=&quot;Linux&quot;&gt; ```bash sudo shutdown -h now ``` &lt;/TabItem&gt; &lt;/Tabs&gt;   ","version":"Next","tagName":"h3"},{"title":"Codeblocks​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#codeblocks","content":" /src/components/HelloCodeTitle.js function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; }   Code to render above example:  ```jsx title=&quot;/src/components/HelloCodeTitle.js&quot; function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; } ```   ","version":"Next","tagName":"h3"},{"title":"Equations​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#equations","content":" $$ I = \\int_0^{2\\pi} \\sin(x),dx $$  Code to render above example:  $$ I = \\int_0^\\{2\\pi\\} \\sin(x)\\,dx $$   ","version":"Next","tagName":"h3"},{"title":"Admonitions​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#admonitions","content":" note Some content with Markdown syntax. Check this api.  tip Some content with Markdown syntax. Check this api.  info Some content with Markdown syntax. Check this api.  caution Some content with Markdown syntax. Check this api.  danger Some content with Markdown syntax. Check this api.  Code to render above examples:  :::note Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::tip Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::info Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::caution Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::danger Some **content** with _Markdown_ `syntax`. Check [this `api`](#). :::   ","version":"Next","tagName":"h3"},{"title":"Line Highlighting​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#line-highlighting","content":" function HighlightSomeText(highlight) { if (highlight) { return &quot;This text is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; } function HighlightMoreText(highlight) { if (highlight) { return &quot;This range is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; }   Code to render above example:  ```js function HighlightSomeText(highlight) { if (highlight) { // highlight-next-line return &quot;This text is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; } function HighlightMoreText(highlight) { // highlight-start if (highlight) { return &quot;This range is highlighted!&quot;; } // highlight-end return &quot;Nothing highlighted&quot;; } ```   ","version":"Next","tagName":"h3"},{"title":"Line Numbering​","type":1,"pageTitle":"FAQ","url":"/docs/docs/contributing/documentation/reference/faq#line-numbering","content":" import React from &quot;react&quot;; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent;   Code to render above example:  ```jsx {1,4-6,11} showLineNumbers import React from &quot;react&quot;; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; ```  ","version":"Next","tagName":"h3"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/ian/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/ian/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Operating Concepts","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts","content":"","keywords":"","version":"Next"},{"title":"Domain Model​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#domain-model","content":" The domain model of IAMS revolves around the following key entities:  Realm – A top-level entity in IAMS. Realms can be used to organize tenants, users, roles, group, and resources.Tenant – A separate entity or organization within a realm. Each tenant has its own set of roles, groups, and resources.User – An individual in a realm. Users can have membership in multiple tenants and assigned multiple roles within the tenant he/she is member of.Role – A role identifies a type or category of user. Admin, user, manager, and employee are all typical roles that may exist in a tenant. Applications often assign access and permissions to specific roles rather than individual users as dealing with users can be too fine-grained and hard to manage.Group – A collection of users within a tenant. Group can be used to assign roles or permissions to multiple users at once.Resource – A resource represents an entity or object within the system that can be accessed or managed by users with the appropriate roles or permissionsScope – A scope is a bounded extent of access that is possible to perform on a resource. It usually indicates what can be done with a given resource. An example of some common scopes are view, edit, delete etc.  ","version":"Next","tagName":"h2"},{"title":"Entity Descriptions​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#entity-descriptions","content":" ","version":"Next","tagName":"h2"},{"title":"Realm​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#realm","content":" Description​  A realm represents a top-level entity that owns multiple tenants. Realms can be used to organize tenants, users, roles, and resources.  Realm need to be created in Keycloak. It can be created via Keycloak Admin Console.  Realm name is unique within Keycloak.  As part of the deployment steps in Quick Start, a default Realm named AOH will be created.  Relationships​  Multiple Realms can be created for each application. But typically only one is created unless you are implementing Separate Realm for Each Tenant approach for multitenancy.Each realm can have one or more tenants.Each realm can have users and users are unique across realm.  ","version":"Next","tagName":"h3"},{"title":"Tenant​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#tenant","content":" Description​  A tenant represents a separate entity or organization within a realm. Each tenant has its own set of roles, group, and resources.  A tenant name is unique within a realm.  Relationships​  A tenant belongs to one realm.A tenant is associated with one or more roles, groups, and resources.  ","version":"Next","tagName":"h3"},{"title":"User​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#user","content":" Description​  A user is an individual who belongs to a realm. User can be a member of one or more tenants within the realm.  Username is unique within a realm.  Users can have multiple roles within a tenant and can belong to multiple groups within a tenant.  User can be granted with scoped access to one or more resources from the tenant that he is member of (i.e., User-based Access Control).  Relationships​  A user belongs to one realm.A user can be member of one or more tenants from the realm he belongs to.A user can be associated with one or more roles from the tenant that he is member of.A user can be associated with one or more groups from the tenant that he is member of.A user can be granted with scoped access to one or more resources from the tenant that he is member of.  ","version":"Next","tagName":"h3"},{"title":"Role​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#role","content":" Description​  A role identifies a type or category of user. Admin, user, manager, and employee are all typical roles that may exist in a tenant. Applications often assign access and permissions to specific roles rather than individual users as dealing with users can be too fine-grained and hard to manage.  Role name is unique within a tenant. Different Roles can exist in different tenants that have the same name.  A tenant can have one or more roles and roles can be granted with scoped access to one or more resources from the tenant it belongs to (i.e., Role-based Access Control).  Roles can be assigned to users or groups.  Relationships​  A role belongs to one tenant.A role can be assigned to one or more users or groups within a tenant.A role is can be granted with scoped access to one or more resources from the tenant that it belongs to.  ","version":"Next","tagName":"h3"},{"title":"Group​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#group","content":" Description​  A is a collection of users within a tenant. Groups can be used to assign roles or permissions to multiple users at once.  A group can have sub-groups. Name of the group must be unique within siblings.  A group can be assigned to one or more roles.  A group can be granted with scoped access to one or more resources from the tenant it belongs to (i.e., Group-based Access Control).  Relationships​  A group belongs to one tenant.A group can have subgroups.A group can be assigned with one or more users within a tenant.A group can be granted with scoped access to one or more resources from the tenant it belongs to.  ","version":"Next","tagName":"h3"},{"title":"Resource​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#resource","content":" Description​  A resource represents an entity or object within the tenant that can be accessed or managed by users with the appropriate roles or permissions.  A resource name must be unique within the tenant.  A resource has one or more scopes that are used to represent scoped access to a resource that can be granted to user, roles, or group within the tenant.  Relationship​  A resource belongs to one tenant.A resource is associated with one or more scopes.  ","version":"Next","tagName":"h3"},{"title":"Scope​","type":1,"pageTitle":"Operating Concepts","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/operating-concepts#scope","content":" Description​  A scope is a bounded extent of access that is possible to perform on a resource. It usually indicates what can be done with a given resource. Example of scopes are view, edit, delete, and so on.  However, scope can also be related to specific information provided by a resource. In this case, you can have an Incident resource and a eSOP scope, where the eSOP scope is used to define specific permissions for users to access the eSOP information of an Incident.  A scope belongs to a tenant and its name must be unique within the tenant.  A scope can be associated with one or more resources in the same tenant.  Relationships​  A scope belongs to one tenant.A scope is associated with one or more resources.A scope and resource pair formed a permission that can be assigned to users, groups, or roles. ","version":"Next","tagName":"h3"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/overview","content":"","keywords":"","version":"Next"},{"title":"Features​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/iams/overview#features","content":" Identity &amp; Access Management Service (IAMS) module provides the following features:  Authentication Single-Sign On and Single-Sign OutOpenID Connect a.k.a. OIDC SupportWeb browser login and logout supportIdentity Brokering Authenticate with external OIDC or SAML Identity Providers. Sync users from LDAP and Active Directory serversTwo-factor Authentication (2FA) Support for TOTP/HOTP via Google Authenticator or FreeOTP. Multi-tenancy supportAuthorization Role-based access control (RBAC)Group-based access control (GBAC)User-based access control (UBAC)  IAMS consists of the followings:  Customized Keycloak (iams-keycloak) to support multi-tenancyAuthorization and Admin Service (IAMS-AAS) that provides APIs to manage authorization access.Database to store configurations. The following database types are supported: PostgreSQLMS SQLOracle ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/authorization","content":"","keywords":"","version":"Next"},{"title":"Basic Authorization​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/iams/development/authorization#basic-authorization","content":" In Basic Authorization, roles are used to grant access to the Application.  Roles are created and assigned to Users. Application permit user access to specific functionality based on the roles that are assigned to User. For example, Timesheet Application can have 2 roles; Manager and Employee. Employee will be able to fill and submit timesheet while the Manager will be able to approve the timesheet submitted.  The checking of role will be based on the roles listed in the active_tenant claim in Access Token.  Typical flow for basic authorization is as follows:    User make a request to Web App Backend. Web App Backend check whether user has Access Token in cookies. If Access Token is not present, user will be denied access and redirect to Keycloak for authentication. If Access Token is present, Web App Backend will then need to validate the Access Token to ensure it is valid. To check for validity of the Access Token, Web App Backend can use the Online or Offline Validation. SeeOnline Validation andOffline Validation for more information on Access Token Validation. Once determine that the access token is valid, Web App Backend will then retrieve the roles that are assigned to user from the Access Token. Based on the roles, Web App Backend will determine whether user has the rights to perform the request. If user doesn’t have the required role, he will be directed to error page. If has the correct role, Web App Backend can then direct the request to the Backend Services.  ","version":"Next","tagName":"h2"},{"title":"Retrieve Roles from Access Token​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/iams/development/authorization#retrieve-roles-from-access-token","content":" To access the roles granted to user from the Access Token, you will need to perform the followings:  Split Access Token into Sections; Header, Payload, and SignatureDecode the payload of Access Token to JSON stringRetrieve the roles from the active_tenant claim in the JSON string.  Please refer to Split Access Token into Sectionand Check Issue Date and Expiry Dateon how to split Access Token and decode the payload into JSON string.  After decoded the access token, you should get a JSON string with structure similar to the followings:  { &quot;exp&quot; : 1721289932, &quot;iat&quot; : 1721289632, &quot;auth_time&quot; : 1721289347, &quot;jti&quot; : &quot;a0b2a1fc-cdd8-43a4-9e95-627defac350b&quot;, &quot;iss&quot; : &quot;http://192.168.6.44:8080/realms/AOH&quot;, &quot;aud&quot; : [ &quot;realm-management&quot;, &quot;account&quot; ], &quot;sub&quot; : &quot;b25aa224-21b2-40d7-a735-3d5f10f99146&quot;, &quot;typ&quot; : &quot;Bearer&quot;, &quot;azp&quot; : &quot;web-app&quot;, &quot;sid&quot; : &quot;9774505b-835e-4729-b1d1-f11920c9284a&quot;, &quot;acr&quot; : &quot;0&quot;, &quot;allowed-origins&quot; : [ &quot;http://localhost:3000&quot; ], … … &quot;preferred_username&quot; : &quot;tbs&quot;, &quot;given_name&quot; : &quot;T&quot;, &quot;active_tenant&quot; : { &quot;tenant_id&quot; : &quot;8ce3c8f2-98fa-4d06-b51c-1724c86e53aa&quot;, &quot;tenant_name&quot; : &quot;test 2&quot;, &quot;roles&quot; : [ &quot;tenant-admin&quot;, &quot;Manager&quot;, &quot;Content Creator&quot; ] }, &quot;family_name&quot; : &quot;BS&quot;, &quot;email&quot; : &quot;tbs@test.com&quot; }   To have the active_tenant claim available in the Access Token, Select Active Tenant required action andActive Tenant Mapper need to be configured in the Realm.  Please refer to Enable Active Tenant Required Actionand Configure Active Tenant Mapperon how to configure the Select Active Tenant required action and Active Tenant Mapper.  ","version":"Next","tagName":"h3"},{"title":"Fine-grained Access Control​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/iams/development/authorization#fine-grained-access-control","content":" In fine-grained access control, resource and scope are used to grant access to the application.  Resource are created to represents protected entity or object within the Application, and scope are the action allowed to be performed on the protected resource.  Permission to perform specific scoped action on a protected resource is then granted to user, via role assigned to user, via group associated with user, or direct granting the permission to user.  IAMS provides the necessary APIs to evaluate whether user has been granted specified permission (specific scoped action on specific resources) and also all the scoped actions that user is permitted on specific resource. The latter is useful in UI rendering to determine whether certain buttons need to be show or hide based on the scoped actions permitted to the user.  For fine-grained access control, IAMS supports:  User-based Access Control (UBAC) – user is directly granted specific permission (specific scoped action on specific resources). Role-based Access Control (RBAC) – the permission is granted to the role(s) such that users assigned with those roles will be allowed to perform the scoped action on the resource. By default, roles granted with the permission are not specified as required meaning access will be granted if the user requesting access has been granted any of the roles. However, you can specify a specific role as required when granting permission to roles, such that user must have the required role in order to be granted access. Group-based Access Control (GBAC) – the permission is granted to the group(s) such that users belonging to any of the group will be permitted to perform the scoped action on the resource. By default, access restriction is only applied to the groups granted with permission. However, it is possible to enable Extend to Children during the granting of the permission such that the users belonging to the subgroup of the granted group will also inherit the permission.  Typical flow for fine-grained authorization is as follows:    User make a request to Web App Backend. Web App Backend check whether user has Access Token in cookies. If Access Token is not present, user will be denied access and redirect to Keycloak for authentication. If Access Token is present, Web App Backend will then need to validate the Access Token to ensure it is valid. To check for validity of the Access Token, Web App Backend can use theOnline orOffline Validation. Once determine that the access token is valid, Web App Backend will then make a request to IAMS to evaluate whether user is granted permission to perform specific scoped action on specific resource. If the evaluation request result is Unauthorized, application will redirect user to error page.  note As part of the permission evaluation processing, IAMS will also perform Access Token validation. Hence, it is possible to skip step 3 since it is also performed by IAMS.  If user has the correct permission, Web App Backend can then direct the request to the Backend Services. Backend Services when received the request can also invoke IAMS again to evaluate whether user has the right permission. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/quickstart/overview","content":"Overview This section provides instruction on how to setup local deployment for development. For production development, please refer to Configuring for Productionon how to make the IAMS ready for production. Directory Structure The IAMS “local development” deployment scripts/manifests files package should contain the following directories: Subdirectory\tContainsiams-ass\tDeployment scripts/manifests files to deploy Authorization and Admin Service (IMAS-AAS) iams-keycloak\tDeployment scripts/manifests files to deploy customized Keycloak image (IAMS-Keycloak) and import scripts to load realm default settings. postgres\tDeployment scripts/manifests files to deploy PostgreSQL server to store IAMS data. note it is possible to reuse existing PostgreSQL server to store IAMS data instead ofdeploying another server instance. Refer to 2.7 (Deploy IAMS-Keycloak) for information to do so.","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/ian/overview","content":"Overview","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/ian/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/ian/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Deploy IAMS-Keycloak","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/deployment/deploy-iams-keycloak","content":"Deploy IAMS-Keycloak Navigate to the “iams-keycloak” directory. Edit 00-ghcr-key.yaml and replace {ghrc personal access key} with the actual base64 encoded dockerconfigjsonstring generated in step 3 of Prepare Personal Access Token Create the GHRC secret kubectl apply -f 00-ghcr-key.yaml Change the access permission of 01-create-configmap.sh chmod 755 01-create-configmap.sh Create the Configmap ./01-create-configmap.sh This step will create a config map that contains the content of realm-import.json file.realm-import.json file contains the default realm settings required for IAMS to work. Create Service kubectl apply -f 02-service.yaml This manifests file creates a ClusterIP services for accessing IAMS-Keycloak pod. Edit 03-deployment.yaml and change the value of the KC_HOSTNAME environment variable to the actual FQDN of your Kubernetes Cluster. The example value in the 03-deployment.yaml is using nip.io to create a FQDN to point to the Kubernetes Cluster. To use nip.io, simply replace 10.10.10.100 in the example value with the Ipv4 address of your Kubernetes Cluster. Create the Deployment kubectl apply -f 03-deployment.yaml This manifests file will instruct Kubernetes to deploy a Pod using iams-keycloak image from AGIL Ops Hub container registry. This manifests defines the following environment variables which instructs how iams-keycloak should be initialized: Environment Variable\tDescriptionDEFAULT_REALM\tThe name of the realm to be created. KC_DB\tThe database type to be used to store the data. Default value is “postgres”. Change this value if you wish to use other database type. KC_DB_URL_DATABASE\tThe name of the database to use. The default value match the POSTGRES_DB value. KC_DB_USERNAME\tThe username of the database user. The default value match the POSTGRES_USER value. KC_DB_PASSWORD\tThe password of the database user. The default value match the POSTGRES_PASSWORD value KC_DB_URL_HOST\tThe host URL of the database. The default value match the name field in 05-service.yaml in the postgres directory. KC_DB_URL_PORT\tThe port number to access the database. The default value match the port field in 05-service.yaml in the postgres directory. KC_HTTP_ENABLED\tEnable HTTP. The default value allow iams-keycloak to be access via HTTP. KEYCLOAK_ADMIN\tThe username of the default administrator. You will use to setup the realm. KEYCLOAK_ADMIN_PASSWORD\tThe password of the default administrator. KC_HOSTNAME\tThe URL to access the iams-keycloak. This value should match the Host value in 04-ingress.yaml. KC_HOSTNAME_BACKCHANNEL_DYNAMIC\tWhether to allow the backchannel URL to be the same as the frontchannel URL (URL specified in KC_HOSTNAME). The backchannel is used by IAMS-AAS to communicate with IAMS-Keycloak. This value should set to true. KC_HTTP_PORT\tThe HTTP port number to use. The default value match the port field value in 04-ingress.yaml. KC_HEALTH_ENABLED\tWhether to enable the liveness and readiness probe endpoints. This value should set to true. Edit 04-ingress.yaml and change the value of the Host field to the same value as KC_HOSTNAME in step 7. Create the ingress kubectl apply -f 04-ingress.yaml This manifests defines an ingress allowing IAMS-Keycloak using the URL defined in KC_HOSTNAME.","keywords":"","version":"Next"},{"title":"ID Token, Access Token, and Refresh Token","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/authentication/id-token-access-token-refresh-tokens","content":"","keywords":"","version":"Next"},{"title":"ID Token​","type":1,"pageTitle":"ID Token, Access Token, and Refresh Token","url":"/docs/docs/modules/iams/development/authentication/id-token-access-token-refresh-tokens#id-token","content":" OpenID Connect always issues ID tokens along with access tokens to provide compatibility with OAuth and match the general tendency for authorizing identity.  ID token carries personal information about end-users that authenticate on an OpenID Connect flow. In addition, this security token contains claims data about the user as saved with Keycloak.  The ID token is used to retrieve the user’s basic profile information like name, username, and email, which is present in the Keycloak.  ::: cautionID token should not be used to gain access to backend APIs. :::  ","version":"Next","tagName":"h2"},{"title":"Access Token​","type":1,"pageTitle":"ID Token, Access Token, and Refresh Token","url":"/docs/docs/modules/iams/development/authentication/id-token-access-token-refresh-tokens#access-token","content":" Access tokens are credentials used to access protected resources (i.e., backend APIs).  Access tokens are used as bearer tokens. A bearer token means that the bearer (who holds the access token) can access authorized resources without further identification.  Because of this, it is important that bearer tokens be protected.  These tokens usually have a short lifespan for security purposes. Typical lifespan is 5 minutes.  The lifespan can be change under the Realm setting &gt; Tokens page in Keycloak Web Admin Console.    When it expires, the new access token must be requested from Keycloak, either through authenticate again or using refresh token, which help to limit the exposure of the fact that it is a bearer token.  An access token is put in the Authorization header of the HTTP request to the backend API, it should be in the following format to ensure that the API that you are calling can verify it:Bearer &lt;access_token&gt;  AGIL Ops Hub Web Framework has the mechanism to auto refresh the access token before it expires to minimize authorization error when invoking backend APIs.  ","version":"Next","tagName":"h2"},{"title":"Refresh Token​","type":1,"pageTitle":"ID Token, Access Token, and Refresh Token","url":"/docs/docs/modules/iams/development/authentication/id-token-access-token-refresh-tokens#refresh-token","content":" This token is a long-lived token compared to the access token and is used to request a new access token in cases where it has expired. Hence, it can be considered as credentials used to obtain access tokens.  Typical lifespan of refresh token is typically 30 minutes and the expiry time will get extended every time it is used to refresh the access token. However, it will not be extended beyond the SSO Session Max timing which typically set to 10 hours.  The lifespan of refresh token is configured in Realm setting &gt; Sessions page, under the SSO Session Idle and SSO Session Max fields.  The refresh token allows a new access token to be requested without needing the user to re-authenticate, thereby providing a seamless user experience.    ","version":"Next","tagName":"h2"},{"title":"Validate Access Token​","type":1,"pageTitle":"ID Token, Access Token, and Refresh Token","url":"/docs/docs/modules/iams/development/authentication/id-token-access-token-refresh-tokens#validate-access-token","content":" As mentioned, Access Tokens are credentials used to access protected resources such as backend APIs. Hence, when received an Access Token alone with the request, the backend application will need to validate the Access Token and reject all requests with invalid or missing tokens.  There are 2 ways to validate the Access Token:  Online validationOffline validation  ","version":"Next","tagName":"h2"},{"title":"Online Validation​","type":1,"pageTitle":"ID Token, Access Token, and Refresh Token","url":"/docs/docs/modules/iams/development/authentication/id-token-access-token-refresh-tokens#online-validation","content":" The simplest way to perform online validation is to invoke the Keycloak’s Userinfo endpoint at:  http(s)://&lt;keycloak URL&gt; /realms/&lt;realm-name&gt;/protocol/openid-connect/userinfo   where &lt;keycloak URL&gt; is the URL to the Keycloak server and &lt;realm-name&gt; is the name of the realm.  In the HTTP request, you need to pass the access token in the Authorization header. Below is an example of access that endpoint using postman:    If the access token is valid, the endpoint will return the HTTP status 200 and if the access token is invalid, the endpoint will return HTTP status 401.  ::: Note some literature will advise using the Token Introspect Endpoint to validate the token instead. However, do note that this endpoint can only be invoked by confidential clients. I.e., you will need to pass the client id and client secret as part of the request. :::  ","version":"Next","tagName":"h3"},{"title":"Offline Validation​","type":1,"pageTitle":"ID Token, Access Token, and Refresh Token","url":"/docs/docs/modules/iams/development/authentication/id-token-access-token-refresh-tokens#offline-validation","content":" Alternative to online validation is to do offline validation which is more efficient as it doesn’t introduce another http/round trip for every validation.  To offline validate access token, you need to perform the following steps:  Split the access token into its sectionsValidate the signature of the access tokenCheck the issue date and expiry date in the token’s claims  Split Access Token into Sections​  As mentioned, access token is a standard JWT token which consists of the following sections:  HeaderPayload (often referred to as body)Signature  Sections are represented as base64url-encoded strings separated by a period (‘.’) delimiter. So, the first steps to validate the access token to split it into the 3 sections.  The following is a Java code snippet to split the access token into 3 sections:  String accessToken ='&lt;access token string&gt;'; String [] parts = accessToken.split (&quot;\\\\.&quot;);   Validate the Signature​  Next, we need to verify the integrity of the header and payload to ensure that they have not been altered by using the signature section.  To begin verifying the payload and header, we need both the signature algorithm that was used originally to sign the token and the secret key. For Keycloak:  Signature algorithm is by default RS256Secret Key is the public key of the realm, which can be obtains from the following endpoint:  http(s)://&lt;keycloak server URL&gt;/realms/&lt;realm name&gt;   You should obtains respond similar to the followings with the public key in the public_key field:  { &quot;realm&quot;: &quot;AOH&quot;, &quot;public_key&quot;: &quot;MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA7IaYbebl3fQ5ZaZBMlMe1kaTeVK0rfnn02bVWbHVudP0S513RdufxqosmKH0r/+QhXpZjfeEUfVrzKMe/NqJiVSv158gbPwX8ovqDTrPl/OEfROgvzMa+DM7HZxz9l1dSmPyXaG5TIl0CLLKibzDupp66/AuUluk4Z2fvBwujrJIQds6URGoVXhDqR136tqomgKU7UIB0dSgLK5ftO4SjPDRfij9+QNyyjF/kzTGqQmxeFEotmQrsGpGJPY7vYgHnlkSwLC7KM5JZwKex8uk6dVAOkM50rh3WxHIabH7h6m3Y5KpNVI/iS1e3clyaiE0iL1WCyByK2lXjVPjnSTQrwIDAQAB&quot;, &quot;token-service&quot;: &quot;http://iams-keycloak.10.10.10.100.nip.io/realms/AOH/protocol/openid-connect&quot;, &quot;account-service&quot;: &quot;http://iams-keycloak.10.10.10.100.nip.io/realms/AOH/account&quot;, &quot;tokens-not-before&quot;: 0 }   The following is a Java code snippet using the Java JWT library from Auth0 to validate the signature:  byte [] keyBytes = Base64.getDecoder().decode(JWTConstants.KEYCLOAK_PUBLIC_KEY); X509EncodedKeySpec encodedKeySpec = new X509EncodedKeySpec(keyBytes); KeyFactory factory = KeyFactory.getInstance(&quot;RSA&quot;); PublicKey pk = factory.generatePublic(encodedKeySpec); Algorithm algorithm = Algorithm.RSA256((RSAPublicKey) pk, null); JWTVerifier verifier = JWT.require(algorithm).ignoreIssuedAt().build(); try { verifier.verify(accessToken); return true; } catch (JWTVerificationException e) { System.err.println(e); return false; }   Check Issue Date and Expiry Date​  Finally, after determine that the signature of the access token is valid, the next step is to check that the access token not expired. To do that, you need to:  Decode the payload of access tokenCheck that the value of exp doesn’t exceed current timestamp  The payload of the access token is encoded in base64.  The following is a Java code snippet show how to decode the payload of the access token:  String[] parts = accessToken.split(&quot;\\\\.&quot;); String decodedAccessToken = new String(Base64.getUrlDecoder().decode(parts[1]));   After decoded the access token, you should get a JSON string with structure similar to the followings:  { &quot;exp&quot; : 1725001351, &quot;iat&quot; : 1725001051, &quot;jti&quot; : &quot;6087d61c-a81a-491c-b2e5-000643bfc3aa&quot;, &quot;iss&quot; : &quot;http://iams-keycloak.10.10.10.100.nip.io/realms/AOH&quot;, &quot;aud&quot; : [ &quot;realm-management&quot;, &quot;account&quot; ], &quot;sub&quot; : &quot;b6a62a0e-ab9f-4369-bc75-3e8bd2b5ddbb&quot;, &quot;typ&quot; : &quot;Bearer&quot;, &quot;azp&quot; : &quot;web-app&quot;, &quot;sid&quot; : &quot;e162ab8f-2f9c-461c-8c42-a713e7572cdb&quot;, &quot;acr&quot; : &quot;1&quot;, &quot;allowed-origins&quot; : [ &quot;http://localhost:3000&quot; ], &quot;realm_access&quot; : { &quot;roles&quot; : [ &quot;system-admin&quot;, &quot;default-roles-aoh&quot;, &quot;offline_access&quot;, &quot;realm-tenant-admin&quot;, &quot;uma_authorization&quot; ] }, … }   The value in the exp field is a JSON NumericDate, which is the number of seconds (not milliseconds) since Epoch (1970-01-01T00:00:00Z UTC).  The following Java code snippet show how to check whether the value of exp exceed current time:  JsonNode payload = mapper.readTree (decodedAccessToken); if (payload.get(&quot;exp&quot;).asLong() &lt; (System.currentTimeMillis() / 1000) ) { System.out.println(&quot;Not Expired.&quot;); }  ","version":"Next","tagName":"h3"},{"title":"Setup System Administrator Account","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/quickstart/setup-system-admin-account","content":"Setup System Administrator Account The system administrator is the administrator that has the right to administer tenant and tenant membership. Open browser and access the Keycloak Admin Console by using the URL configured in KC_HOSTNAME. Login to the Keycloak Admin Console using the username and password pair configured inKC_BOOTSTRAP_ADMIN_USERNAME and KC_BOOTSTRAP_ADMIN_PASSWORD. The default admin username is admin and password is admin. After login, switch to the default realm. The default name of the default realm is AOH. Next, click on the Usersmenu item in the side menu. Click on Add user button. In the Create user form, enter the followings: Email verified – set to OnUsername – the desired username of the system administrator. In the example below, myadmin is used.Email – the email of the system administrator Click on Create button to create the user. Click on Credentials tab. Click on Set password button. Enter the desired password for the administrator and turn off Temporary Click Save button, follow by Save password button. Next, click on Role mapping tab. Click on Assign role button Click Filter by realm roles from the filter dropdown. From the list of roles shown, check on system-admin role and click Assign.","keywords":"","version":"Next"},{"title":"Incident","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/incident","content":"Incident This is an object representing a single incident. This is the fundamental block of the incident management service. You can also list the incidents based on a specific tenant BASE_URL=http://ims.demo.agilopshub.com ","keywords":"","version":"Next"},{"title":"Authentication","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/authentication","content":"Authentication The IMS API uses access token to authenticate requet. The access token must be provided through our Keycloak service. curl https://ims.demo.agilopshub.com/v1/incident -H &quot;Bearer &lt;keycloak_access_token&gt;&quot; ","keywords":"","version":"Next"},{"title":"Assigning attributes","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/incident/custom-attributes/assigning_attributes","content":"","keywords":"","version":"Next"},{"title":"Assigning​","type":1,"pageTitle":"Assigning attributes","url":"/docs/docs/modules/ims/development/incident/custom-attributes/assigning_attributes#assigning","content":" Once the incident configurator establishes the metadata for custom attributes, incident owners and reporters can assign values to these attributes to their respective incident as needed. The following APIs are used to modify these attributes  PUT /v1/incident/{incident_id}/attributes curl -L 'http://ims.demo.agilopshub.com/v1/incident/INC_20241211_0001/attributes' \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer &lt;TOKEN&gt;' --json `{ 'name': 'count' 'type': 'INT' }`   RESPONSE { &quot;message&quot;: &quot;1 attribute(s) inserted. rows affected: 1&quot;, &quot;sent_at&quot;: &quot;2024-12-16T06:45:37Z&quot; }  ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/incident/custom-attributes/custom_attributes","content":"Introduction To enhance user flexibility, our incident management system allows configurators to define custom attributes tailored to their specific needs through a set of APIs. This enables incident owners to assign relevant values to these attributes, enriching the system's adaptability. To ensure data consistency and quality, incident owners must adhere to the predefined names and data types of these custom attributes. example of a incident with attributes { &quot;id&quot;: &quot;INC_20241216_0001&quot;, &quot;name&quot;: &quot;Security Breach&quot;, &quot;attributes&quot;: { &quot;count&quot;: 1235, &quot;pending&quot;: true }, &quot;location&quot;: &quot;&quot; } ","keywords":"","version":"Next"},{"title":"Pre-requisites","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/quickstart/pre-requisites","content":"Pre-requisites Kubernetes distribution with Traefik Ingress installedRead access to AGIL Ops Hub container registry Assumptions All installation instructions assume that You are using the IAMS “local development”deployment scripts/manifests files.You are using Kubernetes cluster with Traefik Kubernetes Ingress The installation instructions are tested using K3S, a lightweight Kubernetes distribution that come with Traefik Ingress preinstalled. You have the necessary permission right to execute kubectl commands against the Kubernetes cluster.You are executing the following instructions in a Linux environment.","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/overview","content":"Development The development section that you write for your module should provide step by step instructions with examples of how to use your API. In this Documentation case, we'll actually be providing you with step-by-step- instructions on how to create the API documentation in the Example API section.","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/ian/development","content":"Development","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/overview","content":"Overview This module provides a framework for reporting and responding to incidents within your project. The Incident Management System (IMS) module is designed to help you: Streamline incident reporting through a centralized system.Facilitate communication between team members, stakeholders, and management.Guide the response process through defined workflows and procedures.Document incident details, actions taken, and lessons learned. This module is meant to be installed via the web-base project.","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/introduction","content":"","keywords":"","version":"Next"},{"title":"Key Personas​","type":1,"pageTitle":"Introduction","url":"/docs/docs/modules/ims/development/introduction#key-personas","content":" This system is designed to be used by several key personas, each with distinct responsibilities.  Configurator: Responsible for defining and managing custom attributes within the system.Incident Owner: Assigned to investigate and resolve incidents, responsible for assigning values to relevant attributes.Incident Reporter: Individuals who initially report incidents within the system.  ","version":"Next","tagName":"h2"},{"title":"API Reference​","type":1,"pageTitle":"Introduction","url":"/docs/docs/modules/ims/development/introduction#api-reference","content":" The IMS API follows a RESTful architecture. This means it uses standard web principles:  Clear URLs: Resource locations are easily identifiable in the URLs. Data Formats: The API responds with data in a standard format (JSON). Input: Data is sent to the API in a common format (form-encoded). Output: The API responds with data in a standard format (JSON). ","version":"Next","tagName":"h2"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/quickstart","content":"","keywords":"","version":"Next"},{"title":"1. Clone web-base repository​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/quickstart#1-clone-web-base-repository","content":" Use the github CLI to clone the web-base project.  git clone https://github.com/mssfoobar/web-base   Please refer to the web-base documentation to setup the pre-requisites for the web-base to work. You may refer to ithere.  ","version":"Next","tagName":"h3"},{"title":"2. Install dependencies​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/quickstart#2-install-dependencies","content":" Install node dependencies  npm install   Once done, you have a functioning web-base to install the dashboard module.  ","version":"Next","tagName":"h3"},{"title":"Installing the Incident Management Service Module​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/quickstart#installing-the-incident-management-service-module","content":" ","version":"Next","tagName":"h2"},{"title":"Configuring Environment Variables​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/quickstart#configuring-environment-variables","content":" The ims module has backend service that we need to connect to. To successfully connect to the backend services, you'll need to configure a few environment variables beforehand. You may refer to here  note Please ensure that the ims services are set up and ready prior to installing the ims module.  IMS_URL = 'link to your ims backend service'  The web-base contains a module installer to install modules to the web-base. You may use this command to install the dashboard module. Running this command will download and copy over the required files over to your web-base repository, together with the dependencies installed into the package.json  ","version":"Next","tagName":"h3"},{"title":"Installing the IMS Module via the CLI.​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/quickstart#installing-the-ims-module-via-the-cli","content":" npx cli install @mssfoobar/ims   ","version":"Next","tagName":"h3"},{"title":"Others​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/quickstart#others","content":" note After installation, you may encounter errors due to the module's dependency on specific shadcn UI components. To resolve these issues, simply install the required shadcn UI components through its command tool. (It's already installed in the web-base) and you'll be able to proceed with your development. ","version":"Next","tagName":"h2"},{"title":"Defining attributes","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/incident/custom-attributes/defining_attributes","content":"","keywords":"","version":"Next"},{"title":"Metadata Attributes​","type":1,"pageTitle":"Defining attributes","url":"/docs/docs/modules/ims/development/incident/custom-attributes/defining_attributes#metadata-attributes","content":" ","version":"Next","tagName":"h2"},{"title":"Management​","type":1,"pageTitle":"Defining attributes","url":"/docs/docs/modules/ims/development/incident/custom-attributes/defining_attributes#management","content":" The meta-table can be dynamically adjusted using the following two APIs:  Endpoints POST /v1/incident/attributes DELETE /v1/incident/attributes   ","version":"Next","tagName":"h3"},{"title":"Attributes​","type":1,"pageTitle":"Defining attributes","url":"/docs/docs/modules/ims/development/incident/custom-attributes/defining_attributes#attributes","content":" name string (unique) The name of the attribute  type string The type of the value. Currently we provide the following types: STRINGINTBOOLEAN  ","version":"Next","tagName":"h3"},{"title":"Querying metadatas​","type":1,"pageTitle":"Defining attributes","url":"/docs/docs/modules/ims/development/incident/custom-attributes/defining_attributes#querying-metadatas","content":" Alternatively, the owners and configurators may query what are the existing metadata already defined as well inorder to ensure the right values are inputted.  GET /v1/incident/attributes curl -L 'http://ims.demo.agilopshub.com/v1/incident/INC_20241211_0001/attributes' \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer &lt;TOKEN&gt;'   RESPONSE { &quot;data&quot;: [ { &quot;name&quot;: &quot;count&quot;, &quot;type&quot;: &quot;INT&quot; }, { &quot;name&quot;: &quot;pending&quot;, &quot;type&quot;: &quot;BOOLEAN&quot; } ], &quot;sent_at&quot;: &quot;2024-12-16T07:47:39Z&quot; }   This will return a list of attributes and what are the types associated with it. ","version":"Next","tagName":"h2"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ims/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ims/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/configuration","content":"","keywords":"","version":"Next"},{"title":"Example Configuration Table​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#example-configuration-table","content":" The following is an example of configuration rendered using a markdown table - this is good for config maps in JSON/YAML where the data type might matter:  Name\tType\tDefault\tDescriptiontitle\tstring\t'Sample Config Title'\tThe title that is displayed for your this example application. outputDir\tstring\t'./'\tThe path to output something from this example application. isSecure\tboolean\tfalse\tSet to true if you want your example application to be secure. friends\tstring[]\t[]\tThe list of friends you wish you had. enemies\tstring[]\t[ &quot;Jon Snow&quot;]\tThe list of enemies you currently have.  For environment variables, the type column can be omitted as they are always strings.  ","version":"Next","tagName":"h2"},{"title":"Example usage in JSON:​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#example-usage-in-json","content":" { &quot;title&quot;: &quot;Sample Config Title&quot;, &quot;outputDir&quot;: &quot;./&quot;, &quot;isSecure&quot;: false, &quot;friends&quot;: [], &quot;enemies&quot;: [ &quot;Jon Snow&quot; ] }   ","version":"Next","tagName":"h3"},{"title":"Example usage in YAML:​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#example-usage-in-yaml","content":" title: Sample Config Title outputDir: ./ isSecure: false friends: [] enemies: - Jon Snow   ","version":"Next","tagName":"h3"},{"title":"Example using markdown headers​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#example-using-markdown-headers","content":" For more complex configurations that require longer descriptions, you should segregate them with markdown headers. The advantage of using markdown headers is that it also creates anchor links you can use to share directly to the section like this for title:  ","version":"Next","tagName":"h2"},{"title":"title​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#title","content":" Type: string​ Default: 'Sample'​ The title that is displayed for your this example application. This longer example might include an image of how the title looks like:      ","version":"Next","tagName":"h3"},{"title":"outputDir​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#outputdir","content":" Type: string​ Default: './'​ The path to output something from this example application.  ","version":"Next","tagName":"h3"},{"title":"isSecure​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#issecure","content":" Type: boolean​ Default: false​ Set to true if you want your example application to be secure. caution In some cases you might also need to call include admonitions like this, which won't fit in a table.  ","version":"Next","tagName":"h3"},{"title":"friends​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#friends","content":" Type: string[]​ Default: []​ The list of friends you wish you had.  You might need to elaborate on your configuration, like how you wish you were friends with Princess Diana or Jeremy Clarkson. In which case you should pass in [ 'Princess Diana', 'Philomena Cunk' ] in this example.  ","version":"Next","tagName":"h3"},{"title":"enemies​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ims/configuration#enemies","content":" Type: string[]​ Default: [ 'Jon Snow' ]​ The list of friends you wish you had.  The list of enemies you currently have. ","version":"Next","tagName":"h3"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/quickstart","content":"","keywords":"","version":"Next"},{"title":"Pre-requisites​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart#pre-requisites","content":" Required:  Docker  Please ensure these steps are done before following.  Install DockerInstall Docker-ComposeDocker Post-installation setup  git clone https://github.com/mssfoobar/dev-containers.git   info If you previously used a different service with a dev container that created an iams container, you'll need to delete the associated volume (.data) file. This ensures the iams service reinitializes correctly with the updated client data.  ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart#installation","content":" ","version":"Next","tagName":"h2"},{"title":"1. Clone Repository​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart#1-clone-repository","content":" Please run the following command to clone the ims container repository.  Next, we are to clone our container. We have provided a container that provides you the IMS service together with all the other necessary services in a single docker file for ease of setup.  ","version":"Next","tagName":"h3"},{"title":"2. Setting up environment variables​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart#2-setting-up-environment-variables","content":" DEV_DOMAIN string A rough title of what the incident is about. This is the domain that the IMS service will be using. E.g. if the value of DEV_DOMAIN is 127.0.0.1.nip.io, inorder to access the IMS service, the appropriate url will be http://ims.127.0.0.1.nip.io  DEV_USER string The username to be used for all services for convenience of access and development  DEV_PASSWORD string The password to be used for all services for convenience of access and development  DEV_USER_URL_ENCODED string Global username used for everything - but URL encoded - this is needed for connection strings#  DEV_PASSWORD_URL_ENCODED string Global password used for everything - but URL encoded - this is needed for connection strings (E.g. If DEV_PASSWORD was P@ssw0rd, then DEV_PASSWORD_URL_ENCODED would be P%40ssword as the special character '@' must be encoded)  ","version":"Next","tagName":"h3"},{"title":"3. Running the container​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart#3-running-the-container","content":" You may now run the container using the following command  docker compose --env-file .env -f ims/compose.yml -f override.yml up -d   This will create the container and you may start to use the ims service. In order to access the IMS service for testing, you may use the following URL  ims.&lt;insert the value of DEV_DOMAIN here&gt;/v1/incident   With this, you may proceed to start your local development!  ","version":"Next","tagName":"h3"},{"title":"Using a Local Configuration File for Container Settings​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart#using-a-local-configuration-file-for-container-settings","content":" For greater development flexibility, we provide a customizable YAML file that allows you to adjust certain container settings. This file is intended for your personal development environment and is not part of the project's shared configuration. This file will overwrite any specified parameters that is in the ./ims/compose.yml  To customize, please modify the following file with your desired IDE  vi ./overwrite.yml   Below is an snippet of overwrite.yml  ims-db: ports: - 5432:5432   ","version":"Next","tagName":"h2"},{"title":"","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart##","content":" For this specific scenario, we are adjusting the ports parameter to redirect port 5432 in ./ims/compose.yml  ","version":"Next","tagName":"h2"},{"title":"Clean up​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ims/development/quickstart#clean-up","content":" Once done with the service, if you want to delete the container, you may use the following command  docker compose -f ims/compose.yml down -v  ","version":"Next","tagName":"h2"},{"title":"Tools","type":0,"sectionRef":"#","url":"/docs/docs/modules/prerequisites/tools","content":"","keywords":"","version":"Next"},{"title":"Git​","type":1,"pageTitle":"Tools","url":"/docs/docs/modules/prerequisites/tools#git","content":" We use Git for our souce control. Follow the guide onGitHub's Git Guides for thorough instructions on how to install Git on Windows, MacOS and Linux.  ","version":"Next","tagName":"h2"},{"title":"Docker / Podman​","type":1,"pageTitle":"Tools","url":"/docs/docs/modules/prerequisites/tools#docker--podman","content":" We use Podman as our container engine - visit their website at https://podman-desktop.io/ and follow their instructions to install Podman desktop.  note To run podman compose, you must perform additional set up, refer to the following link for details: https://podman-desktop.io/docs/compose/running-compose  ","version":"Next","tagName":"h2"},{"title":"Visual Studio Code​","type":1,"pageTitle":"Tools","url":"/docs/docs/modules/prerequisites/tools#visual-studio-code","content":" https://code.visualstudio.com/  ","version":"Next","tagName":"h2"},{"title":"Extensions:​","type":1,"pageTitle":"Tools","url":"/docs/docs/modules/prerequisites/tools#extensions","content":" https://marketplace.visualstudio.com/items?itemName=SonarSource.sonarlint-vscode  JavaScript &amp; Web:​  VSCode comes with TypeScript support out-of-the-box.  Svelte: https://marketplace.visualstudio.com/items?itemName=svelte.svelte-vscodeESLint: https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslintPrettier: https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscodeTailwind CSS: https://marketplace.visualstudio.com/items?itemName=bradlc.vscode-tailwindcss  Backend:​  Golang https://marketplace.visualstudio.com/items?itemName=golang.go  ","version":"Next","tagName":"h3"},{"title":"Podman Desktop​","type":1,"pageTitle":"Tools","url":"/docs/docs/modules/prerequisites/tools#podman-desktop","content":" https://podman-desktop.io/ ","version":"Next","tagName":"h2"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/ptmgr/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/ptmgr/overview","content":"Overview","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/ptmgr/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/ian/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ian/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ian/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/ptmgr/development","content":"Development","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/ptmgr/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"The Incident object","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/development/incident/incident_object","content":"","keywords":"","version":"Next"},{"title":"Attributes​","type":1,"pageTitle":"The Incident object","url":"/docs/docs/modules/ims/development/incident/incident_object#attributes","content":" id string The unique identifier of the incident class. The id follows the format of 'INC_YYYYMMDD_N' where: 'INC' is a fixed prefix indicating an incident.'YYYYMMDD' represents the date the incident was created (Year, Month, Day).'N' is a sequential number assigned to each incident created on that specific date, starting from 1.&quot;  name string A rough title of what the incident is about.  status string Tracks the current status of an incident (e.g., &quot;Open,&quot; &quot;In Progress,&quot; &quot;Resolved&quot;).  type string Classifies the type of incident.  date timestamp The time where the incident took place.   description string Stores a detailed textual account of an incident. Provides crucial information for investigating the root cause of the incident. Enables clear communication about the incident between different teams and stakeholders.  reported_by string Name of the reporter that reported this incident.  location string The location where the incident took place.  severity string The severity of the incident provided by the reporter/reporting agent  resolved_date timestamp The date where this incident was resolved  backdated boolean Indicates whether this incident was from a legacy system, or when this incident was created.  occ_lock integer The value of occ_lock represents the current state of the occupancy lock in a given context. It's typically used to track whether a resource or space is currently occupied or available.  attributes* map A map of customised attributes determined by the IMS configurator. For more information, you may refer tohere ","version":"Next","tagName":"h3"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/prerequisites/overview","content":"Overview This section serves as a reference point for common setup instructions across all modules. For example, how to get access to our source code, or how to set up Docker or Podman to pull from our container registry.","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/rnr/overview","content":"Overview RNR is a backend service which record &amp; replay past db events.","keywords":"","version":"Next"},{"title":"Customize Login Screen","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/authentication/customize-login-screen","content":"","keywords":"","version":"Next"},{"title":"Development​","type":1,"pageTitle":"Customize Login Screen","url":"/docs/docs/modules/iams/development/authentication/customize-login-screen#development","content":" During theme development, you might want to perform the following setup to simply development:  Create a persistent volume using local Kubernetes host file system and mount it to the Keycloak container’s /opt/keycloak/themes folder. By doing so, you can directly create theme folders and files in the host file system which will appear in the Keycloak theme folder.Add the following environment variable to Keycloak deployment file to disable theme caching. Doing so, will allow changes in theme files to take effect immediately without having to restart Keycloak pod: KC_SPI_THEME_STATIC_MAX_AGE – 1KC_SPI_THEME_CACHE_THEMES – falseKC_SPI_THEME_CACHE_TEMPLATES – false  The followings are sample scripts to setup Keycloak in Kubernetes for themes development:  Script to create persistent volume. It maps the persistent volume to host file system directory /data/themes/mytheme where mytheme is the customize theme.  keycloak-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: keycloak-volume #namespace: ar2 labels: type: local app: iams-keycloak spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteMany hostPath: path: /data/themes/mytheme   Save the script above as yaml file (e.g., keycloak-pv.yaml) Execute the following command to create the persistent volume in Kubernetes:  kubectl apply –f keycloak-pv.yaml   Followings are script to create a persistent volume claim:  keycloak-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: keycloak-volume-claim labels: app: iams-keycloak spec: storageClassName: manual accessModes: - ReadWriteMany resources: requests: storage: 10Gi   Save the script above as yaml file (e.g., keycloak-pvc.yaml) Execute the following command to create the persistent volume claim in Kubernetes:  kubectl apply –f keycloak-pvc.yaml   Followings are script to create keycloak deployment. Text highlighted in red are the changes to the deployment manifests scripts used in Deploy IAMS-Keycloak.  04-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: iams-keycloak labels: app: iams-keycloak spec: replicas: 1 selector: matchLabels: app: iams-keycloak template: metadata: labels: app: iams-keycloak spec: imagePullSecrets: - name: gh-regcred containers: - name: iams-keycloak imagePullPolicy: Always image: ghcr.io/mssfoobar/iams/iams-keycloak:latest-dev args: - start - --import-realm env: - name: DEFAULT_REALM value: &quot;AOH&quot; - name: KC_DB value: &quot;postgres&quot; - name: KC_DB_URL_DATABASE value: &quot;keycloak&quot; - name: KC_DB_USERNAME value: &quot;keycloak&quot; - name: KC_DB_PASSWORD value: &quot;admin&quot; - name: KC_DB_URL_HOST value: &quot;postgres&quot; - name: KC_DB_URL_PORT value: &quot;5432&quot; - name: KC_HTTP_ENABLED value: &quot;true&quot; - name: KEYCLOAK_ADMIN value: &quot;admin&quot; - name: KEYCLOAK_ADMIN_PASSWORD value: &quot;admin&quot; - name: KC_HOSTNAME value: &quot;http://iams-keycloak.10.10.10.100.nip.io&quot; - name: KC_HOSTNAME_BACKCHANNEL_DYNAMIC value: &quot;true&quot; - name: KC_HTTP_PORT value: &quot;80&quot; - name: KC_SPI_THEME_STATIC_MAX_AGE value: &quot;1&quot; - name: KC_SPI_THEME_CACHE_THEMES value: &quot;false&quot; - name: KC_SPI_THEME_CACHE_TEMPLATES value: &quot;false&quot; ports: - name: http containerPort: 80 - name: https containerPort: 8443 volumeMounts: - name: keycloak-realm-config mountPath: /opt/keycloak/data/import/realm-import.json subPath: realm-import.json volumeMounts: - mountPath: /opt/keycloak/themes/mytheme name: themesdata volumes: - name: keycloak-realm-config configMap: name: keycloak-realm-config items: - key: realm-import.json path: realm-import.json volumes: - name: themesdata persistentVolumeClaim: claimName: keycloak-volume-claim   Save the script above as yaml file (e.g., 04-deployment.yaml) Execute the following command to create the keycloak deployment in Kubernetes:  kubectl apply –f 04-deployment.yaml   Next, you will also need to perform the following to activate the themes:  Login to the Web Admin Console and navigate to the realm. Click on Realm settings in the side menu    Click on the Themes tab:    Select you theme from the dropdown menu of the theme type and click on Save to activate the themes    ","version":"Next","tagName":"h2"},{"title":"Production​","type":1,"pageTitle":"Customize Login Screen","url":"/docs/docs/modules/iams/development/authentication/customize-login-screen#production","content":" For production, you can continue to use the persistent volume method as in development but remove the environment variables that disable the caching of themes.  Alternatively, you can also add the themes folder directly into the container images. The steps are as followings:  Build a new image based on IAMS Keycloak image and add the custom themes to the image.Tag and push the new image to your container registryChange the deployment manifests file to point to the new image.  The following is a sample docker build script to create a new container image. This script assume that your custom theme is in the mytheme folder.  DockerBuild FROM ghcr.io/mssfoobar/iams/iams-keycloak:latest-dev AS builder WORKDIR /opt/keycloak COPY --chown=keycloak:keycloak --chmod=744 mytheme /opt/keycloak/themes/mytheme RUN /opt/keycloak/bin/kc.sh build FROM ghcr.io/mssfoobar/iams/iams-keycloak:latest-dev COPY --from=builder /opt/keycloak/ /opt/keycloak/ COPY --from=builder /opt/keycloak/themes/mytheme /opt/keycloak/themes/mytheme ENTRYPOINT [&quot;/opt/keycloak/bin/kc.sh&quot;]   Save the content as Dockerfile and execute the following command to build the image:  docker build –t &lt;container registry url&gt;/&lt;image name&gt; .   where &lt;container registry url&gt; should be replace with the actual URL to your container registry and &lt;image name&gt; is the name of your container image.  Next, execute the following command to push the newly created image to your container registry:  docker push &lt;container registry url&gt;/&lt;image name&gt;:latest   Modified the Keycloak deployment manifests file in 2.7 to point to your image.  … containers: - name: iams-keycloak imagePullPolicy: Always image: &lt; container registry url&gt;/&lt;image name&gt;:latest args: …   Execute the command to deploy the new container:  kubectl apply -f 04-deployment.yaml  ","version":"Next","tagName":"h2"},{"title":"Access Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/prerequisites/access","content":"","keywords":"","version":"Next"},{"title":"Configuring Docker to have access to pull images​","type":1,"pageTitle":"Access Configuration","url":"/docs/docs/modules/prerequisites/access#configuring-docker-to-have-access-to-pull-images","content":" If you've already taken a look at our image tags, you'll notice they begin with ghcr.io/mssfoobar - if the image you're trying to pull is refering to a different website (e.g. AWS EKS) rather than ghcr.io, your team might have pulled the images and stored them to a different container registry. In that case, these instructions might not apply to you.  If you have not already been given access to the container images, request for access from one of the team members. You will need to have a GitHub account with access to the images. You can check to see if you have access by signing-in to your GitHub account, then browsing the packages here: https://github.com/orgs/mssfoobar/packages If you don't see the container image you need, your account does not have access to it. Generate a personal access token with read:packages scope to download container images. If you're doing development and also need to push container images, you will also need the write:packages scope. You can refer to GitHub's official instructions on how to generate personal access tokens here: Creating a GitHub Personal Access Token Login to Docker with the token:  $ echo TOKEN | docker login ghcr.io -u USERNAME --password-stdin  Replace TOKEN with your token generated in step 2Replace USERNAME with your GitHub username  For more details on working with GitHub's container registry, you can refer to theirofficial documentation.  ","version":"Next","tagName":"h2"},{"title":"Preparing your Personal Access Token for Kubernetes​","type":1,"pageTitle":"Access Configuration","url":"/docs/docs/modules/prerequisites/access#preparing-your-personal-access-token-for-kubernetes","content":" To set your Docker installation up to be able to access AGIL Ops Hub's containers on ghcr.io/mssfoobar, you will need a personal access token that is able to read packages from ghcr.io/mssfoobar.  You can skip this steps if you are using other means to deploy the iams-keycloak and iams-aas images.  Refer to the following link to create a personal access token:  https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic  Once you have generated and saved the access token, execute the following command to base64 encode your access token:  echo -n &quot;USERNAME:PAT&quot; | base64  Replace USERNAME with your GitHub usernameReplace PAT with the token retrieved from step 1.  Next you will need to compose a new dockerconfigjson string and base64 encode it using the following command:  $ echo -n '{&quot;auths&quot;:{&quot;ghcr.io&quot;:{&quot;auth&quot;:&quot;ENCODED_TOKEN&quot;}}}' | base64  Replace ENCODED_TOKEN with the output string obtained in step 2.  ","version":"Next","tagName":"h2"},{"title":"Configuring npm to have access to pull packages​","type":1,"pageTitle":"Access Configuration","url":"/docs/docs/modules/prerequisites/access#configuring-npm-to-have-access-to-pull-packages","content":" By default, our repositories will include an .npmrc file which points to npm.pkg.github.com for npm packages with the @mssfoobar scope. If you are setting up a new project (e.g. using the web-base) or your development workflow invol  npm set //npm.pkg.github.com/:_authToken=&lt;YOUR_GITHUB_PERSONAL_ACCESS_TOKEN&gt;  ","version":"Next","tagName":"h2"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/ptmgr/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ptmgr/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ptmgr/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/ptmgr/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/rnr/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/rnr/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/rnr/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/rnr/development","content":"Development","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/rnr/configuration","content":"","keywords":"","version":"Next"},{"title":"DB Snapshot Retention Configuration​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/rnr/configuration#db-snapshot-retention-configuration","content":" Replaymgr service DB_SNAPSHOT_RETENTION_IN_DAY env variable set how long the data should be kept.  DB_SNAP_SHOT_RETENTION_IN_DAY=10   Default value will be set to 7 days if the env variable is not set.  ","version":"Next","tagName":"h2"},{"title":"DebeziumStream Retention Configuration​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/rnr/configuration#debeziumstream-retention-configuration","content":" Install nats-cli toolCreate the DebeziumStream (skip to step 3 if the stream exists)  nats stream add DebeziumStream --storage=file -s nats://{host}:{port}   Edit the stream to write to file system &amp; limit the retention period (for example: 7 days)  nats stream edit DebeziumStream --max-age=7d -s nats://{host}:{port}   ","version":"Next","tagName":"h2"},{"title":"Minio Object Expiry Configuration​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/rnr/configuration#minio-object-expiry-configuration","content":" Install minio client tool mcSet alias to the minio server (replace placeholder in with your environment setup)  mc alias set {name} {minio-server-url} {access-key} {secret-key}   Use the alias created in step 2 to set the bucket expiry by day  mc ilm rule add --expire-days 7 {name}/{bucketname}   ","version":"Next","tagName":"h2"},{"title":"DB Snapshot Frequency Configuration​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/rnr/configuration#db-snapshot-frequency-configuration","content":" We use kubernetes cronjob to run bash script to take db snapshot.  To update the frequency of the db snapshot, update the kubernetes cronjob yaml file.  For example, update the schedule value in yaml file to &quot;*/5 * * *&quot; to set frequency to every 5 minutes.  apiVersion: batch/v1 kind: CronJob metadata: name: rnr-periodic-snapshot-raw namespace: common-rnr spec: schedule: &quot;*/5 * * * *&quot;  ","version":"Next","tagName":"h2"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/rnr/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/rnr/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/rtus/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/rtus/development","content":"Development","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/prerequisites/local-development","content":"","keywords":"","version":"Next"},{"title":"Guide​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#guide","content":" ","version":"Next","tagName":"h2"},{"title":"1. Clone the repository​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#1-clone-the-repository","content":" 1.1 - Clone the git repository​  git clone https://github.com/mssfoobar/dev-containers   1.2 - Navigate into the root folder​  cd dev-containers   ","version":"Next","tagName":"h3"},{"title":"2. Set up the environment variables​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#2-set-up-the-environment-variables","content":" By default, the environment variable file already contains all the default values you need to get up and running except for the DEV_PASSWORD &amp; DEV_PASSWORD_URL_ENCODED.  .env.template # Root domain to use for all services DEV_DOMAIN=127.0.0.1.nip.io # Global username and password used for everything DEV_USER=admin DEV_PASSWORD= # Global username and password used for everything - but URL encoded - this is needed for connection strings # as some passwords might have special symbols in them, such as &quot;@&quot;, causing database connection strings to fail # For example, if your DEV_PASSWORD is &quot;strong_p@ssword&quot;, in DEV_PASSWORD_URL_ENCODED, it must be &quot;strong_p%40ssword&quot; DEV_USER_URL_ENCODED=admin DEV_PASSWORD_URL_ENCODED=   2.1 - Make a copy of the file and rename it to .env​  cp .env.template .env   2.2 - Choose a suitable password and set it for both variables.​  .env # Root domain to use for all services DEV_DOMAIN=127.0.0.1.nip.io # Global username and password used for everything DEV_USER=admin DEV_PASSWORD=myfavpassword # Global username and password used for everything - but URL encoded - this is needed for connection strings # as some passwords might have special symbols in them, such as &quot;@&quot;, causing database connection strings to fail # For example, if your DEV_PASSWORD is &quot;strong_p@ssword&quot;, in DEV_PASSWORD_URL_ENCODED, it must be &quot;strong_p%40ssword&quot; DEV_USER_URL_ENCODED=admin DEV_PASSWORD_URL_ENCODED=myfavpassword   ","version":"Next","tagName":"h3"},{"title":"3. Run the command for the module you want​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#3-run-the-command-for-the-module-you-want","content":" Every compose.yml file in each folder is designed to already include the services they need, simply start it up with the .env file and apply any overrides you need:  3.1 - Example, for the IAN module, you can run it like so.​  podman compose --env-file .env -f ian/compose.yml -f ./override.yml up -d   override.yml is meant for you to fill in overrides for port mapping (for scenarios such as directly accessing the databases with your database management client).  warning For Docker users, you must use the override to override the default traefik settings as it is assuming you are using Podman - the volumes: entry is different to bind to the appropriate docker.sock.  In the future, we intend to provide tooling with this information integrated. For now, each module states all their dependencies in their respective folders, so if you want to include the GIS(Geospatial Information Services) module, look at the README.md in it's folder and run that command.  ","version":"Next","tagName":"h3"},{"title":"Additional Configuration​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#additional-configuration","content":" Wildcard DNS vs Host file​  Custom hostname resolution is required for proper development - the system relies on cookies being forwarded to the front-facing servers for authentication.  You can use a wildcard DNS service like https://nip.io or modify your /etc/hosts file.  ","version":"Next","tagName":"h2"},{"title":"Wildcard DNS (nip.io)​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#wildcard-dns-nipio","content":" If you're using nip.io, set the DEV_DOMAIN environment variable to get nip.io to resolve it to the loopback address:  DEV_DOMAIN=127.0.0.1.nip.io   For your web services, make sure the PUBLIC_DOMAIN is set appropriately (to nip.io)  PUBLIC_DOMAIN=nip.io   This requires no additional changes to your system - you may then access services via their appropriate subdomain, for example, to visit the Keycloak admin site, you will go to http://iams-keycloak.127.0.0.1.nip.io  ","version":"Next","tagName":"h3"},{"title":"Host file​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#host-file","content":" Alternatively, you can modify your hosts file to allow your machine to resolve the addresses appropriately, you can do this by adding the values below to your hosts file in C:\\Windows\\System32\\Drivers\\etc\\hosts in Windows, or/etc/hosts in Unix systems.  ## -------------------- AOH Local Development -------------------- ## ========== Traefik ========== 127.0.0.1 traefik.localhost ## ========== IAMS ========== 127.0.0.1 iams-keycloak.localhost iams-aas.localhost iams-web.localhost iams-db.localhost ## ========== Web Base ========== 127.0.0.1 web-base.localhost ## ========== RTUS ========== 127.0.0.1 rtus-seh.localhost rtus-pms.localhost rtus-db.localhost ## ========== TAG ========== 127.0.0.1 tag.localhost tag-web.localhost tag-db.localhost ## ========== UNH ========== 127.0.0.1 unh.localhost unh-web.localhost unh-db.localhost ## ========== IAN ========== 127.0.0.1 ian.localhost ian-web.localhost ian-db.localhost ## ========== DASH ========== 127.0.0.1 dash.localhost dash-web.localhost dash-db.localhost ## ========== GIS ========== 127.0.0.1 gis.localhost gis-web.localhost gis-db.localhost ## ========== WFE ========== 127.0.0.1 wfe.localhost wfe-web.localhost wfe-db.localhost   For your web services, make sure the PUBLIC_DOMAIN is left blank, this is a mandatory cookie setting for browsers when the domain is meant to be localhost, you can use the compose.override.yml to achieve this.  name: aoh services: iams-web: environment: - PUBLIC_DOMAIN= ...   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Updating Container Images​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#updating-container-images","content":" By default, the dev-containers repository uses latest-dev for all our tags. Docker/Podman will not automatically pull the newest image as we're re-using the 'latest-dev' tag. You'll have to either delete the image locally or manually pull the latest[-dev] tagged image to receive the newest updated images.  ","version":"Next","tagName":"h3"},{"title":"Changing Env Vars​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#changing-env-vars","content":" The env vars are used as part of the initialization process - it includes generating users, setting credentials, etc. and all these are persisted in the .data folder as it is used in the Docker volumes. If you need to change any of the environment variables (DEV_DOMAIN, DEV_USER or DEV_PASSWORD), do not expect the values to be reflected without re-initializing everything (you will need to delete .data and restart everything).  ","version":"Next","tagName":"h3"},{"title":"Unable to login to Keycloak​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#unable-to-login-to-keycloak","content":" Since Keycloak 26, the KEYCLOAK_ADMIN and KEYCLOAK_PASSWORD have been deprecated and replaced withKC_BOOTSTRAP_ADMIN_USERNAME and KC_BOOTSTRAP_ADMIN_PASSWORD. If you are having difficulties logging in, you might be using an older container.  ","version":"Next","tagName":"h3"},{"title":"Podman​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/prerequisites/local-development#podman","content":" Credential issues when migrating from Docker​  There is a common issue you might face when Docker was installed and you are switching over to podman. If you are able to pull the containers manually, but not when you run podman compose ... please visit the following link to see how you can resolve it: https://github.com/containers/podman/issues/22682  Traefik​  For Traefik to perform routing, it needs access to the Docker API via the docker.sock, for Podman, that is available at /run/podman/podman.sock by default. If you're having problems with Traefik look for the remoteSocket config:  podman info   remoteSocket: exists: true path: /run/podman/podman.sock   Under &quot;path&quot;: You might see something other than /run/podman/podman.sock, in that scenario, use the override.ymlfile to modify it appropriately:  cp override.sample.yml override.yml   ## Use this override file to override any default configuration ## Example Use Case: Forward ports to a database you want to access name: aoh services: traefik: volumes: - /run/user/example/podman.sock:/var/run/docker.sock   Then, run the command with the module you want and try again:  podman compose --env-file .env -f example_module/compose.yml -f override.yml  ","version":"Next","tagName":"h3"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/rtus/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/rtus/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/rtus/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/rtus/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/rtus/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/tag/development","content":"Development","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/tag/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/tag/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/rtus/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Administration","type":0,"sectionRef":"#","url":"/docs/docs/modules/iams/development/operating-concept-and-administration/administration","content":"Administration Each realm has 2 default system roles: system-admintenant-admin Users will system-admin role can perform all administrative tasks within the realm while user with tenant-admin role can only perform administrative tasks pertaining to the tenant that he is assigned administer to. User must be a member of a tenant in order to be assign with tenant-admin role for that tenant. User can be a member of multiple tenants and hence, tenant-admin of multiple tenant. IAMS provides APIs to allow management of the followings: UserTenantRoleGroupResourceScopePermission Please refer to the API documentation for more information. The following table document what user with system-admin and tenant-admin roles can invoke: Administration Tasks system-admin tenant-admin User Management List user ✔️ ✔️ Create user ✔️ Delete user ✔️ Update user ✔️ Get user ✔️ ✔️ Tenant Management List tenants ✔️ ✔️ Only return tenants that he is administrator to. Create tenant ✔️ Create tenant ✔️ ✔️ Only return tenants that he is administrator to. Delete Tenant ✔️ ✔️ Only return tenants that he is administrator to. Get tenant ✔️ ✔️ Only can access tenant he is administrator to. Tenant Memberships Management List member of a tenant ✔️ ✔️ Only can access tenant he is administrator to. Add user as member to tenant ✔️ ✔️ Only can access tenant he is administrator to. Remove user from tenant ✔️ ✔️ Only can access tenant he is administrator to. List memberships of user ✔️ List user not member of tenant ✔️ ✔️ Only can access tenant he is administrator to. System Administrator Management Assign user with system-admin role ✔️ Un-assign user with system-admin role ✔️ List user with system-admin role. ✔️ Tenant Administrator Management List all user with tenant-admin role ✔️ ✔️ Only can access tenant he is administrator to. Assign user as tenant-admin ✔️ ✔️ Only can access tenant he is administrator to. The assignee need to be member of the tenant. Un-assign user as tenant-admin ✔️ ✔️ Only can access tenant he is administrator to. Tenant Group Management List top level tenant groups ✔️ ✔️ Only can access tenant he is administrator to. Create top-level tenant group ✔️ ✔️ Only can access tenant he is administrator to. List subgroup from a group ✔️ ✔️ Only can access tenant he is administrator to. Create subgroup to a group ✔️ ✔️ Only can access tenant he is administrator to. Get group ✔️ ✔️ Only can access tenant he is administrator to. Delete group ✔️ ✔️ Only can access tenant he is administrator to. Add user to group ✔️ ✔️ Only can access tenant he is administrator to. Remove user from group ✔️ ✔️ Only can access tenant he is administrator to. List users in group ✔️ ✔️ Only can access tenant he is administrator to. List groups user is in ✔️ ✔️ Only can access tenant he is administrator to. List roles assigned to group ✔️ ✔️ Only can access tenant he is administrator to. Assign role to group ✔️ ✔️ Only can access tenant he is administrator to. Un-assign role from group ✔️ ✔️ Only can access tenant he is administrator to. List roles that can be assign to group ✔️ ✔️ Only can access tenant he is administrator to. Tenant Role Management List roles in tenant ✔️ ✔️ Only can access tenant he is administrator to. Create a role ✔️ ✔️ Only can access tenant he is administrator to. Delete role ✔️ ✔️ Only can access tenant he is administrator to. List user assigned with a role ✔️ ✔️ Only can access tenant he is administrator to. Assign users with a role ✔️ ✔️ Only can access tenant he is administrator to. Un-assign users from a role ✔️ ✔️ Only can access tenant he is administrator to. List of users that can be assigned with role ✔️ ✔️ Only can access tenant he is administrator to. List roles assigned to user ✔️ ✔️ Only can access tenant he is administrator to. Assign roles to user ✔️ ✔️ Only can access tenant he is administrator to. Un-assign roles from user ✔️ ✔️ Only can access tenant he is administrator to. List roles that can be assigned to user ✔️ ✔️ Only can access tenant he is administrator to. Tenant Scope Management List all scopes ✔️ ✔️ Only can access tenant he is administrator to. Create a scope ✔️ ✔️ Only can access tenant he is administrator to. Delete scope ✔️ ✔️ Only can access tenant he is administrator to. Update scope ✔️ ✔️ Only can access tenant he is administrator to. List resources with scope ✔️ ✔️ Only can access tenant he is administrator to. Tenant Resource Management List all resources ✔️ ✔️ Only can access tenant he is administrator to. Create resource ✔️ ✔️ Only can access tenant he is administrator to. Get resource ✔️ ✔️ Only can access tenant he is administrator to. Update resource ✔️ ✔️ Only can access tenant he is administrator to. Delete resource ✔️ ✔️ Only can access tenant he is administrator to. Add scope to resource ✔️ ✔️ Only can access tenant he is administrator to. Remove scope from resource ✔️ ✔️ Only can access tenant he is administrator to. List all resource scopes ✔️ ✔️ Only can access tenant he is administrator to. Tenant Resource Permissions List all users granted scoped access to a resource ✔️ ✔️ Only can access tenant he is administrator to. Grant users with a scoped access to a resource ✔️ ✔️ Only can access tenant he is administrator to. List all roles granted scoped access to a resource ✔️ ✔️ Only can access tenant he is administrator to. Grant roles with a scoped access to a resource ✔️ ✔️ Only can access tenant he is administrator to. List all groups granted scoped access to a resource ✔️ ✔️ Only can access tenant he is administrator to. Grant groups with a scoped access to a resource ✔️ ✔️ Only can access tenant he is administrator to.","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/tag/overview","content":"Overview The tag module is a simple service that's designed to share tags with other services. Other services utilizing the tag service is expected to create its own association tables - associating the entity's id with the tag id. note TODO: These docs need to be written!","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/rnr/quickstart","content":"","keywords":"","version":"Next"},{"title":"Pre-requisites​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/rnr/quickstart#pre-requisites","content":" ","version":"Next","tagName":"h2"},{"title":"Required Services​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/rnr/quickstart#required-services","content":" postgres (live db)postgres (replay db)nats (jetstream enable)minio (store db snapshot)Debezium (nats connector)  note Debezium should be configured to capture change data event on postgres (live db). Refer to this document for how to configure debezium with nats.  ","version":"Next","tagName":"h3"},{"title":"Running Locally​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/rnr/quickstart#running-locally","content":" Clone the repository:  git clone https://github.com/mssfoobar/ar2-rnr.git   Each module have their own { service_name }.env.sample file. Copy and remove the .sample in repository root directory. Provide the necessary variables inside the .env file. Start services by go command  go run cmd/replay-manager/main.go -c ./.replay-manager.env go run cmd/db-operator/main.go -c ./.db-operator.env go run cmd/msg-operator/main.go -c ./.msg-operator.env   ","version":"Next","tagName":"h2"},{"title":"Running with Docker​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/rnr/quickstart#running-with-docker","content":" Install DockerInstall Docker-ComposeDocker Post-installation setupClone  git clone https://github.com/mssfoobar/ar2-rnr.git   Start docker-compose  docker-compose up -d   Cleanup Docker  docker-compose down   warning By this point, record &amp; replay services will be up and running. You can verify the service liveness by hitting at the liveness &amp; readiness endpoint. Even though service is alive you won't be able to start the playback session yet since there is no db snapshot to start the playback session. Refer to this infra repo on how we set up periodic backup &amp; clean cronjob. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/rtus/overview","content":"Overview note TODO: These docs need to be written!","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/tag/quickstart","content":"","keywords":"","version":"Next"},{"title":"Pre-requisites​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/tag/quickstart#pre-requisites","content":" Required:  Docker  ","version":"Next","tagName":"h2"},{"title":"Running Locally​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/tag/quickstart#running-locally","content":" Clone Repo  git clone https://github.com/mssfoobar/tag.git   Copy and remove the .sample in repository root directory. Provide the necessary variables inside the .env file. Start services by running command  go run cmd/tag/main.go   ","version":"Next","tagName":"h2"},{"title":"Docker setup​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/tag/quickstart#docker-setup","content":" Install DockerInstall Docker-ComposeDocker Post-installation setupClone  git clone https://github.com/mssfoobar/ar2-ims.git   Start docker-compose  docker-compose up -d   Cleanup Docker  docker-compose down  ","version":"Next","tagName":"h2"},{"title":"Nats","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/development/nats","content":"Nats We used nats subject-based messaging within UCS services. Below is the list of nats subject in use. UPDATEROOM_TOPIC string = &quot;roomUpdates.{sid}&quot; STARTRECORDING_TOPIC string = &quot;recordingStart&quot; ENDRECORDING_TOPIC string = &quot;recordingEnd.{sid}&quot; STARTPLAYBACK_TOPIC string = &quot;playbackStart&quot; ENDPLAYBACK_TOPIC string = &quot;playbackEnd.{sid}&quot; DELETEPLAYBACK_TOPIC string = &quot;playbackDelete.{sid}&quot; PAUSEPLAYBACK_TOPIC string = &quot;playbackPause.{sid}&quot; PLAYBACK_TOPIC string = &quot;playback.{sid}&quot; INSERT_CALLUSERID_TOPIC string = &quot;insertCallUserId.{sid}&quot; DELETE_CALLUSERID_TOPIC string = &quot;deleteCallUserId.{sid}&quot; INSERT_CHATUSERID_TOPIC string = &quot;insertChatUserId.{sid}&quot; DELETE_CHATUSERID_TOPIC string = &quot;deleteChatUserId.{sid}&quot; ","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/tag/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/tag/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/tag/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/tag/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/deployment","content":"","keywords":"","version":"Next"},{"title":"Building Images​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#building-images","content":" Building all services in docker-compose file.  docker-compose build   Building a specific service.  docker-compose build {service-name}   warning It is important to build the image following below naming convention. Otherwise, pushing to ghcr will fail. ghcr.io/NAMESPACE/IMAGE_NAME:tag NAMESPACE must be personal account or organization to which the image will be scoped to.  ","version":"Next","tagName":"h2"},{"title":"Pushing to Container Registry​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#pushing-to-container-registry","content":" This project used ghcr (GitHub container registry) to store images.  Working with ghcr  create a new GitHub personal access token with at least write:pacakges access.login to ghcr using cli.  export CR_PAT=YOUR_TOKEN echo $CR_PAT | docker login ghcr.io -u USERNAME --password-stdin   push  docker push ghcr.io/NAMESPACE/IMAGE_NAME:latest   For more details, refer to GitHub official documents  ","version":"Next","tagName":"h2"},{"title":"Deploy UCS in Docker Container​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#deploy-ucs-in-docker-container","content":" docker-compose up -d --remove-orphans   info UCS services use both .env and .toml. .env is used for passing secret keys and .toml is for configuration.  Configuring docker-compose file.  Set env variable   environment: - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY} - MINIO_SECRET_KEY=${MINIO_SECRET_KEY} - POSTGRESQL_USER=${POSTGRESQL_USER} - POSTGRESQL_PASSWORD=${POSTGRESQL_PASSWORD}   Set toml config file location   volumes: - &quot;./configs/docker/app-room-mgmt.toml:/configs/app-room-mgmt.toml&quot;   ","version":"Next","tagName":"h2"},{"title":"Deploy UCS in kubernetes​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#deploy-ucs-in-kubernetes","content":" ","version":"Next","tagName":"h2"},{"title":"Preparing Manifest Repo​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#preparing-manifest-repo","content":" Create a new repository with below directory for UCS services.  Create three yaml files for each UCS service.  . └── manifests ├── {service-name}-config.yaml ├── {service-name}-deployment.yaml ├── {service-name}-service.yaml   For detail explanation of yaml files, refer to official documents for:  deployment.yamlservice.yamlconfig.yaml  You can use kompose to easily convert from docker-compose.yaml into deployment.yaml &amp; service.yaml.  kompose convert -f docker-compose.yaml   config.yaml is to load ucs config .toml into deployment.  Example config.yaml.  apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: null name: app-room-mgmt-config namespace: common-ucs data: app-room-mgmt.toml: | [log] level = &quot;info&quot; ...   And mount the config.yaml in deployment.yaml volumes.  apiVersion: apps/v1 kind: Deployment spec: template: spec: volumes: - name: app-room-mgmt-claim0 configMap: name: app-room-mgmt-config   ","version":"Next","tagName":"h3"},{"title":"Deploy Using ArgoCD​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#deploy-using-argocd","content":" If ArgoCD is set up for kubernetes deployment, you just have to create entrypoint for ArgoCD to find the manifest repo.  Create a yaml inside the repo where ArgoCD will look for entrypoint.  Make sure to provide the repoURL and path of manifest repo.  apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: ucs namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: common-ucs name: in-cluster project: appbundle-project-ar2-dev source: path: manifests repoURL: https://github.com/example/ucs targetRevision: main syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true   Check if the Deployment was created using ArgoCD dashboard.  ","version":"Next","tagName":"h3"},{"title":"Deploy Using kubectl​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#deploy-using-kubectl","content":" Before you begin, make sure your Kubernetes cluster is up and running. Follow the steps given below to create the above Deployment:  Create the Deployment by running the following command:  kubectl apply -f https://github.com/example/ucs/manifest/app-room-mgmt-deployment.yaml   Run kubectl get deployments to check if the Deployment was created.  ","version":"Next","tagName":"h3"},{"title":"CI/CD Pipeline​","type":1,"pageTitle":"Deployment","url":"/docs/docs/modules/ucs/deployment#cicd-pipeline","content":" Every commit to main branch will trigger the GitHub action workflow.  Following actions will be triggered by workflow -  build &amp; push the images into ghcr (GitHub container registry)update the image tags in manifest files of staging server  Upon the update of manifest files, ArgoCD will pick up new images' tag to deploy latest services.  Refer to publish.yml inside .github/workflows for more details. ","version":"Next","tagName":"h2"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/unh/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/unh/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/unh/development","content":"Development","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/quickstart","content":"","keywords":"","version":"Next"},{"title":"Pre-requisites​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ucs/quickstart#pre-requisites","content":" Required:  DockerK3s  ","version":"Next","tagName":"h2"},{"title":"Running Locally​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ucs/quickstart#running-locally","content":" For configuration of the forked services, please refer to official documentations from the referenced repositories:-  ION - ION backend services: SFU, signal, app-room, islbION-app-web  ","version":"Next","tagName":"h2"},{"title":"Docker setup​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ucs/quickstart#docker-setup","content":" Install DockerInstall Docker-ComposeDocker Post-installation setupClone  git clone https://github.com/mssfoobar/VidConf.git   Change &quot;localhost&quot; to hostname or domain name in file VidConf/VidConf-ion-app-web/CaddyfileUpdate the postgresql/minio/systemUserId configuration for the following:- ./VidConf-ion/configs/docker/app-room.toml./VidConf-ion/configs/docker/app-room-mgmt.toml./VidConf-ion/configs/docker/app-room-sentry.toml./VidConf-ion/configs/docker/app-room-recorder.toml./VidConf-ion/configs/docker/app-room-playback.toml Setup   cd VidConf/ ./scripts/setupDocker.sh   VerifyChat: https://localhost:9090Stop containers   ./scripts/stopDocker.sh   Start containers   ./scripts/startDocker.sh   Cleanup Docker   ./scripts/cleanupDocker.sh   ","version":"Next","tagName":"h3"},{"title":"K3s setup:-​","type":1,"pageTitle":"Quickstart","url":"/docs/docs/modules/ucs/quickstart#k3s-setup-","content":" Install K3sInstall krelay for UDP port forwardingClone project   git clone https://github.com/mssfoobar/VidConf.git   Configure IP address - Update local IP address inside file: web-caddy-file---configmap.yamlSetup   ./scripts/setupK3s.sh k3s kubectl relay --address 0.0.0.0 deployment/sfu 5000:5000 k3s kubectl port-forward deployment/signal 5551:5551 --address='0.0.0.0' k3s kubectl port-forward deployment/sfu 3478:3478 --address='0.0.0.0' k3s kubectl port-forward deployment/nats 4222:4222 --address='0.0.0.0' k3s kubectl port-forward deployment/redis 6379:6379 --address='0.0.0.0' k3s kubectl port-forward deployment/web 9090:9090 --address='0.0.0.0'   Verify Chat: http://local_ip_address:9090 ","version":"Next","tagName":"h3"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/unh/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ucs/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/ucs/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/unh/overview","content":"Overview","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/unh/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/web-base/development","content":"Development","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/unh/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/unh/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/unh/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/web-base/overview","content":"","keywords":"","version":"Next"},{"title":"Key Features​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/web-base/overview#key-features","content":" Public/Private Route Separation: The template includes a built-in routing system that separates protected and public routes, ensuring that sensitive areas of the application are only accessible to authenticated users while providing that control to the user.Keycloak Authentication: Web-base integrates with Keycloak, a popular open-source identity and access management solution, to provide secure authentication and authorization as well as multitenancy.Basic UI Components: The template includes a set of basic UI components, such as navigation, buttons, and forms, to help you get started with building your frontend module.  ","version":"Next","tagName":"h2"},{"title":"Pre-requisites:​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/web-base/overview#pre-requisites","content":" To perform front-end development effectively with the web-base, you will need to know:  TypeScript: https://www.typescriptlang.org/docs/handbook/typescript-from-scratch.htmlSvelte: https://svelte.dev/tutorial/svelte/welcome-to-svelteSvelte Kit: https://svelte.dev/tutorial/kit/introducing-sveltekitTailwind CSS: https://tailwindcss.com/docs/utility-first  Additional Reading:  Shadcn-Svelte: https://next.shadcn-svelte.com/  A brief description and justification for these technologies can be found at ourweb technologies section. ","version":"Next","tagName":"h2"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/web-base/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/web-base/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/ims/reference/faq","content":"","keywords":"","version":"Next"},{"title":"Example: How do I directly access my local development database?​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#example-how-do-i-directly-access-my-local-development-database","content":" When running the dev-containers, add an override file with port mappings to allow your database client to connect to the database running in the Docker network.  name: aoh serivces: my-module-db: 5500:5432   Include the override file when you up your compose file.  docker compose --env-file .env -f iams/compose.yml -f override.yml up -d   ","version":"Next","tagName":"h2"},{"title":"Example: How can I grow a bigger chest?​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#example-how-can-i-grow-a-bigger-chest","content":" Do more chest exercises such as:  Bench PressDipsSeated Chest Press  Gradually increase the number of sets you do each week.  ","version":"Next","tagName":"h2"},{"title":"Gists​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#gists","content":" If necessary and separate from the FAQ section, you can provide lists of commands, code recipes etc. here for users to copy-paste. These should be very short sections.  The gists below is a set of useful Docusaurus references to help you with writing your documentation for Docusaurus and is itself a good example of what gists should be.  ","version":"Next","tagName":"h2"},{"title":"Docusaurus Reference​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#docusaurus-reference","content":" The following is a list of useful examples for reference when editing the markdown files on Docusaurus.  note Formatting is very important for MDX: https://github.com/facebook/docusaurus/issues/3890You must follow the formatting somewhat strictly (especially excluding spaces at the start of the sentence).  ","version":"Next","tagName":"h2"},{"title":"Images​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#images","content":"   Code to render above example:  ![Svelte Logo](/img/sample.png)   ","version":"Next","tagName":"h3"},{"title":"Tabs​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#tabs","content":"     WindowsmacOSLinux shutdown -t 0 -s   Code to render above example:  &lt;!-- You must import the React components --&gt; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; &lt;Tabs&gt; &lt;TabItem value=&quot;Windows&quot; label=&quot;Windows&quot; default&gt; ```bash shutdown -t 0 -s ``` &lt;/TabItem&gt; &lt;TabItem value=&quot;macOS&quot; label=&quot;macOS&quot;&gt; ```bash sudo shutdown -h now ``` &lt;/TabItem&gt; &lt;TabItem value=&quot;Linux&quot; label=&quot;Linux&quot;&gt; ```bash sudo shutdown -h now ``` &lt;/TabItem&gt; &lt;/Tabs&gt;   ","version":"Next","tagName":"h3"},{"title":"Codeblocks​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#codeblocks","content":" /src/components/HelloCodeTitle.js function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; }   Code to render above example:  ```jsx title=&quot;/src/components/HelloCodeTitle.js&quot; function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; } ```   ","version":"Next","tagName":"h3"},{"title":"Equations​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#equations","content":" $$ I = \\int_0^{2\\pi} \\sin(x),dx $$  Code to render above example:  $$ I = \\int_0^\\{2\\pi\\} \\sin(x)\\,dx $$   ","version":"Next","tagName":"h3"},{"title":"Admonitions​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#admonitions","content":" note Some content with Markdown syntax. Check this api.  tip Some content with Markdown syntax. Check this api.  info Some content with Markdown syntax. Check this api.  caution Some content with Markdown syntax. Check this api.  danger Some content with Markdown syntax. Check this api.  Code to render above examples:  :::note Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::tip Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::info Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::caution Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::danger Some **content** with _Markdown_ `syntax`. Check [this `api`](#). :::   ","version":"Next","tagName":"h3"},{"title":"Line Highlighting​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#line-highlighting","content":" function HighlightSomeText(highlight) { if (highlight) { return &quot;This text is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; } function HighlightMoreText(highlight) { if (highlight) { return &quot;This range is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; }   Code to render above example:  ```js function HighlightSomeText(highlight) { if (highlight) { // highlight-next-line return &quot;This text is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; } function HighlightMoreText(highlight) { // highlight-start if (highlight) { return &quot;This range is highlighted!&quot;; } // highlight-end return &quot;Nothing highlighted&quot;; } ```   ","version":"Next","tagName":"h3"},{"title":"Line Numbering​","type":1,"pageTitle":"FAQ","url":"/docs/docs/modules/ims/reference/faq#line-numbering","content":" import React from &quot;react&quot;; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent;   Code to render above example:  ```jsx {1,4-6,11} showLineNumbers import React from &quot;react&quot;; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; ```  ","version":"Next","tagName":"h3"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/wfe/configuration","content":"Configuration","keywords":"","version":"Next"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/web-base/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/web-base/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/web-base/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/web-base/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/configuration","content":"","keywords":"","version":"Next"},{"title":"SFU Configuration​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ucs/configuration#sfu-configuration","content":" If browser cannot establish the ice candidate pair with SFU, consider mapping external ip address to sfu.toml file.  # if the sfu is deployed in a DMZ between two 1-1 NAT for internal and # external users. nat1to1 = [&quot;xxx.xxx.xxx.xxx&quot;] icelite = true   Or use stun/turn server  [[webrtc.iceserver]] urls = [&quot;stun:example:3478&quot;] [[webrtc.iceserver]] urls = [&quot;turn:turn.example.org:3478&quot;] username = &quot;username&quot; credential = &quot;password&quot;   Or enable embeded sfu turn server  [turn] # Enables embeded turn server enabled = true # Sets the realm for turn server realm = &quot;ion&quot; # The address the TURN server will listen on. address = &quot;0.0.0.0:3478&quot; # Certs path to config tls/dtls # cert=&quot;path/to/cert.pem&quot; # key=&quot;path/to/key.pem&quot; # Port range that turn relays to SFU # WARNING: It shouldn't overlap webrtc.portrange # Format: [min, max] # portrange = [5201, 5400] [turn.auth] # Use an auth secret to generate long-term credentials defined in RFC5389-10.2 # NOTE: This takes precedence over `credentials` if defined. # secret = &quot;secret&quot; # Sets the credentials pairs credentials = &quot;pion=ion,pion2=ion2&quot;   info credentials pair are separated by '='. pion=ion means username = pion and crendential = ion  ","version":"Next","tagName":"h2"},{"title":"Browser Configuration​","type":1,"pageTitle":"Configuration","url":"/docs/docs/modules/ucs/configuration#browser-configuration","content":" If SFU is configured to be used with stun/turn server, please specific the stun/turn in browser configuration.  const config = { iceServers: [ { urls: [&quot;turn:turn.example.org:3478&quot;], username: &quot;username&quot;, credential: &quot;password&quot;, }, ], };  ","version":"Next","tagName":"h2"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/wfe/development","content":"Development","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/docs/docs/modules/wfe/quickstart","content":"Quickstart","keywords":"","version":"Next"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/docs/docs/modules/wfe/deployment","content":"Deployment","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/wfe/overview","content":"Overview","keywords":"","version":"Next"},{"title":"Architecture","type":0,"sectionRef":"#","url":"/docs/docs/overview/design/architecture","content":"Architecture","keywords":"architecture diagram","version":"Next"},{"title":"Scope","type":0,"sectionRef":"#","url":"/docs/docs/overview/features/scope","content":"Scope Problems that AGIL Ops Hub is designed to solve What AGIL Ops Hub is not meant to cover","keywords":"features modules","version":"Next"},{"title":"Philosophy","type":0,"sectionRef":"#","url":"/docs/docs/overview/design/philosophy","content":"Philosophy","keywords":"philosophy","version":"Next"},{"title":"Packaging Strategy","type":0,"sectionRef":"#","url":"/docs/docs/overview/design/packaging-strategy","content":"","keywords":"distribution packaging release development source code","version":"Next"},{"title":"Typical Structure of Module Components​","type":1,"pageTitle":"Packaging Strategy","url":"/docs/docs/overview/design/packaging-strategy#typical-structure-of-module-components","content":"   This results in a few types of packages that might built for each module  Container Images: in a production environment, this would typically be deployed by a container orchestrator such as Kubernetes. In development environments, it is up to projects to decide how they will work, but this would typically be via running Docker compose / swarm or a locak k3s cluster.Web Libraries: these libraries might contain ready-to-use components or pages specific to the moduleDeployment Files: these would be Kubernetes helm charts to facilitate adapting the modules to projects' infrastructural needs  ","version":"Next","tagName":"h2"},{"title":"Application Structure / Release Artifacts​","type":1,"pageTitle":"Packaging Strategy","url":"/docs/docs/overview/design/packaging-strategy#application-structure--release-artifacts","content":"   The Web Base is a template repository, containing the minimum components required by all modules with a front-end. This includes the authentication framework, as well as other basic commonly needed web components such as menu items for navigation, and themes. The Web Core is a set of libraries that each modules' web libraries would depend on to ensure they function within the web framework/base. Module... comprises of each modules' Container Images, Web Libraries and Deployment Files. The Sample App is a reference architecture that projects can refer to on how to use all the modules together, this is not a release artifact for them to use, they would start with the Web Base and add Modules in as desired, however, the Sample App exists as an integration and test environment to validate all the modules can work together cohesively. ","version":"Next","tagName":"h2"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/wfe/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Features by Module","type":0,"sectionRef":"#","url":"/docs/docs/overview/features/modules","content":"","keywords":"features modules","version":"Next"},{"title":"Web Base​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#web-base","content":" The Web Base is a web server based on Svelte Kit and includes authentication and integration with our own packaging tools to allow quick installation of different modules. Our web modules are distrubted as source code for complete customizability.  Key Features:  Quickly build powerful Progressively Enhanced Single Page Applications (PESPA)Built in authentication via integration with IAMSComponent FrameworkCustomizable Base Design System ThemingForm Validation  ","version":"Next","tagName":"h2"},{"title":"IAMS - Identity & Access Management Service​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#iams---identity--access-management-service","content":" The IAMS module is based on Keycloak and extends it to support multi-tenancy and simplifies authorization to allow extreme flexibility to configure access for complex domain-specific requirements.  Key Features:  Identity ManagementAuthenticationAuthorizationMulti-tenancy  ","version":"Next","tagName":"h2"},{"title":"User Details Page​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#user-details-page","content":"   ","version":"Next","tagName":"h3"},{"title":"RTUS - Real-time Update Service​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#rtus---real-time-update-service","content":" Key Features:  Update your web browser live via Server-Sent EventsDistributed cache to enable scaling up  ","version":"Next","tagName":"h2"},{"title":"UNH - Universal Notifications Hub​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#unh---universal-notifications-hub","content":" Key Features:  Built-in email and push notification supportSupport to extend and add custom notifications channelsConfigure where notifications will be sent with distribution listsCustomize your message templates for each notification channel  ","version":"Next","tagName":"h2"},{"title":"IAN - In-App Notifications​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#ian---in-app-notifications","content":" Receive notifications live, directly within the AGIL Ops Hub system.  ","version":"Next","tagName":"h2"},{"title":"WFE - Workflow Engine​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#wfe---workflow-engine","content":" Key Features:  Design custom workflows with a web UI based on industry standards - BPMNDistributed workflow engineCustomizable workflow activities  ","version":"Next","tagName":"h2"},{"title":"IMS - Incident Management System​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#ims---incident-management-system","content":" Key Features:  Manage incidents and their statusesIntegrated with our Workflow Engine (WFE)  ","version":"Next","tagName":"h2"},{"title":"DASH - Dashboarding​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#dash---dashboarding","content":" Key Features:  Drag &amp; drop widget interfaceCustomizable widgetsCustomizable widget configuration  ","version":"Next","tagName":"h2"},{"title":"RNR - Record & Replay​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#rnr---record--replay","content":" ","version":"Next","tagName":"h2"},{"title":"UCS - Universal Communications System​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#ucs---universal-communications-system","content":" System-wide video, audio, and text chat messaging.  ","version":"Next","tagName":"h2"},{"title":"GIS - Geospatial Information System​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#gis---geospatial-information-system","content":" The GIS service provides web components as well as backend services to enable  Live track displayBookmarking  ","version":"Next","tagName":"h2"},{"title":"Map Display Page​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#map-display-page","content":"   ","version":"Next","tagName":"h3"},{"title":"PTMGR - Push Token Manager​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#ptmgr---push-token-manager","content":" An application server to handle Firebase Cloud Messaging token lifecycle management. This is required to for push notifications to mobile phones.  ","version":"Next","tagName":"h2"},{"title":"TAG - Tagging​","type":1,"pageTitle":"Features by Module","url":"/docs/docs/overview/features/modules#tag---tagging","content":" An extremely simple service used to allow other modules to share tags. ","version":"Next","tagName":"h2"},{"title":"🔧 Testing","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/development/testing","content":"","keywords":"","version":"Next"},{"title":"SFU load testing tool​","type":1,"pageTitle":"🔧 Testing","url":"/docs/docs/modules/ucs/development/testing#sfu-load-testing-tool","content":" Clone the vidconf-loadtest repository  git clone https://github.com/mssfoobar/vidconf-loadtest.git   ","version":"Next","tagName":"h2"},{"title":"Test File​","type":1,"pageTitle":"🔧 Testing","url":"/docs/docs/modules/ucs/development/testing#test-file","content":" Publishing of files in the following formats are supported.  Container\tVideo Codecs\tAudioWEBM\tVP8\tOPUS  If your data is not webm, you can use ffmpeg to make one This show how to make a 0.5Mbps webm:  ffmpeg -i djrm480p.mp4 -strict -2 -b:v 0.4M -vcodec libvpx -acodec opus djrm480p.webm   See the ffmpeg docs on VP8 for encoding options  ","version":"Next","tagName":"h3"},{"title":"Quick Start​","type":1,"pageTitle":"🔧 Testing","url":"/docs/docs/modules/ucs/development/testing#quick-start","content":" Change necessary Makefile variables  # number of client publishing the video stream pubClient = 1 # number of client subscribing the video stream subClient = 10 # signal address signalAddr = 127.0.0.1:5551 # session id sessionId = ion # duration the load testing tool will run duration = 600   Use makefile commands to start the tool in docker  make build make start   ","version":"Next","tagName":"h3"},{"title":"Run tool in another Linux server​","type":1,"pageTitle":"🔧 Testing","url":"/docs/docs/modules/ucs/development/testing#run-tool-in-another-linux-server","content":" # build a linux version, we test on linux because mac fd limit env GOOS=linux go build -o sfu-loadtest ./sfu-loadtest/main.go   # pub.sh #./sfu-loadtest -file ./djrm480p.webm -clients 1 -role pubsub -gaddr &quot;yoursfuip:5551&quot; -session 'ion' -log debug -cycle 1000 -a -v # sub.sh #../sfu-loadtest -file /Volumes/vm/media/djrm480p.webm -clients 10 -role sub -gaddr &quot;127.0.0.1:5551&quot; -session 'ion'  ","version":"Next","tagName":"h3"},{"title":"Infrastructure Technologies","type":0,"sectionRef":"#","url":"/docs/docs/overview/technologies/infrastructure","content":"","keywords":"","version":"Next"},{"title":"Technologist List​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#technologist-list","content":" The following is a non-exhaustive list of technologies we use in the management of our infrastructure.  Docker :Kubernetes :Traefik : Cloud-native reverse-proxy.ArgoCD : GitOps    ","version":"Next","tagName":"h2"},{"title":"Docker​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#docker","content":" Docker is an open source container provider. Essentially, we are containerizing and distributing our applications as containers. We're directly dependent on Docker, but Docker &amp; containers has become synonymous.  We're actually officially using Podman but projects are free to purchase and use Docker Desktop if it suites them.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Docker​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#why-weve-chosen-docker","content":" Any OCI (Open Container Initiative) compliant container engine should be sufficient.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#links","content":" Official Docker WebsiteOfficial Podman WebsiteOfficial Containerd Website    ","version":"Next","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#kubernetes","content":" In order to support a micro-services architecture, we need to be able to orchestrate the automatic scaling or services. Our services need to be containerized and monitored, stood up or down depending on their needs, and have their traffic routed accordingly.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Kubernetes​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#why-weve-chosen-kubernetes","content":" The de-facto standard across the industry for container orchestration is Kubernetes, which is what we've adopted.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#links-1","content":" Official Kubernetes Website    ","version":"Next","tagName":"h3"},{"title":"Traefik​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#traefik","content":" Traefik is a cloud-native HTTP reverse proxy and load balancer designed for use with microservices.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Traefik​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#why-weve-chosen-traefik","content":" We use Traefik as a reverse proxy and load balancer to manage access to cluster services in Kubernetes. It is open source, well-documented, and is actively maintained and developed by a large community of users and contributors.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#links-2","content":" Official Traefik Website    ","version":"Next","tagName":"h3"},{"title":"ArgoCD​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#argocd","content":" Declarative continuous delivery GitOps tool with a fully-loaded UI built for Kubernetes.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen ArgoCD​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#why-weve-chosen-argocd","content":" ArgoCD allows to easily automate deployments using Git repositories as the source of truth.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Infrastructure Technologies","url":"/docs/docs/overview/technologies/infrastructure#links-3","content":" Official ArgoCD WebsiteWhat is GitOps? ","version":"Next","tagName":"h3"},{"title":"Known Issues","type":0,"sectionRef":"#","url":"/docs/docs/modules/wfe/reference/known-issues","content":"","keywords":"","version":"Next"},{"title":"Example Known Issue 1​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/wfe/reference/known-issues#example-known-issue-1","content":" Example problem occurs when example thing happens.  ","version":"Next","tagName":"h2"},{"title":"Example Known issue 2​","type":1,"pageTitle":"Known Issues","url":"/docs/docs/modules/wfe/reference/known-issues#example-known-issue-2","content":" Example problem only occurs when example known issue 1 doesn't occur. ","version":"Next","tagName":"h2"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/docs/modules/web-base/reference/faq","content":"FAQ This section is meant to address and document common questions, mistakes, errors, and pitfalls that people might run into.","keywords":"","version":"Next"},{"title":"Backend Technologies","type":0,"sectionRef":"#","url":"/docs/docs/overview/technologies/backend","content":"","keywords":"","version":"Next"},{"title":"Technology List​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#technology-list","content":" The following is a non-exhaustive list of technologies chosen to support backend development on our platform.  Keycloak : Identity and access management, as well as authorization.Temporal : Microservice orchestration framework.Open API : HTTP API specification standard.Tile38 : Geofencing server.PostgreSQL : Relational database.  Other Notable Libraries:​  Pion : Open-source Golang implementation of WebRTC.Golang Test Package : Golang's built-in testing framework.Allure Framework : Reporting tool for tests.  ","version":"Next","tagName":"h2"},{"title":"Keycloak​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#keycloak","content":" Keycloak is an open-source identity and access management (IAM) solution developed by Red Hat. Their enterprise fork is called Red Hat Single-Sign On (SSO). It follows the OpenID Connect and OAuth 2.0 standards and it highly extensible, allowing it to integrate seamlessly with many other applications, frameworks, and even perform user federation and integration with LDAP providers.  As Keycloak acts as our IAM server, it handles the generation of JSON Web Tokens JWT's as well as authentication. Managing Keycloak itself is a large topic, and requires some understanding of OIDC authentication flows, as well as the the security concepts surrounding authentication using JWT's.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Keycloak​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#why-weve-chosen-keycloak","content":" Red Hat is one of the leading providers of open-source security solutions. They are highly reputable and are leading the development of Keycloak. With it's backing of a strong security-focused company, quick development pace, as well as the ability to provide enterprise support, Keycloak is the obvious choice for for providing OIDC-compliant, cloud-native identity and access management services.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#links","content":" KeycloakOIDC and OAuth    ","version":"Next","tagName":"h3"},{"title":"Temporal​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#temporal","content":" Temporal is an open-source microservice orchestration platform for writing durable workflow as code. It simplifies the difficult problem of distributing workloads across workers, ensuring that they run to completion, performing compensating actions when issues arise, and provides versioning of workflows which is especially necessary for to be able to execute long-running workflows without interruption.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Temporal​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#why-weve-chosen-temporal","content":" Temporal's team has a long history of running workflows in the cloud; with a lot of that experience coming from working on large distributed systems at AWS, Microsoft, and Uber. The concepts behind Temporal are elegant and sound, and their company is growing quickly (with one of the highest series B funding rounds in history, including hitting a unicorn valuation at that stage).  They have SDK's that support many languages including Golang, TypeScript, Java, Python, and .NET.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#links-1","content":" Temporal    ","version":"Next","tagName":"h3"},{"title":"Open API​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#open-api","content":" Open API, formerly known as Swagger currently is the most widely adopted API specification standard for HTTP API's.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Open API​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#why-weve-chosen-open-api","content":" There aren't very many other options, especially ones with strong tooling. AsyncAPI is available for asynchronous API's, but that's about it.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#links-2","content":" Open API    ","version":"Next","tagName":"h3"},{"title":"Tile38​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#tile38","content":" Tile38 is a geospatial database &amp; geofencing server. It allows us to store and make fast queries on geospatial entities in a scalable fashion.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Tile38​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#why-weve-chosen-tile38","content":" Tile38 is well design and free and open-source. Other geofencing options are significantly more costly and usually requires integration with a much larger body of products, which might not make sense without going with the full suite of products. Tile38 is a high-quality stand-alone solution that gives us flexibility.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#links-3","content":" Tile38    ","version":"Next","tagName":"h3"},{"title":"PostgreSQL​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#postgresql","content":" PostgreSQL is a highly popular, advanced open-source relational database with a long history.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen PostgreSQL​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#why-weve-chosen-postgresql","content":" PostgreSQL's wide-adoption and maturity means it is very stable, and problems that are discovered with it have a lot of manpower behind it to resolve those issues quickly. It is also very feature rich, with many plugins, resources, and companies that provide enterprise support, and ones that provide distributed versions of it.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Backend Technologies","url":"/docs/docs/overview/technologies/backend#links-4","content":" PostgreSQLYugabyteDB ","version":"Next","tagName":"h3"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/overview","content":"","keywords":"","version":"Next"},{"title":"Supported Features:​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/ucs/overview#supported-features","content":" Containerised deployment with DevOps consideration in placeWeb-based frontend for ease of deploymentVideo conference to share video, audio, chat messages and files primarily for team-sized collaborations in the course of fulfilling daily tasksHTTPS-API for service provisioning, e.g. advance-room-bookingRecording and Playback of conference sessions to record-and-playback the data streams instead of screen capturing of conference sessionsData-encryption for all network trafficWeb-security to disable unencrpyted data through well-known portsScalable backend architectureHigh-availability architectureMax participant around 200 paxScreen-sharing for meetingsSystem-monitoring for health statusPartial-disabling of video, audio or chatISO8601 zulu timeRoomId is google UUIDQuery params for HTTP POST is inside JSON bodyKick user by userIdQuery Room by roomIdBookroom: optional prompt RelativeFrom:Start/End(Default End) RelativeTimeInSeconds:123 Message:PromptMessageEdit room bookingCancel room booking  ","version":"Next","tagName":"h2"},{"title":"Forked Projects:​","type":1,"pageTitle":"Overview","url":"/docs/docs/modules/ucs/overview#forked-projects","content":" UCS is forked from the following open-source projects:-  ION - a complete WebRTC-compatible backend suite written in golangION-app-web - a WebRTC-compatible web-frontend written in golang ","version":"Next","tagName":"h2"},{"title":"Development","type":0,"sectionRef":"#","url":"/docs/docs/modules/ucs/development","content":"","keywords":"","version":"Next"},{"title":"Message​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#message","content":" Message is the payload send through data channel.  Message can be chat, incoming/outgoing call signal or file attachment.  Please see mineType enumeration for different types of messages.  ","version":"Next","tagName":"h2"},{"title":"Schema​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#schema","content":" { &quot;uid&quot;: &quot;string&quot;, &quot;name&quot;: &quot;string&quot;, &quot;mime_type&quot;: &quot;string&quot;, &quot;text&quot;: &quot;string&quot;, &quot;attachment&quot;: { &quot;name&quot;: &quot;string&quot;, &quot;size&quot;: 0, &quot;file_path&quot;: &quot;string&quot; } }   ","version":"Next","tagName":"h3"},{"title":"MineType Enumeration​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#minetype-enumeration","content":" MineType determines the type of UCS communications.  MineType\tDescriptionmessage\tChat message call-start\tStart video/audio call call-join\tJoin video/audio call call-end\tEnd video/audio call attachment\tShare file attachment broadcast-start\tStart sharing video broadcast-end\tStop sharing video disconnect\tDisconnect from the room  ","version":"Next","tagName":"h3"},{"title":"Chat​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#chat","content":" Set mineType to message and set the value of text in message schema.  { &quot;uid&quot;: &quot;10206739&quot;, &quot;name&quot;: &quot;alex&quot;, &quot;mime_type&quot;: &quot;message&quot;, &quot;text&quot;: &quot;This is a test message&quot; }   ","version":"Next","tagName":"h2"},{"title":"Audio/Video Call​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#audiovideo-call","content":" Examples of message payloads for call.  ","version":"Next","tagName":"h2"},{"title":"Start Call​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#start-call","content":" { &quot;uid&quot;: &quot;10206739&quot;, &quot;name&quot;: &quot;alex&quot;, &quot;mime_type&quot;: &quot;call-start&quot; }   ","version":"Next","tagName":"h3"},{"title":"Join Call​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#join-call","content":" { &quot;uid&quot;: &quot;10208888&quot;, &quot;name&quot;: &quot;bob&quot;, &quot;mime_type&quot;: &quot;call-join&quot; }   ","version":"Next","tagName":"h3"},{"title":"End Call​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#end-call","content":" { &quot;uid&quot;: &quot;10208888&quot;, &quot;name&quot;: &quot;bob&quot;, &quot;mime_type&quot;: &quot;call-end&quot; }   ","version":"Next","tagName":"h3"},{"title":"File Sharing​","type":1,"pageTitle":"Development","url":"/docs/docs/modules/ucs/development#file-sharing","content":" There is a size limitation in WebRTC data channel which only accepts up to 16MB max.  We used MinIO presigned Url to upload/download file attachment. For more details on MinIO, please refer to their official documentation.  File_path is the location of MinIO object. Set this to the value received from the room management API (TODO: Provide info on what this is).  { &quot;uid&quot;: &quot;10206739&quot;, &quot;name&quot;: &quot;alex&quot;, &quot;mime_type&quot;: &quot;attachment&quot;, &quot;attachment&quot;: { &quot;name&quot;: &quot;testFile.txt&quot;, &quot;size&quot;: 100, &quot;file_path&quot;: &quot;/attachment/19649298-b4da-4f2f-9e24-c7ebdc20a766&quot; } }  ","version":"Next","tagName":"h2"},{"title":"Web Technologies","type":0,"sectionRef":"#","url":"/docs/docs/overview/technologies/web","content":"","keywords":"","version":"Next"},{"title":"Technology List​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#technology-list","content":" The following is a non-exhaustive list of the key technologies used in our web development stack.  Svelte : Web component framework.SvelteKit : Progressively-enhanced single-page application framework for Svelte.Tailwind CSS : Utility-class CSS framework.Shadcn-Svelte : Components for Svelte utilizing Tailwind CSS and Shadcn design system.Iconify : Iconography library and framework.Docusaurus : Static site generator for Markdown-based documentation.  Other Notable Libraries​  JointJS : JavaScript diagramming library.Apache ECharts : Open-source charting library.CesiumJS : Open-source JavaScript library used for creating 3D globes and maps in web browsers.urql : Extensible GraphQL client with support for caching, retries, deduplication, subscriptions and more.Vitest : Vite-native testing framework with Jest compatible API, and out-of-the-box TypeScript supportPlaywright : End-to-end web testing and automation framework.    ","version":"Next","tagName":"h2"},{"title":"TypeScript​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#typescript","content":" TypeScript is JavaScript with tooling that adds static type-checking support. Though it adds a lot build complexity and tools on top of vanilla JavaScript, it massively helps with the issue of scaling JavaScript code bases, and is an absolute must for code to be maintainable in large organisations.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen TypeScript​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#why-weve-chosen-typescript","content":" With TypeScript, code can be validated better with developer tools. Developers' intentions can be stated clearly with types, and stronger tooling allows for large code bases to be easier to understand and easier to validate.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#links","content":" Official Keycloak Website    ","version":"Next","tagName":"h3"},{"title":"Svelte​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#svelte","content":" Svelte is a component-based JavaScript framework for building user interfaces. It is much like the ever-popularReact, however, unlike React, it does not create a virtual DOM and diff changes against it. Svelte shifts that work into the compile step instead.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Svelte​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#why-weve-chosen-svelte","content":" The knee-jerk reaction you might have is why use Svelte when we already have React, a mature, popular framework with the backing of Facebook/Meta.  Svelte is easier to pick up than React and is simply easy to learn in absolute terms. It's also feature-rich and highly performant - because it does not use a virtual DOM, Svelte is fast.  Our choice to go with Svelte is also to get ahead of the curve. Trends are showing that Svelte is well-loved by the developer community and is constantly growing in popularity. Svelte was voted themost loved web frameworkin the Stack Overflows' 2021 survey won the highest satisfaction ratingsin State of JS's 2020 survey.  With Rich Harris' induction into Vercel at the end of 2021, Svelte also now has the backing of a strong web-oriented tech-company and we believe it is a safe and forward-looking choice to use as our predominant framework.  ","version":"Next","tagName":"h3"},{"title":"SvelteKit​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#sveltekit","content":" SvelteKit is a web application server framework for building extremely high-performance progressively enhanced single-page applications using Svelte.  Building an app with all the modern best practices is complicated. Those practices include build optimizations, so that you load only the minimal required code... offline support... prefetching pages before the user initiates navigation... and configurable rendering that allows you to generate HTML on the server or in the browser at runtime or at build-time. SvelteKit does all that for us.  It uses Vite with a Svelte plugin to provide a lightning-fast and feature-rich development experience with Hot Module Replacement (HMR), where changes to your code are reflected in the browser instantly.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen SvelteKit​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#why-weve-chosen-sveltekit","content":" SvelteKit is the easiest way to get started with Svelte. It's the official web application framework for Svelteand is being developed closely in tandem with Svelte. Apart from that, it also has many best practices and optimizations built-in whilst providing a great developer experience with Vite's speedy Hot Module Replacement.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#links-1","content":" Official Svelte WebsiteOfficial SvelteKit WebsiteOfficial Vite Website    ","version":"Next","tagName":"h3"},{"title":"Tailwind CSS​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#tailwind-css","content":" Tailwind is a utility CSS Framework with flexibility, optimization and heavy DX focus. It is a highly customizable, low-level CSS framework that gives you all the building blocks you need to build bespoke designs without opinionated styles you have to fight to override.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Tailwind CSS​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#why-weve-chosen-tailwind-css","content":" When thinking about utility-class CSS Frameworks, two frameworks come to mind: Tailwind CSS and Bootstrap.  While Bootstrap comes with a set of pre-styled responsive, components that make up a UI kit, Tailwind CSS offers a responsive design system that is more flexible and customizable. It also adds its powerfulstate variants system.  Using Tailwind CSS​  Simply add the Tailwind CSS classes to your HTML elements to get going!  &lt;h1 class=&quot;text-3xl font-bold underline&quot;&gt;Hello world!&lt;/h1&gt;   You can also use arbitrary values, which basically works just like inline-styles. It is not recommended to break out of the constraints, but you may do so if it is really necessary (you should consider if you really have to whenever you decide to do this).  &lt;div class=&quot;top-[117px]&quot;&gt;Hello world!&lt;/div&gt;   Another powerful Tailwind feature to take note of is the ability toconditionally apply classes based on states.  In this example, hovering over the &lt;div&gt; will apply the bg-gray-300 class.  &lt;div class=&quot;bg-gray-200 hover:bg-gray-300&quot;&gt;Hello world!&lt;/div&gt;   This feature even works with stylingbased on other related elements' state.  Tailwind is highly extensible and has many more useful features. Read their official documentation for more info.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#links-2","content":" Official Tailwind CSS WebsiteInstall Tailwind CSS with SvelteKitTailwind Core Concepts    ","version":"Next","tagName":"h3"},{"title":"Shadcn-Svelte​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#shadcn-svelte","content":" Shadcn-Svelte works as a 'base' design system, i.e. a design system that has been designed to be extended upon. Critically, it uses with the other technologies and shares many of its philosophies (Svelte and Tailwind CSS).  Note that Shadcn-Svelte is based on the Shadcn design system. It is the Svelte implementation of the project.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Shadcn-Svelte​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#why-weve-chosen-shadcn-svelte","content":" Apart from very high-quality components that address many issues such as themeing, accessibility, and validation,Shadcn-Svelte uses a distribution pattern that fits perfectly with how we want to release our modules. Since projects often need the option to extend or even completely change their web components, Shadcn-Svelte's method of distributing the source code directly allows users to change anything they want. It provides a CLI tool to help with quick installation of the components users need.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#links-3","content":" Official Shadcn-Svelte WebsiteOfficial Shadcn Design System Figma    ","version":"Next","tagName":"h3"},{"title":"Iconify​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#iconify","content":" Iconify is a massive library of icons - the aggregation of all the free and open-source icons available on the web. It also provides a strong pattern of accessing and rendering these icons.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Iconify​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#why-weve-chosen-iconify","content":" Apart from the large number of icons available, the three main reasons for using iconify are:  You can extend it by adding your own custom iconsYou can serve icons dynamically by loading icons from a server, which you can also self-hostIt's open-source, free, and easy to use  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#links-4","content":" Official Iconify Website    ","version":"Next","tagName":"h3"},{"title":"Docusaurus​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#docusaurus","content":" Docusaurus is an open-source static site generator that creates documentation-focused websites based on Markdown files. It is based on React.  ","version":"Next","tagName":"h2"},{"title":"Why we've chosen Docusaurus​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#why-weve-chosen-docusaurus","content":" It's easy to use and is very widely adopted in the industry as a documentation framework with a highly reputable company (Meta) backing its development and maintenance.  ","version":"Next","tagName":"h3"},{"title":"Links​","type":1,"pageTitle":"Web Technologies","url":"/docs/docs/overview/technologies/web#links-5","content":" Official Docusaurus Website ","version":"Next","tagName":"h3"},{"title":"Go","type":0,"sectionRef":"#","url":"/docs/docs/contributing/language-specific/golang","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#introduction","content":" The goal of this guide is to help us ensure the code we write is clear, readable and idiomatic Go code.  We follow the official Go Coding guidelines by Google  https://go.dev/doc/effective_gohttps://google.github.io/styleguide/go/  In addition to official guidelines, we created the list of conventions that are specific to AOH.  This coding guide is inspired from the Uber Go style guide at https://github.com/uber-go/guide/blob/master/style.md, with modifications specific to AOH.  ","version":"Next","tagName":"h2"},{"title":"Guidelines​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#guidelines","content":" ","version":"Next","tagName":"h2"},{"title":"Pointers to Interfaces​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#pointers-to-interfaces","content":" You almost never need a pointer to an interface. You should be passing interfaces as values—the underlying data can still be a pointer.  An interface is two fields:  A pointer to some type-specific information. You can think of this as &quot;type.&quot;Data pointer. If the data stored is a pointer, it’s stored directly. If the data stored is a value, then a pointer to the value is stored.  If you want interface methods to modify the underlying data, you must use a pointer.  ","version":"Next","tagName":"h3"},{"title":"Verify Interface Compliance​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#verify-interface-compliance","content":" Verify interface compliance at compile time where appropriate. This includes:  Exported types that are required to implement specific interfaces as part of their API contractExported or unexported types that are part of a collection of types implementing the same interfaceOther cases where violating an interface would break users  Bad\tGood type Handler struct { // ... } func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request, ) { ... } type Handler struct { // ... } var _ http.Handler = (*Handler)(nil) func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request, ) { // ... }   The statement var _ http.Handler = (*Handler)(nil) will fail to compile if *Handler ever stops matching thehttp.Handler interface.  The right hand side of the assignment should be the zero value of the asserted type. This is nil for pointer types (like *Handler), slices, and maps, and an empty struct for struct types.  type LogHandler struct { h http.Handler log *zap.Logger } var _ http.Handler = LogHandler{} func (h LogHandler) ServeHTTP( w http.ResponseWriter, r *http.Request, ) { // ... }   ","version":"Next","tagName":"h3"},{"title":"Receivers and Interfaces​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#receivers-and-interfaces","content":" Methods with value receivers can be called on pointers as well as values. Methods with pointer receivers can only be called on pointers or addressable values.  For example,  type S struct { data string } func (s S) Read() string { return s.data } func (s *S) Write(str string) { s.data = str } // We cannot get pointers to values stored in maps, because they are not // addressable values. sVals := map[int]S{1: {&quot;A&quot;}} // We can call Read on values stored in the map because Read // has a value receiver, which does not require the value to // be addressable. sVals[1].Read() // We cannot call Write on values stored in the map because Write // has a pointer receiver, and it's not possible to get a pointer // to a value stored in a map. // // sVals[1].Write(&quot;test&quot;) sPtrs := map[int]*S{1: {&quot;A&quot;}} // You can call both Read and Write if the map stores pointers, // because pointers are intrinsically addressable. sPtrs[1].Read() sPtrs[1].Write(&quot;test&quot;)   Similarly, an interface can be satisfied by a pointer, even if the method has a value receiver.  type F interface { f() } type S1 struct{} func (s S1) f() {} type S2 struct{} func (s *S2) f() {} s1Val := S1{} s1Ptr := &amp;S1{} s2Val := S2{} s2Ptr := &amp;S2{} var i F i = s1Val i = s1Ptr i = s2Ptr // The following doesn't compile, since s2Val is a value, and there is no value receiver for f. // i = s2Val   Effective Go has a good write up on Pointers vs. Values.  ","version":"Next","tagName":"h3"},{"title":"Zero-value Mutexes are Valid​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#zero-value-mutexes-are-valid","content":" The zero-value of sync.Mutex and sync.RWMutex is valid, so you almost never need a pointer to a mutex.  Bad\tGood mu := new(sync.Mutex) mu.Lock() var mu sync.Mutex mu.Lock()   If you use a struct by pointer, then the mutex should be a non-pointer field on it. Do not embed the mutex on the struct, even if the struct is not exported.  Bad\tGood type SMap struct { sync.Mutex data map[string]string } func NewSMap() *SMap { return &amp;SMap{ data: make(map[string]string), } } func (m *SMap) Get(k string) string { m.Lock() defer m.Unlock() return m.data[k] } type SMap struct { mu sync.Mutex data map[string]string } func NewSMap() *SMap { return &amp;SMap{ data: make(map[string]string), } } func (m *SMap) Get(k string) string { m.mu.Lock() defer m.mu.Unlock() return m.data[k] } The Mutex field, and the Lock and Unlock methods are unintentionally part of the exported API of SMap. The mutex and its methods are implementation details of SMap hidden from its callers.  ","version":"Next","tagName":"h3"},{"title":"Copy Slices and Maps at Boundaries​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#copy-slices-and-maps-at-boundaries","content":" Slices and maps contain pointers to the underlying data so be wary of scenarios when they need to be copied.  Receiving Slices and Maps​  Keep in mind that users can modify a map or slice you received as an argument if you store a reference to it.  Bad Good func (d *Driver) SetTrips(trips []Trip) { d.trips = trips } trips := ... d1.SetTrips(trips) // Did you mean to modify d1.trips? trips[0] = ... func (d *Driver) SetTrips(trips []Trip) { d.trips = make([]Trip, len(trips)) copy(d.trips, trips) } trips := ... d1.SetTrips(trips) // We can now modify trips[0] without affecting d1.trips. trips[0] = ...   Returning Slices and Maps​  Similarly, be wary of user modifications to maps or slices exposing internal state.  Bad\tGood type Stats struct { mu sync.Mutex counters map[string]int } // Snapshot returns the current stats. func (s *Stats) Snapshot() map[string]int { s.mu.Lock() defer s.mu.Unlock() return s.counters } // snapshot is no longer protected by the mutex, so any // access to the snapshot is subject to data races. snapshot := stats.Snapshot() type Stats struct { mu sync.Mutex counters map[string]int } func (s *Stats) Snapshot() map[string]int { s.mu.Lock() defer s.mu.Unlock() result := make(map[string]int, len(s.counters)) for k, v := range s.counters { result[k] = v } return result } // Snapshot is now a copy. snapshot := stats.Snapshot()   ","version":"Next","tagName":"h3"},{"title":"Defer to Clean Up​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#defer-to-clean-up","content":" Use defer to clean up resources such as files and locks.  Bad\tGood p.Lock() if p.count &lt; 10 { p.Unlock() return p.count } p.count++ newCount := p.count p.Unlock() return newCount // easy to miss unlocks due to multiple returns p.Lock() defer p.Unlock() if p.count &lt; 10 { return p.count } p.count++ return p.count // more readable   Defer has an extremely small overhead and should be avoided only if you can prove that your function execution time is in the order of nanoseconds. The readability win of using defers is worth the miniscule cost of using them. This is especially true for larger methods that have more than simple memory accesses, where the other computations are more significant than the defer.  ","version":"Next","tagName":"h3"},{"title":"Channel Size is One or None​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#channel-size-is-one-or-none","content":" Channels should usually have a size of one or be unbuffered. By default, channels are unbuffered and have a size of zero. Any other size must be subject to a high level of scrutiny. Consider how the size is determined, what prevents the channel from filling up under load and blocking writers, and what happens when this occurs.  Bad\tGood // Ought to be enough for anybody! c := make(chan int, 64) // Size of one c := make(chan int, 1) // or // Unbuffered channel, size of zero c := make(chan int)   ","version":"Next","tagName":"h3"},{"title":"Start Enums at One​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#start-enums-at-one","content":" The standard way of introducing enumerations in Go is to declare a custom type and a const group with iota. Since variables have a 0 default value, you should usually start your enums on a non-zero value.  Bad\tGood type Operation int const ( Add Operation = iota Subtract Multiply ) // Add=0, Subtract=1, Multiply=2 type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) // Add=1, Subtract=2, Multiply=3   There are cases where using the zero value makes sense, for example when the zero value case is the desirable default behavior.  type LogOutput int const ( LogToStdout LogOutput = iota LogToFile LogToRemote ) // LogToStdout=0, LogToFile=1, LogToRemote=2   ","version":"Next","tagName":"h3"},{"title":"Use \"time\" to handle time​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#use-time-to-handle-time","content":" Time is complicated. Incorrect assumptions often made about time include the following.  A day has 24 hoursAn hour has 60 minutesA week has 7 daysA year has 365 daysAnd a lot more  For example, 1 means that adding 24 hours to a time instant will not always yield a new calendar day.  Therefore, always use the &quot;time&quot; package when dealing with time because it helps deal with these incorrect assumptions in a safer, more accurate manner.  Use time.Time for instants of time​  Use time.Time when dealing with instants of time, and the methods on time.Time when comparing, adding, or subtracting time.  Bad\tGood func isActive(now, start, stop int) bool { return start &lt;= now &amp;&amp; now &lt; stop } func isActive(now, start, stop time.Time) bool { return (start.Before(now) || start.Equal(now)) &amp;&amp; now.Before(stop) }   Use time.Duration for periods of time​  Use time.Duration when dealing with periods of time.  Bad\tGood func poll(delay int) { for { // ... time.Sleep(time.Duration(delay) * time.Millisecond) } } poll(10) // was it seconds or milliseconds? func poll(delay time.Duration) { for { // ... time.Sleep(delay) } } poll(10*time.Second)   Going back to the example of adding 24 hours to a time instant, the method we use to add time depends on intent. If we want the same time of the day, but on the next calendar day, we should use Time.AddDate. However, if we want an instant of time guaranteed to be 24 hours after the previous time, we should use Time.Add.  newDay := t.AddDate(0 /* years */, 0 /* months */, 1 /* days */) maybeNewDay := t.Add(24 * time.Hour)   Use time.Time and time.Duration with external systems​  Use time.Duration and time.Time in interactions with external systems when possible. For example:  Command-line flags: flag supports time.Durationvia time.ParseDurationJSON: encoding/json supports encoding time.Time as an RFC 3339 string via its UnmarshalJSON methodSQL: database/sql supports converting DATETIME or TIMESTAMP columns into time.Time and back if the underlying driver supports itYAML: gopkg.in/yaml.v2 supports time.Time as an RFC 3339 string, and time.Durationvia time.ParseDuration.  When it is not possible to use time.Duration in these interactions, use int or float64 and include the unit in the name of the field.  For example, since encoding/json does not support time.Duration, the unit is included in the name of the field.  Bad\tGood // {&quot;interval&quot;: 2} type Config struct { Interval int `json:&quot;interval&quot;` } // {&quot;intervalMillis&quot;: 2000} type Config struct { IntervalMillis int `json:&quot;intervalMillis&quot;` }   When it is not possible to use time.Time in these interactions, unless an alternative is agreed upon, use string and format timestamps as defined in RFC 3339. This format is used by default by Time.UnmarshalText and is available for use in Time.Formatand time.Parse via time.RFC3339.  Although this tends to not be a problem in practice, keep in mind that the &quot;time&quot; package does not support parsing timestamps with leap seconds (8728), nor does it account for leap seconds in calculations (15190). If you compare two instants of time, the difference will not include the leap seconds that may have occurred between those two instants.  ","version":"Next","tagName":"h3"},{"title":"Errors​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#errors","content":" Error Types​  There are few options for declaring errors. Consider the following before picking the option best suited for your use case.  Does the caller need to match the error so that they can handle it? If yes, we must support the errors.Is or errors.As functions by declaring a top-level error variable or a custom type.Is the error message a static string, or is it a dynamic string that requires contextual information? For the former, we can use errors.New, but for the latter we must use fmt.Errorf or a custom error type.Are we propagating a new error returned by a downstream function? If so, see the section on error wrapping.  Error matching?\tError Message\tGuidanceNo\tstatic\terrors.New No\tdynamic\tfmt.Errorf Yes\tstatic\ttop-level var with errors.New Yes\tdynamic\tcustom error type  For example, use errors.New for an error with a static string. Export this error as a variable to support matching it with errors.Isif the caller needs to match and handle this error.  No error matching\tError matching // package foo func Open() error { return errors.New(&quot;could not open&quot;) } // package bar if err := foo.Open(); err != nil { // Can't handle the error. panic(&quot;unknown error&quot;) } // package foo var ErrCouldNotOpen = errors.New(&quot;could not open&quot;) func Open() error { return ErrCouldNotOpen } // package bar if err := foo.Open(); err != nil { if errors.Is(err, foo.ErrCouldNotOpen) { // handle the error } else { panic(&quot;unknown error&quot;) } }   For an error with a dynamic string, use fmt.Errorf if the caller does not need to match it, and a custom error if the caller does need to match it.  No error matching\tError matching // package foo func Open(file string) error { return fmt.Errorf(&quot;file %q not found&quot;, file) } // package bar if err := foo.Open(&quot;testfile.txt&quot;); err != nil { // Can't handle the error. panic(&quot;unknown error&quot;) } // package foo type NotFoundError struct { File string } func (e *NotFoundError) Error() string { return fmt.Sprintf(&quot;file %q not found&quot;, e.File) } func Open(file string) error { return &amp;NotFoundError{File: file} } // package bar if err := foo.Open(&quot;testfile.txt&quot;); err != nil { var notFound *NotFoundError if errors.As(err, &amp;notFound) { // handle the error } else { panic(&quot;unknown error&quot;) } }   Note that if you export error variables or types from a package, they will become part of the public API of the package.  Error Wrapping​  There are three main options for propagating errors if a call fails:  return the original error as-isadd context with fmt.Errorf and the %w verbadd context with fmt.Errorf and the %v verb  Return the original error as-is if there is no additional context to add. This maintains the original error type and message. This is well suited for cases when the underlying error message has sufficient information to track down where it came from.  Otherwise, add context to the error message where possible so that instead of a vague error such as &quot;connection refused&quot;, you get more useful errors such as &quot;call service foo: connection refused&quot;.  Use fmt.Errorf to add context to your errors, picking between the %w or %v verbs based on whether the caller should be able to match and extract the underlying cause.  Use %w if the caller should have access to the underlying error. This is a good default for most wrapped errors, but be aware that callers may begin to rely on this behavior. So for cases where the wrapped error is a known var or type, document and test it as part of your function's contract.Use %v to obfuscate the underlying error. Callers will be unable to match it, but you can switch to %w in the future if needed.  When adding context to returned errors, keep the context succinct by avoiding phrases like &quot;failed to&quot;, which state the obvious and pile up as the error percolates up through the stack:  Bad\tGood s, err := store.New() if err != nil { return fmt.Errorf( &quot;failed to create new store: %w&quot;, err) } s, err := store.New() if err != nil { return fmt.Errorf( &quot;new store: %w&quot;, err) } failed to x: failed to y: failed to create new store: the error x: y: new store: the error   However once the error is sent to another system, it should be clear the message is an error (e.g. an err tag or &quot;Failed&quot; prefix in logs).  See alsoDon't just check errors, handle them gracefully.  Error Naming​  For error values stored as global variables, use the prefix Err or err depending on whether they're exported. This guidance supersedes the Prefix Unexported Globals with _.  var ( // The following two errors are exported // so that users of this package can match them // with errors.Is. ErrBrokenLink = errors.New(&quot;link is broken&quot;) ErrCouldNotOpen = errors.New(&quot;could not open&quot;) // This error is not exported because // we don't want to make it part of our public API. // We may still use it inside the package // with errors.Is. errNotFound = errors.New(&quot;not found&quot;) )   For custom error types, use the suffix Error instead.  // Similarly, this error is exported // so that users of this package can match it // with errors.As. type NotFoundError struct { File string } func (e *NotFoundError) Error() string { return fmt.Sprintf(&quot;file %q not found&quot;, e.File) } // And this error is not exported because // we don't want to make it part of the public API. // We can still use it inside the package // with errors.As. type resolveError struct { Path string } func (e *resolveError) Error() string { return fmt.Sprintf(&quot;resolve %q&quot;, e.Path) }   Handle Errors Once​  When a caller receives an error from a callee, it can handle it in a variety of different ways depending on what it knows about the error.  These include, but not are limited to:  if the callee contract defines specific errors, matching the error with errors.Is or errors.As and handling the branches differentlyif the error is recoverable, logging the error and degrading gracefullyif the error represents a domain-specific failure condition, returning a well-defined errorreturning the error, either wrapped or verbatim  Regardless of how the caller handles the error, it should typically handle each error only once. The caller should not, for example, log the error and then return it, because its callers may handle the error as well.  For example, consider the following cases:  Description\tCode Bad: Log the error and return it Callers further up the stack will likely take a similar action with the error. Doing so causing a lot of noise in the application logs for little value. u, err := getUser(id) if err != nil { // BAD: See description log.Printf(&quot;Could not get user %q: %v&quot;, id, err) return err } Good: Wrap the error and return it Callers further up the stack will handle the error. Use of %w ensures they can match the error with errors.Isor errors.As if relevant. u, err := getUser(id) if err != nil { return fmt.Errorf(&quot;get user %q: %w&quot;, id, err) } Good: Log the error and degrade gracefully If the operation isn't strictly necessary, we can provide a degraded but unbroken experience by recovering from it. if err := emitMetrics(); err != nil { // Failure to write metrics should not // break the application. log.Printf(&quot;Could not emit metrics: %v&quot;, err) } Good: Match the error and degrade gracefully If the callee defines a specific error in its contract, and the failure is recoverable, match on that error case and degrade gracefully. For all other cases, wrap the error and return it. Callers further up the stack will handle other errors. tz, err := getUserTimeZone(id) if err != nil { if errors.Is(err, ErrUserNotFound) { // User doesn't exist. Use UTC. tz = time.UTC } else { return fmt.Errorf(&quot;get user %q: %w&quot;, id, err) } }   ","version":"Next","tagName":"h3"},{"title":"Handle Type Assertion Failures​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#handle-type-assertion-failures","content":" The single return value form of a type assertion will panic on an incorrect type. Therefore, always use the &quot;comma ok&quot; idiom.  Bad\tGood t := i.(string) t, ok := i.(string) if !ok { // handle the error gracefully }   ","version":"Next","tagName":"h3"},{"title":"Don't Panic​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#dont-panic","content":" Code running in production must avoid panics. Panics are a major source of cascading failures. If an error occurs, the function must return an error and allow the caller to decide how to handle it.  Bad\tGood func run(args []string) { if len(args) == 0 { panic(&quot;an argument is required&quot;) } // ... } func main() { run(os.Args[1:]) } func run(args []string) error { if len(args) == 0 { return errors.New(&quot;an argument is required&quot;) } // ... return nil } func main() { if err := run(os.Args[1:]); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } }   Panic/recover is not an error handling strategy. A program must panic only when something irrecoverable happens such as a nil dereference. An exception to this is program initialization: bad things at program startup that should abort the program may cause panic.  var _statusTemplate = template.Must(template.New(&quot;name&quot;).Parse(&quot;_statusHTML&quot;))   Even in tests, prefer t.Fatal or t.FailNow over panics to ensure that the test is marked as failed.  Bad\tGood // func TestFoo(t *testing.T) f, err := os.CreateTemp(&quot;&quot;, &quot;test&quot;) if err != nil { panic(&quot;failed to set up test&quot;) } // func TestFoo(t *testing.T) f, err := os.CreateTemp(&quot;&quot;, &quot;test&quot;) if err != nil { t.Fatal(&quot;failed to set up test&quot;) }   ","version":"Next","tagName":"h3"},{"title":"Avoid Mutable Globals​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#avoid-mutable-globals","content":" Avoid mutating global variables, instead opting for dependency injection. This applies to function pointers as well as other kinds of values.  Bad\tGood // sign.go var _timeNow = time.Now func sign(msg string) string { now := _timeNow() return signWithTime(msg, now) } // sign.go type signer struct { now func() time.Time } func newSigner() *signer { return &amp;signer{ now: time.Now, } } func (s *signer) Sign(msg string) string { now := s.now() return signWithTime(msg, now) } // sign_test.go func TestSign(t *testing.T) { oldTimeNow := _timeNow _timeNow = func() time.Time { return someFixedTime } defer func() { _timeNow = oldTimeNow }() assert.Equal(t, want, sign(give)) } // sign_test.go func TestSigner(t *testing.T) { s := newSigner() s.now = func() time.Time { return someFixedTime } assert.Equal(t, want, s.Sign(give)) }   ","version":"Next","tagName":"h3"},{"title":"Avoid Embedding Types in Public Structs​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#avoid-embedding-types-in-public-structs","content":" These embedded types leak implementation details, inhibit type evolution, and obscure documentation.  Assuming you have implemented a variety of list types using a shared AbstractList, avoid embedding the AbstractListin your concrete list implementations. Instead, hand-write only the methods to your concrete list that will delegate to the abstract list.  type AbstractList struct {} // Add adds an entity to the list. func (l *AbstractList) Add(e Entity) { // ... } // Remove removes an entity from the list. func (l *AbstractList) Remove(e Entity) { // ... }   Bad\tGood // ConcreteList is a list of entities. type ConcreteList struct { *AbstractList } // ConcreteList is a list of entities. type ConcreteList struct { list *AbstractList } // Add adds an entity to the list. func (l *ConcreteList) Add(e Entity) { l.list.Add(e) } // Remove removes an entity from the list. func (l *ConcreteList) Remove(e Entity) { l.list.Remove(e) }   Go allows type embedding as a compromise between inheritance and composition. The outer type gets implicit copies of the embedded type's methods. These methods, by default, delegate to the same method of the embedded instance.  The struct also gains a field by the same name as the type. So, if the embedded type is public, the field is public. To maintain backward compatibility, every future version of the outer type must keep the embedded type.  An embedded type is rarely necessary. It is a convenience that helps you avoid writing tedious delegate methods.  Even embedding a compatible AbstractList interface, instead of the struct, would offer the developer more flexibility to change in the future, but still leak the detail that the concrete lists use an abstract implementation.  Bad\tGood // AbstractList is a generalized implementation // for various kinds of lists of entities. type AbstractList interface { Add(Entity) Remove(Entity) } // ConcreteList is a list of entities. type ConcreteList struct { AbstractList } // ConcreteList is a list of entities. type ConcreteList struct { list AbstractList } // Add adds an entity to the list. func (l *ConcreteList) Add(e Entity) { l.list.Add(e) } // Remove removes an entity from the list. func (l *ConcreteList) Remove(e Entity) { l.list.Remove(e) }   Either with an embedded struct or an embedded interface, the embedded type places limits on the evolution of the type.  Adding methods to an embedded interface is a breaking change.Removing methods from an embedded struct is a breaking change.Removing the embedded type is a breaking change.Replacing the embedded type, even with an alternative that satisfies the same interface, is a breaking change.  Although writing these delegate methods is tedious, the additional effort hides an implementation detail, leaves more opportunities for change, and also eliminates indirection for discovering the full List interface in documentation.  ","version":"Next","tagName":"h3"},{"title":"Avoid Using Built-In Names​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#avoid-using-built-in-names","content":" The Go language specification outlines several built-in, predeclared identifiers that should not be used as names within Go programs.  Depending on context, reusing these identifiers as names will either shadow the original within the current lexical scope (and any nested scopes) or make affected code confusing. In the best case, the compiler will complain; in the worst case, such code may introduce latent, hard-to-grep bugs.  Bad\tGood var error string // `error` shadows the builtin // or func handleErrorMessage(error string) { // `error` shadows the builtin } var errorMessage string // `error` refers to the builtin // or func handleErrorMessage(msg string) { // `error` refers to the builtin } type Foo struct { // While these fields technically don't // constitute shadowing, grepping for // `error` or `string` strings is now // ambiguous. error error string string } func (f Foo) Error() error { // `error` and `f.error` are // visually similar return f.error } func (f Foo) String() string { // `string` and `f.string` are // visually similar return f.string } type Foo struct { // `error` and `string` strings are // now unambiguous. err error str string } func (f Foo) Error() error { return f.err } func (f Foo) String() string { return f.str }   Note that the compiler will not generate errors when using predeclared identifiers, but tools such as go vet should correctly point out these and other cases of shadowing.  ","version":"Next","tagName":"h3"},{"title":"Avoid init()​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#avoid-init","content":" Avoid init() where possible. When init() is unavoidable or desirable, code should attempt to:  Be completely deterministic, regardless of program environment or invocation.Avoid depending on the ordering or side-effects of other init() functions. While init() ordering is well-known, code can change, and thus relationships between init() functions can make code brittle and error-prone.Avoid accessing or manipulating global or environment state, such as machine information, environment variables, working directory, program arguments/inputs, etc.Avoid I/O, including both filesystem, network, and system calls.  Code that cannot satisfy these requirements likely belongs as a helper to be called as part of main() (or elsewhere in a program's lifecycle), or be written as part of main() itself. In particular, libraries that are intended to be used by other programs should take special care to be completely deterministic and not perform &quot;init magic&quot;.  Bad\tGood type Foo struct { // ... } var _defaultFoo Foo func init() { _defaultFoo = Foo{ // ... } } var _defaultFoo = Foo{ // ... } // or, better, for testability: var _defaultFoo = defaultFoo() func defaultFoo() Foo { return Foo{ // ... } } type Config struct { // ... } var _config Config func init() { // Bad: based on current directory cwd, _ := os.Getwd() // Bad: I/O raw, _ := os.ReadFile( path.Join(cwd, &quot;config&quot;, &quot;config.yaml&quot;), ) yaml.Unmarshal(raw, &amp;_config) } type Config struct { // ... } func loadConfig() Config { cwd, err := os.Getwd() // handle err raw, err := os.ReadFile( path.Join(cwd, &quot;config&quot;, &quot;config.yaml&quot;), ) // handle err var config Config yaml.Unmarshal(raw, &amp;config) return config }   Considering the above, some situations in which init() may be preferable or necessary might include:  Complex expressions that cannot be represented as single assignments.Pluggable hooks, such as database/sql dialects, encoding type registries, etc.Optimizations toGoogle Cloud Functionsand other forms of deterministic precomputation.  ","version":"Next","tagName":"h3"},{"title":"Exit in Main​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#exit-in-main","content":" Go programs use os.Exit or log.Fatal* to exit immediately. (Panicking is not a good way to exit programs, please don't panic.)  Call one of os.Exit or log.Fatal* only in main(). All other functions should return errors to signal failure.  Bad\tGood func main() { body := readFile(path) fmt.Println(body) } func readFile(path string) string { f, err := os.Open(path) if err != nil { log.Fatal(err) } b, err := io.ReadAll(f) if err != nil { log.Fatal(err) } return string(b) } func main() { body, err := readFile(path) if err != nil { log.Fatal(err) } fmt.Println(body) } func readFile(path string) (string, error) { f, err := os.Open(path) if err != nil { return &quot;&quot;, err } b, err := io.ReadAll(f) if err != nil { return &quot;&quot;, err } return string(b), nil }   Rationale: Programs with multiple functions that exit present a few issues:  Non-obvious control flow: Any function can exit the program so it becomes difficult to reason about the control flow.Difficult to test: A function that exits the program will also exit the test calling it. This makes the function difficult to test and introduces risk of skipping other tests that have not yet been run by go test.Skipped cleanup: When a function exits the program, it skips function calls enqueued with defer statements. This adds risk of skipping important cleanup tasks.  Exit Once​  If possible, prefer to call os.Exit or log.Fatal at most once in your main(). If there are multiple error scenarios that halt program execution, put that logic under a separate function and return errors from it.  This has the effect of shortening your main() function and putting all key business logic into a separate, testable function.  Bad\tGood package main func main() { args := os.Args[1:] if len(args) != 1 { log.Fatal(&quot;missing file&quot;) } name := args[0] f, err := os.Open(name) if err != nil { log.Fatal(err) } defer f.Close() // If we call log.Fatal after this line, // f.Close will not be called. b, err := io.ReadAll(f) if err != nil { log.Fatal(err) } // ... } package main func main() { if err := run(); err != nil { log.Fatal(err) } } func run() error { args := os.Args[1:] if len(args) != 1 { return errors.New(&quot;missing file&quot;) } name := args[0] f, err := os.Open(name) if err != nil { return err } defer f.Close() b, err := io.ReadAll(f) if err != nil { return err } // ... }   The example above uses log.Fatal, but the guidance also applies to os.Exit or any library code that calls os.Exit.  func main() { if err := run(); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } }   You may alter the signature of run() to fit your needs. For example, if your program must exit with specific exit codes for failures, run() may return the exit code instead of an error. This allows unit tests to verify this behavior directly as well.  func main() { os.Exit(run(args)) } func run() (exitCode int) { // ... }   More generally, note that the run() function used in these examples is not intended to be prescriptive. There's flexibility in the name, signature, and setup of the run() function. Among other things, you may:  accept unparsed command line arguments (e.g., run(os.Args[1:]))parse command line arguments in main() and pass them onto runuse a custom error type to carry the exit code back to main()put business logic in a different layer of abstraction from package main  This guidance only requires that there's a single place in your main() responsible for actually exiting the process.  ","version":"Next","tagName":"h3"},{"title":"Use field tags in marshaled structs​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#use-field-tags-in-marshaled-structs","content":" Any struct field that is marshaled into JSON, YAML, or other formats that support tag-based field naming should be annotated with the relevant tag.  Bad\tGood type Stock struct { Price int Name string } bytes, err := json.Marshal(Stock{ Price: 137, Name: &quot;UBER&quot;, }) type Stock struct { Price int `json:&quot;price&quot;` Name string `json:&quot;name&quot;` // Safe to rename Name to Symbol. } bytes, err := json.Marshal(Stock{ Price: 137, Name: &quot;UBER&quot;, })   Rationale: The serialized form of the structure is a contract between different systems. Changes to the structure of the serialized form--including field names--break this contract. Specifying field names inside tags makes the contract explicit, and it guards against accidentally breaking the contract by refactoring or renaming fields.  ","version":"Next","tagName":"h3"},{"title":"Don't fire-and-forget goroutines​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#dont-fire-and-forget-goroutines","content":" Goroutines are lightweight, but they're not free: at minimum, they cost memory for their stack and CPU to be scheduled. While these costs are small for typical uses of goroutines, they can cause significant performance issues when spawned in large numbers without controlled lifetimes. Goroutines with unmanaged lifetimes can also cause other issues like preventing unused objects from being garbage collected and holding onto resources that are otherwise no longer used.  In general, every goroutine:  must have a predictable time at which it will stop running; orthere must be a way to signal to the goroutine that it should stop  In both cases, there must be a way code to block and wait for the goroutine to finish.  For example:  Bad\tGood go func() { for { flush() time.Sleep(delay) } }() var ( stop = make(chan struct{}) // tells the goroutine to stop done = make(chan struct{}) // tells us that the goroutine exited ) go func() { defer close(done) ticker := time.NewTicker(delay) defer ticker.Stop() for { select { case &lt;-ticker.C: flush() case &lt;-stop: return } } }() // Elsewhere... close(stop) // signal the goroutine to stop &lt;-done // and wait for it to exit There's no way to stop this goroutine. This will run until the application exits. This goroutine can be stopped with close(stop), and we can wait for it to exit with &lt;-done.  Wait for goroutines to exit​  Given a goroutine spawned by the system, there must be a way to wait for the goroutine to exit. There are two popular ways to do this:  Use a sync.WaitGroup. Do this if there are multiple goroutines that you want to wait for var wg sync.WaitGroup for i := 0; i &lt; N; i++ { wg.Add(1) go func() { defer wg.Done() // ... }() } // To wait for all to finish: wg.Wait() Add another chan struct{} that the goroutine closes when it's done. Do this if there's only one goroutine. done := make(chan struct{}) go func() { defer close(done) // ... }() // To wait for the goroutine to finish: &lt;-done   No goroutines in init()​  init() functions should not spawn goroutines. See also Avoid init().  If a package has need of a background goroutine, it must expose an object that is responsible for managing a goroutine's lifetime. The object must provide a method (Close, Stop, Shutdown, etc) that signals the background goroutine to stop, and waits for it to exit.  Bad\tGood func init() { go doWork() } func doWork() { for { // ... } } type Worker struct{ /* ... */ } func NewWorker(...) *Worker { w := &amp;Worker{ stop: make(chan struct{}), done: make(chan struct{}), // ... } go w.doWork() } func (w *Worker) doWork() { defer close(w.done) for { // ... case &lt;-w.stop: return } } // Shutdown tells the worker to stop // and waits until it has finished. func (w *Worker) Shutdown() { close(w.stop) &lt;-w.done } Spawns a background goroutine unconditionally when the user exports this package. The user has no control over the goroutine or a means of stopping it. Spawns the worker only if the user requests it. Provides a means of shutting down the worker so that the user can free up resources used by the worker. Note that you should use WaitGroups if the worker manages multiple goroutines. See Wait for goroutines to exit.  ","version":"Next","tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#performance","content":" Performance-specific guidelines apply only to the hot path.  ","version":"Next","tagName":"h2"},{"title":"Prefer strconv over fmt​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#prefer-strconv-over-fmt","content":" When converting primitives to/from strings, strconv is faster thanfmt.  Bad\tGood for i := 0; i &lt; b.N; i++ { s := fmt.Sprint(rand.Int()) } for i := 0; i &lt; b.N; i++ { s := strconv.Itoa(rand.Int()) } BenchmarkFmtSprint-4 143 ns/op 2 allocs/op BenchmarkStrconv-4 64.2 ns/op 1 allocs/op   ","version":"Next","tagName":"h3"},{"title":"Avoid repeated string-to-byte conversions​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#avoid-repeated-string-to-byte-conversions","content":" Do not create byte slices from a fixed string repeatedly. Instead, perform the conversion once and capture the result.  Bad\tGood for i := 0; i &lt; b.N; i++ { w.Write([]byte(&quot;Hello world&quot;)) } data := []byte(&quot;Hello world&quot;) for i := 0; i &lt; b.N; i++ { w.Write(data) } BenchmarkBad-4 50000000 22.2 ns/op BenchmarkGood-4 500000000 3.25 ns/op   ","version":"Next","tagName":"h3"},{"title":"Prefer Specifying Container Capacity​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#prefer-specifying-container-capacity","content":" Specify container capacity where possible in order to allocate memory for the container up front. This minimizes subsequent allocations (by copying and resizing of the container) as elements are added.  Specifying Map Capacity Hints​  Where possible, provide capacity hints when initializing maps with make().  make(map[T1]T2, hint)   Providing a capacity hint to make() tries to right-size the map at initialization time, which reduces the need for growing the map and allocations as elements are added to the map.  Note that, unlike slices, map capacity hints do not guarantee complete, preemptive allocation, but are used to approximate the number of hashmap buckets required. Consequently, allocations may still occur when adding elements to the map, even up to the specified capacity.  Bad\tGood m := make(map[string]os.FileInfo) files, _ := os.ReadDir(&quot;./files&quot;) for _, f := range files { m[f.Name()] = f } files, _ := os.ReadDir(&quot;./files&quot;) m := make(map[string]os.DirEntry, len(files)) for _, f := range files { m[f.Name()] = f } m is created without a size hint; there may be more allocations at assignment time. m is created with a size hint; there may be fewer allocations at assignment time.  Specifying Slice Capacity​  Where possible, provide capacity hints when initializing slices with make(), particularly when appending.  make([]T, length, capacity)   Unlike maps, slice capacity is not a hint: the compiler will allocate enough memory for the capacity of the slice as provided to make(), which means that subsequent append() operations will incur zero allocations (until the length of the slice matches the capacity, after which any appends will require a resize to hold additional elements).  Bad\tGood for n := 0; n &lt; b.N; n++ { data := make([]int, 0) for k := 0; k &lt; size; k++{ data = append(data, k) } } for n := 0; n &lt; b.N; n++ { data := make([]int, 0, size) for k := 0; k &lt; size; k++{ data = append(data, k) } } BenchmarkBad-4 100000000 2.48s BenchmarkGood-4 100000000 0.21s   ","version":"Next","tagName":"h3"},{"title":"Style​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#style","content":" ","version":"Next","tagName":"h2"},{"title":"Avoid overly long lines​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#avoid-overly-long-lines","content":" Avoid lines of code that require readers to scroll horizontally or turn their heads too much.  We recommend a line length limit of 120 characters. Authors should aim to wrap lines before hitting this limit.  ","version":"Next","tagName":"h3"},{"title":"Be Consistent​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#be-consistent","content":" Some of the guidelines outlined in this document can be evaluated objectively; others are situational, contextual, or subjective.  Above all else, be consistent.  Consistent code is easier to maintain, is easier to rationalize, requires less cognitive overhead, and is easier to migrate or update as new conventions emerge or classes of bugs are fixed.  Conversely, having multiple disparate or conflicting styles within a single codebase causes maintenance overhead, uncertainty, and cognitive dissonance, all of which can directly contribute to lower velocity, painful code reviews, and bugs.  When applying these guidelines to a codebase, it is recommended that changes are made at a package (or larger) level: application at a sub-package level violates the above concern by introducing multiple styles into the same code.  ","version":"Next","tagName":"h3"},{"title":"Group Similar Declarations​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#group-similar-declarations","content":" Go supports grouping similar declarations.  Bad\tGood import &quot;a&quot; import &quot;b&quot; import ( &quot;a&quot; &quot;b&quot; )   This also applies to constants, variables, and type declarations.  Bad\tGood const a = 1 const b = 2 var a = 1 var b = 2 type Area float64 type Volume float64 const ( a = 1 b = 2 ) var ( a = 1 b = 2 ) type ( Area float64 Volume float64 )   Only group related declarations. Do not group declarations that are unrelated.  Bad\tGood type Operation int const ( Add Operation = iota + 1 Subtract Multiply EnvVar = &quot;MY_ENV&quot; ) type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) const EnvVar = &quot;MY_ENV&quot;   Groups are not limited in where they can be used. For example, you can use them inside of functions.  Bad\tGood func f() string { red := color.New(0xff0000) green := color.New(0x00ff00) blue := color.New(0x0000ff) // ... } func f() string { var ( red = color.New(0xff0000) green = color.New(0x00ff00) blue = color.New(0x0000ff) ) // ... }   Exception: Variable declarations, particularly inside functions, should be grouped together if declared adjacent to other variables. Do this for variables declared together even if they are unrelated.  Bad\tGood func (c *client) request() { caller := c.name format := &quot;json&quot; timeout := 5*time.Second var err error // ... } func (c *client) request() { var ( caller = c.name format = &quot;json&quot; timeout = 5*time.Second err error ) // ... }   ","version":"Next","tagName":"h3"},{"title":"Import Group Ordering​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#import-group-ordering","content":" There should be two import groups:  Standard libraryEverything else  This is the grouping applied by goimports by default.  Bad\tGood import ( &quot;fmt&quot; &quot;os&quot; &quot;go.uber.org/atomic&quot; &quot;golang.org/x/sync/errgroup&quot; ) import ( &quot;fmt&quot; &quot;os&quot; &quot;go.uber.org/atomic&quot; &quot;golang.org/x/sync/errgroup&quot; )   ","version":"Next","tagName":"h3"},{"title":"Package Names​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#package-names","content":" When naming packages, choose a name that is:  All lower-case. No capitals or underscores.Does not need to be renamed using named imports at most call sites.Short and succinct. Remember that the name is identified in full at every call site.Not plural. For example, net/url, not net/urls.Not &quot;common&quot;, &quot;util&quot;, &quot;shared&quot;, or &quot;lib&quot;. These are bad, uninformative names.  See also Package Names and Style guideline for Go packages.  ","version":"Next","tagName":"h3"},{"title":"Function Names​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#function-names","content":" We follow the Go community's convention of using MixedCaps for function names. An exception is made for test functions, which may contain underscores for the purpose of grouping related test cases, e.g., TestMyFunction_WhatIsBeingTested.  ","version":"Next","tagName":"h3"},{"title":"Import Aliasing​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#import-aliasing","content":" Import aliasing must be used if the package name does not match the last element of the import path.  import ( &quot;net/http&quot; client &quot;example.com/client-go&quot; trace &quot;example.com/trace/v2&quot; )   In all other scenarios, import aliases should be avoided unless there is a direct conflict between imports.  Bad\tGood import ( &quot;fmt&quot; &quot;os&quot; nettrace &quot;golang.net/x/trace&quot; ) import ( &quot;fmt&quot; &quot;os&quot; &quot;runtime/trace&quot; nettrace &quot;golang.net/x/trace&quot; )   ","version":"Next","tagName":"h3"},{"title":"Function Grouping and Ordering​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#function-grouping-and-ordering","content":" Functions should be sorted in rough call order.Functions in a file should be grouped by receiver.  Therefore, exported functions should appear first in a file, after struct, const, var definitions.  A newXYZ()/NewXYZ() may appear after the type is defined, but before the rest of the methods on the receiver.  Since functions are grouped by receiver, plain utility functions should appear towards the end of the file.  Bad\tGood func (s *something) Cost() { return calcCost(s.weights) } type something struct{ ... } func calcCost(n []int) int {...} func (s *something) Stop() {...} func newSomething() *something { return &amp;something{} } type something struct{ ... } func newSomething() *something { return &amp;something{} } func (s *something) Cost() { return calcCost(s.weights) } func (s *something) Stop() {...} func calcCost(n []int) int {...}   ","version":"Next","tagName":"h3"},{"title":"Reduce Nesting​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#reduce-nesting","content":" Code should reduce nesting where possible by handling error cases/special conditions first and returning early or continuing the loop. Reduce the amount of code that is nested multiple levels.  Bad\tGood for _, v := range data { if v.F1 == 1 { v = process(v) if err := v.Call(); err == nil { v.Send() } else { return err } } else { log.Printf(&quot;Invalid v: %v&quot;, v) } } for _, v := range data { if v.F1 != 1 { log.Printf(&quot;Invalid v: %v&quot;, v) continue } v = process(v) if err := v.Call(); err != nil { return err } v.Send() }   ","version":"Next","tagName":"h3"},{"title":"Unnecessary Else​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#unnecessary-else","content":" If a variable is set in both branches of an if, it can be replaced with a single if.  Bad\tGood var a int if b { a = 100 } else { a = 10 } a := 10 if b { a = 100 }   ","version":"Next","tagName":"h3"},{"title":"Top-level Variable Declarations​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#top-level-variable-declarations","content":" At the top level, use the standard var keyword. Do not specify the type, unless it is not the same type as the expression.  Bad\tGood var _s string = F() func F() string { return &quot;A&quot; } var _s = F() // Since F already states that it returns a string, we don't need to specify // the type again. func F() string { return &quot;A&quot; }   Specify the type if the type of the expression does not match the desired type exactly.  type myError struct{} func (myError) Error() string { return &quot;error&quot; } func F() myError { return myError{} } var _e error = F() // F returns an object of type myError but we want error.   ","version":"Next","tagName":"h3"},{"title":"Prefix Unexported Globals with _​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#prefix-unexported-globals-with-_","content":" Prefix unexported top-level vars and consts with _ to make it clear when they are used that they are global symbols.  Rationale: Top-level variables and constants have a package scope. Using a generic name makes it easy to accidentally use the wrong value in a different file.  Bad\tGood // foo.go const ( defaultPort = 8080 defaultUser = &quot;user&quot; ) // bar.go func Bar() { defaultPort := 9090 ... fmt.Println(&quot;Default port&quot;, defaultPort) // We will not see a compile error if the first line of // Bar() is deleted. } // foo.go const ( _defaultPort = 8080 _defaultUser = &quot;user&quot; )   Exception: Unexported error values may use the prefix err without the underscore. See Error Naming.  ","version":"Next","tagName":"h3"},{"title":"Embedding in Structs​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#embedding-in-structs","content":" Embedded types should be at the top of the field list of a struct, and there must be an empty line separating embedded fields from regular fields.  Bad\tGood type Client struct { version int http.Client } type Client struct { http.Client version int }   Embedding should provide tangible benefit, like adding or augmenting functionality in a semantically-appropriate way. It should do this with zero adverse user-facing effects (see also: Avoid Embedding Types in Public Structs).  Exception: Mutexes should not be embedded, even on unexported types. See also: Zero-value Mutexes are Valid.  Embedding should not:  Be purely cosmetic or convenience-oriented.Make outer types more difficult to construct or use.Affect outer types' zero values. If the outer type has a useful zero value, it should still have a useful zero value after embedding the inner type.Expose unrelated functions or fields from the outer type as a side-effect of embedding the inner type.Expose unexported types.Affect outer types' copy semantics.Change the outer type's API or type semantics.Embed a non-canonical form of the inner type.Expose implementation details of the outer type.Allow users to observe or control type internals.Change the general behavior of inner functions through wrapping in a way that would reasonably surprise users.  Simply put, embed consciously and intentionally. A good litmus test is, &quot;would all of these exported inner methods/fields be added directly to the outer type&quot;; if the answer is &quot;some&quot; or &quot;no&quot;, don't embed the inner type - use a field instead.  Bad\tGood type A struct { // Bad: A.Lock() and A.Unlock() are // now available, provide no // functional benefit, and allow // users to control details about // the internals of A. sync.Mutex } type countingWriteCloser struct { // Good: Write() is provided at this // outer layer for a specific // purpose, and delegates work // to the inner type's Write(). io.WriteCloser count int } func (w *countingWriteCloser) Write(bs []byte) (int, error) { w.count += len(bs) return w.WriteCloser.Write(bs) } type Book struct { // Bad: pointer changes zero value usefulness io.ReadWriter // other fields } // later var b Book b.Read(...) // panic: nil pointer b.String() // panic: nil pointer b.Write(...) // panic: nil pointer type Book struct { // Good: has useful zero value bytes.Buffer // other fields } // later var b Book b.Read(...) // ok b.String() // ok b.Write(...) // ok type Client struct { sync.Mutex sync.WaitGroup bytes.Buffer url.URL } type Client struct { mtx sync.Mutex wg sync.WaitGroup buf bytes.Buffer url url.URL }   ","version":"Next","tagName":"h3"},{"title":"Local Variable Declarations​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#local-variable-declarations","content":" Short variable declarations (:=) should be used if a variable is being set to some value explicitly.  Bad\tGood var s = &quot;foo&quot; s := &quot;foo&quot;   However, there are cases where the default value is clearer when the var keyword is used. Declaring Empty Slices, for example.  Bad\tGood func f(list []int) { filtered := []int{} for _, v := range list { if v &gt; 10 { filtered = append(filtered, v) } } } func f(list []int) { var filtered []int for _, v := range list { if v &gt; 10 { filtered = append(filtered, v) } } }   ","version":"Next","tagName":"h3"},{"title":"nil is a valid slice​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#nil-is-a-valid-slice","content":" nil is a valid slice of length 0. This means that,  You should not return a slice of length zero explicitly. Return nil instead. Bad\tGood if x == &quot;&quot; { return []int{} } if x == &quot;&quot; { return nil } To check if a slice is empty, always use len(s) == 0. Do not check for nil. Bad\tGood func isEmpty(s []string) bool { return s == nil } func isEmpty(s []string) bool { return len(s) == 0 } The zero value (a slice declared with var) is usable immediately without make(). Bad\tGood nums := []int{} // or, nums := make([]int) if add1 { nums = append(nums, 1) } if add2 { nums = append(nums, 2) } var nums []int if add1 { nums = append(nums, 1) } if add2 { nums = append(nums, 2) }   Remember that, while it is a valid slice, a nil slice is not equivalent to an allocated slice of length 0 - one is nil and the other is not - and the two may be treated differently in different situations (such as serialization).  ","version":"Next","tagName":"h3"},{"title":"Reduce Scope of Variables​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#reduce-scope-of-variables","content":" Where possible, reduce scope of variables. Do not reduce the scope if it conflicts with Reduce Nesting.  Bad\tGood err := os.WriteFile(name, data, 0644) if err != nil { return err } if err := os.WriteFile(name, data, 0644); err != nil { return err }   If you need a result of a function call outside of the if, then you should not try to reduce the scope.  Bad\tGood if data, err := os.ReadFile(name); err == nil { err = cfg.Decode(data) if err != nil { return err } fmt.Println(cfg) return nil } else { return err } data, err := os.ReadFile(name) if err != nil { return err } if err := cfg.Decode(data); err != nil { return err } fmt.Println(cfg) return nil   ","version":"Next","tagName":"h3"},{"title":"Use Raw String Literals to Avoid Escaping​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#use-raw-string-literals-to-avoid-escaping","content":" Go supports raw string literals, which can span multiple lines and include quotes. Use these to avoid hand-escaped strings which are much harder to read.  Bad\tGood wantError := &quot;unknown name:\\&quot;test\\&quot;&quot; wantError := `unknown error:&quot;test&quot;`   ","version":"Next","tagName":"h3"},{"title":"Initializing Structs​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#initializing-structs","content":" Use Field Names to Initialize Structs​  You should almost always specify field names when initializing structs. This is now enforced by go vet.  Bad\tGood k := User{&quot;John&quot;, &quot;Doe&quot;, true} k := User{ FirstName: &quot;John&quot;, LastName: &quot;Doe&quot;, Admin: true, }   Exception: Field names may be omitted in test tables when there are 3 or fewer fields.  tests := []struct{ op Operation want string }{ {Add, &quot;add&quot;}, {Subtract, &quot;subtract&quot;}, }   Omit Zero Value Fields in Structs​  When initializing structs with field names, omit fields that have zero values unless they provide meaningful context. Otherwise, let Go set these to zero values automatically.  Bad\tGood user := User{ FirstName: &quot;John&quot;, LastName: &quot;Doe&quot;, MiddleName: &quot;&quot;, Admin: false, } user := User{ FirstName: &quot;John&quot;, LastName: &quot;Doe&quot;, }   This helps reduce noise for readers by omitting values that are default in that context. Only meaningful values are specified.  Include zero values where field names provide meaningful context. For example, test cases in Test Tablescan benefit from names of fields even when they are zero-valued.  tests := []struct{ give string want int }{ {give: &quot;0&quot;, want: 0}, // ... }   Use var for Zero Value Structs​  When all the fields of a struct are omitted in a declaration, use the var form to declare the struct.  Bad\tGood user := User{} var user User   This differentiates zero valued structs from those with non-zero fields similar to the distinction created for map initialization, and matches how we prefer to declare empty slices.  Initializing Struct References​  Use &amp;T{} instead of new(T) when initializing struct references so that it is consistent with the struct initialization.  Bad\tGood sval := T{Name: &quot;foo&quot;} // inconsistent sptr := new(T) sptr.Name = &quot;bar&quot; sval := T{Name: &quot;foo&quot;} sptr := &amp;T{Name: &quot;bar&quot;}   ","version":"Next","tagName":"h3"},{"title":"Initializing Maps​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#initializing-maps","content":" Prefer make(..) for empty maps, and maps populated programmatically. This makes map initialization visually distinct from declaration, and it makes it easy to add size hints later if available.  Bad\tGood var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = map[T1]T2{} m2 map[T1]T2 ) var ( // m1 is safe to read and write; // m2 will panic on writes. m1 = make(map[T1]T2) m2 map[T1]T2 ) Declaration and initialization are visually similar. Declaration and initialization are visually distinct.  Where possible, provide capacity hints when initializing maps with make(). See Specifying Map Capacity Hints for more information.  On the other hand, if the map holds a fixed list of elements, use map literals to initialize the map.  Bad\tGood m := make(map[T1]T2, 3) m[k1] = v1 m[k2] = v2 m[k3] = v3 m := map[T1]T2{ k1: v1, k2: v2, k3: v3, }   The basic rule of thumb is to use map literals when adding a fixed set of elements at initialization time, otherwise use make (and specify a size hint if available).  ","version":"Next","tagName":"h3"},{"title":"Format Strings outside Printf​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#format-strings-outside-printf","content":" If you declare format strings for Printf-style functions outside a string literal, make them const values.  This helps go vet perform static analysis of the format string.  Bad\tGood msg := &quot;unexpected values %v, %v\\n&quot; fmt.Printf(msg, 1, 2) const msg = &quot;unexpected values %v, %v\\n&quot; fmt.Printf(msg, 1, 2)   ","version":"Next","tagName":"h3"},{"title":"Naming Printf-style Functions​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#naming-printf-style-functions","content":" When you declare a Printf-style function, make sure that go vet can detect it and check the format string.  This means that you should use predefined Printf-style function names if possible. go vet will check these by default. See Printf family for more information.  If using the predefined names is not an option, end the name you choose with f: Wrapf, not Wrap. go vet can be asked to check specific Printf-style names but they must end with f.  go vet -printfuncs=wrapf,statusf   See also go vet: Printf family check.  ","version":"Next","tagName":"h3"},{"title":"Patterns​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#patterns","content":" ","version":"Next","tagName":"h2"},{"title":"Test Tables​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#test-tables","content":" Table-driven tests with subtests can be a helpful pattern for writing tests to avoid duplicating code when the core test logic is repetitive.  If a system under test needs to be tested against multiple conditions where certain parts of the the inputs and outputs change, a table-driven test should be used to reduce redundancy and improve readability.  Bad\tGood // func TestSplitHostPort(t *testing.T) host, port, err := net.SplitHostPort(&quot;192.0.2.0:8000&quot;) require.NoError(t, err) assert.Equal(t, &quot;192.0.2.0&quot;, host) assert.Equal(t, &quot;8000&quot;, port) host, port, err = net.SplitHostPort(&quot;192.0.2.0:http&quot;) require.NoError(t, err) assert.Equal(t, &quot;192.0.2.0&quot;, host) assert.Equal(t, &quot;http&quot;, port) host, port, err = net.SplitHostPort(&quot;:8000&quot;) require.NoError(t, err) assert.Equal(t, &quot;&quot;, host) assert.Equal(t, &quot;8000&quot;, port) host, port, err = net.SplitHostPort(&quot;1:8&quot;) require.NoError(t, err) assert.Equal(t, &quot;1&quot;, host) assert.Equal(t, &quot;8&quot;, port) // func TestSplitHostPort(t *testing.T) tests := []struct{ give string wantHost string wantPort string }{ { give: &quot;192.0.2.0:8000&quot;, wantHost: &quot;192.0.2.0&quot;, wantPort: &quot;8000&quot;, }, { give: &quot;192.0.2.0:http&quot;, wantHost: &quot;192.0.2.0&quot;, wantPort: &quot;http&quot;, }, { give: &quot;:8000&quot;, wantHost: &quot;&quot;, wantPort: &quot;8000&quot;, }, { give: &quot;1:8&quot;, wantHost: &quot;1&quot;, wantPort: &quot;8&quot;, }, } for _, tt := range tests { t.Run(tt.give, func(t *testing.T) { host, port, err := net.SplitHostPort(tt.give) require.NoError(t, err) assert.Equal(t, tt.wantHost, host) assert.Equal(t, tt.wantPort, port) }) }   Test tables make it easier to add context to error messages, reduce duplicate logic, and add new test cases.  We follow the convention that the slice of structs is referred to as tests and each test case tt. Further, we encourage explicating the input and output values for each test case with give and want prefixes.  tests := []struct{ give string wantHost string wantPort string }{ // ... } for _, tt := range tests { // ... }   Avoid Unnecessary Complexity in Table Tests​  Table tests can be difficult to read and maintain if the subtests contain conditional assertions or other branching logic. Table tests should NOT be used whenever there needs to be complex or conditional logic inside subtests (i.e. complex logic inside the for loop).  Large, complex table tests harm readability and maintainability because test readers may have difficulty debugging test failures that occur.  Table tests like this should be split into either multiple test tables or multiple individual Test... functions.  Some ideals to aim for are:  Focus on the narrowest unit of behaviorMinimize &quot;test depth&quot;, and avoid conditional assertions (see below)Ensure that all table fields are used in all testsEnsure that all test logic runs for all table cases  In this context, &quot;test depth&quot; means &quot;within a given test, the number of successive assertions that require previous assertions to hold&quot; (similar to cyclomatic complexity). Having &quot;shallower&quot; tests means that there are fewer relationships between assertions and, more importantly, that those assertions are less likely to be conditional by default.  Concretely, table tests can become confusing and difficult to read if they use multiple branching pathways ( e.g. shouldError, expectCall, etc.), use many if statements for specific mock expectations (e.g. shouldCallFoo), or place functions inside the table (e.g. setupMocks func(*FooMock)).  However, when testing behavior that only changes based on changed input, it may be preferable to group similar cases together in a table test to better illustrate how behavior changes across all inputs, rather than splitting otherwise comparable units into separate tests and making them harder to compare and contrast.  If the test body is short and straightforward, it's acceptable to have a single branching pathway for success versus failure cases with a table field like shouldErr to specify error expectations.  Bad\tGood func TestComplicatedTable(t *testing.T) { tests := []struct { give string want string wantErr error shouldCallX bool shouldCallY bool giveXResponse string giveXErr error giveYResponse string giveYErr error }{ // ... } for _, tt := range tests { t.Run(tt.give, func(t *testing.T) { // setup mocks ctrl := gomock.NewController(t) xMock := xmock.NewMockX(ctrl) if tt.shouldCallX { xMock.EXPECT().Call().Return( tt.giveXResponse, tt.giveXErr, ) } yMock := ymock.NewMockY(ctrl) if tt.shouldCallY { yMock.EXPECT().Call().Return( tt.giveYResponse, tt.giveYErr, ) } got, err := DoComplexThing(tt.give, xMock, yMock) // verify results if tt.wantErr != nil { require.EqualError(t, err, tt.wantErr) return } require.NoError(t, err) assert.Equal(t, want, got) }) } } func TestShouldCallX(t *testing.T) { // setup mocks ctrl := gomock.NewController(t) xMock := xmock.NewMockX(ctrl) xMock.EXPECT().Call().Return(&quot;XResponse&quot;, nil) yMock := ymock.NewMockY(ctrl) got, err := DoComplexThing(&quot;inputX&quot;, xMock, yMock) require.NoError(t, err) assert.Equal(t, &quot;want&quot;, got) } func TestShouldCallYAndFail(t *testing.T) { // setup mocks ctrl := gomock.NewController(t) xMock := xmock.NewMockX(ctrl) yMock := ymock.NewMockY(ctrl) yMock.EXPECT().Call().Return(&quot;YResponse&quot;, nil) _, err := DoComplexThing(&quot;inputY&quot;, xMock, yMock) assert.EqualError(t, err, &quot;Y failed&quot;) }   This complexity makes it more difficult to change, understand, and prove the correctness of the test.  While there are no strict guidelines, readability and maintainability should always be top-of-mind when deciding between Table Tests versus separate tests for multiple inputs/outputs to a system.  Parallel Tests​  Parallel tests, like some specialized loops (for example, those that spawn goroutines or capture references as part of the loop body), must take care to explicitly assign loop variables within the loop's scope to ensure that they hold the expected values.  tests := []struct{ give string // ... }{ // ... } for _, tt := range tests { tt := tt // for t.Parallel t.Run(tt.give, func(t *testing.T) { t.Parallel() // ... }) }   In the example above, we must declare a tt variable scoped to the loop iteration because of the use of t.Parallel()below. If we do not do that, most or all tests will receive an unexpected value for tt, or a value that changes as they're running.  ","version":"Next","tagName":"h3"},{"title":"Functional Options​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#functional-options","content":" Functional options is a pattern in which you declare an opaque Option type that records information in some internal struct. You accept a variadic number of these options and act upon the full information recorded by the options on the internal struct.  Use this pattern for optional arguments in constructors and other public APIs that you foresee needing to expand, especially if you already have three or more arguments on those functions.  Bad\tGood // package db func Open( addr string, cache bool, logger *zap.Logger ) (*Connection, error) { // ... } // package db type Option interface { // ... } func WithCache(c bool) Option { // ... } func WithLogger(log *zap.Logger) Option { // ... } // Open creates a connection. func Open( addr string, opts ...Option, ) (*Connection, error) { // ... } The cache and logger parameters must always be provided, even if the user wants to use the default. db.Open(addr, db.DefaultCache, zap.NewNop()) db.Open(addr, db.DefaultCache, log) db.Open(addr, false /* cache */, zap.NewNop()) db.Open(addr, false /* cache */, log) Options are provided only if needed. db.Open(addr) db.Open(addr, db.WithLogger(log)) db.Open(addr, db.WithCache(false)) db.Open( addr, db.WithCache(false), db.WithLogger(log), )   Our suggested way of implementing this pattern is with an Option interface that holds an unexported method, recording options on an unexported options struct.  type options struct { cache bool logger *zap.Logger } type Option interface { apply(*options) } type cacheOption bool func (c cacheOption) apply(opts *options) { opts.cache = bool(c) } func WithCache(c bool) Option { return cacheOption(c) } type loggerOption struct { Log *zap.Logger } func (l loggerOption) apply(opts *options) { opts.logger = l.Log } func WithLogger(log *zap.Logger) Option { return loggerOption{Log: log} } // Open creates a connection. func Open( addr string, opts ...Option, ) (*Connection, error) { options := options{ cache: defaultCache, logger: zap.NewNop(), } for _, o := range opts { o.apply(&amp;options) } // ... }   Note that there's a method of implementing this pattern with closures but we believe that the pattern above provides more flexibility for authors and is easier to debug and test for users. In particular, it allows options to be compared against each other in tests and mocks, versus closures where this is impossible. Further, it lets options implement other interfaces, including fmt.Stringer which allows for user-readable string representations of the options.  See also,  Self-referential functions and the design of optionsFunctional options for friendly APIs  ","version":"Next","tagName":"h3"},{"title":"Linting​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#linting","content":" More importantly than any &quot;blessed&quot; set of linters, lint consistently across a codebase.  We recommend using the following linters at a minimum, because we feel that they help to catch the most common issues and also establish a high bar for code quality without being unnecessarily prescriptive:  errcheck to ensure that errors are handledgoimports to format code and manage importsgolint to point out common style mistakesgovet to analyze code for common mistakesstaticcheck to do various static analysis checks  ","version":"Next","tagName":"h2"},{"title":"Lint Runners​","type":1,"pageTitle":"Go","url":"/docs/docs/contributing/language-specific/golang#lint-runners","content":" We recommend golangci-lint as the go-to lint runner for Go code, largely due to its performance in larger codebases and ability to configure and use many canonical linters at once. ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}